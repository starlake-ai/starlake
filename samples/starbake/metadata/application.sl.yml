version: 1
application:
  connectionRef: "{{connectionRef}}"

  audit:
    sink:
      connectionRef: "{{connectionRef}}"


  connections:
    snowflake:
      type: jdbc
      options:
        url: "jdbc:snowflake://{{SNOWFLAKE_ACCOUNT}}.snowflakecomputing.com/"
        driver: "net.snowflake.client.jdbc.SnowflakeDriver"
        account: "{{SNOWFLAKE_ACCOUNT}}"
        user: "{{SNOWFLAKE_USER}}"
        password: "{{SNOWFLAKE_PASSWORD}}"
        warehouse: "{{SNOWFLAKE_WAREHOUSE}}"
        db: "{{SNOWFLAKE_DB}}"
        schema: "${SNOWFLAKE_SCHEMA}"
        keep_column_case: "off"
        preActions: "ALTER SESSION SET QUERY_TAG = 'starlake';ALTER SESSION SET TIMESTAMP_TYPE_MAPPING = 'TIMESTAMP_LTZ';ALTER SESSION SET QUOTED_IDENTIFIERS_IGNORE_CASE = true"
    spark_local:
      type: "fs" # Connection to local file system (delta files)
    duckdb:
      type: "jdbc" # Connection to DuckDB
      options:
        url: "jdbc:duckdb:{{SL_ROOT}}/datasets/duckdb.db" # Location of the DuckDB database
        driver: "org.duckdb.DuckDBDriver"
        # to use ducklake, install the ducklake extension as follows:
        # preActions: "INSTALL ducklake;LOAD ducklake;ATTACH 'ducklake:metadata.ducklake' AS my_ducklake (DATA_PATH '/Users/me/datasets/ducklake/');USE my_ducklake;"
    bigquery:
      # When accessing from your desktop, do not forget to set GOOGLE_APPLICATION_CREDENTIALS to your application credentials file
      # example on MacOS: export GOOGLE_APPLICATION_CREDENTIALS="$HOME/.config/gcloud/application_default.json"
      type: "bigquery"
      options:
        location: europe-west1
        authType: "APPLICATION_DEFAULT"
        authScopes: "https://www.googleapis.com/auth/cloud-platform"
        writeMethod: "direct"
  dagRef:
    load: "airflow_load_shell"
    transform: "airflow_transform_shell"
