"use strict";(self.webpackChunkstarlake=self.webpackChunkstarlake||[]).push([[3942],{87469:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>a,metadata:()=>i,toc:()=>c});var r=o(74848),t=o(28453);const a={},s="Azure Synapse Spark Pools",i={id:"configuration/platforms/azure",title:"Azure Synapse Spark Pools",description:"Running Locally",source:"@site/versioned_docs/version-1.2.0/0500-configuration/0700-platforms/020.azure.md",sourceDirName:"0500-configuration/0700-platforms",slug:"/configuration/platforms/azure",permalink:"/starlake/docs/configuration/platforms/azure",draft:!1,unlisted:!1,editUrl:"https://github.com/starlake-ai/starlake/edit/master/docs/versioned_docs/version-1.2.0/0500-configuration/0700-platforms/020.azure.md",tags:[],version:"1.2.0",sidebarPosition:20,frontMatter:{},sidebar:"starlakeSidebar",previous:{title:"Amazon Web Services",permalink:"/starlake/docs/configuration/platforms/aws"},next:{title:"Databricks on any cloud",permalink:"/starlake/docs/configuration/platforms/databricks"}},l={},c=[{value:"Running Locally",id:"running-locally",level:2},{value:"Running on Azure",id:"running-on-azure",level:2}];function u(e){const n={code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"azure-synapse-spark-pools",children:"Azure Synapse Spark Pools"}),"\n",(0,r.jsx)(n.h2,{id:"running-locally",children:"Running Locally"}),"\n",(0,r.jsx)(n.p,{children:"Starlake need to access ADFS. You need to provide the credentials in one of the three ways below:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Through a core-site.xml file present in the classpath (you'll probably use this method when running the ingestion process from your laptop):"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:' <?xml version="1.0" encoding="UTF-8"?>\n <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n <configuration>\n     <property>\n         <name>fs.azure.account.key.ebizcomet.dfs.core.windows.net</name>\n         <value>*******==</value>\n     </property>\n     <property>\n         <name>fs.default.name</name>\n         <value>abfs://cometfs@ebizcomet.dfs.core.windows.net/</value>\n     </property>\n </configuration>\n'})}),"\n",(0,r.jsx)(n.h2,{id:"running-on-azure",children:"Running on Azure"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["At cluster creation as specified ",(0,r.jsx)(n.code,{children:"here <https://docs.microsoft.com/fr-fr/azure/databricks/data/data-sources/azure/azure-datalake-gen2#rdd-api>"}),"_.\n(you'll probably use this method on a production cluster)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Through a specific application.conf file in the starlake-assembly.jar classpath.\nYou must add the spark.hadoop. prefix to the corresponding Hadoop configuration keys to propagate them to the Hadoop configurations that are used used in the Starlake Spark Job."}),"\n"]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}},28453:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>i});var r=o(96540);const t={},a=r.createContext(t);function s(e){const n=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);