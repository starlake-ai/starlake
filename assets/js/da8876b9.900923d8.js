"use strict";(self.webpackChunkstarlake=self.webpackChunkstarlake||[]).push([[1762],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>X});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),c=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},u=function(e){var t=c(e.components);return a.createElement(s.Provider,{value:t},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),p=c(n),m=r,X=p["".concat(s,".").concat(m)]||p[m]||d[m]||o;return n?a.createElement(X,l(l({ref:t},u),{},{components:n})):a.createElement(X,l({ref:t},u))}));function X(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,l=new Array(o);l[0]=m;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i[p]="string"==typeof e?e:r,l[1]=i;for(var c=2;c<o;c++)l[c]=n[c];return a.createElement.apply(null,l)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},85162:(e,t,n)=>{n.d(t,{Z:()=>l});var a=n(67294),r=n(86010);const o={tabItem:"tabItem_Ymn6"};function l(e){let{children:t,hidden:n,className:l}=e;return a.createElement("div",{role:"tabpanel",className:(0,r.Z)(o.tabItem,l),hidden:n},t)}},74866:(e,t,n)=>{n.d(t,{Z:()=>v});var a=n(87462),r=n(67294),o=n(86010),l=n(12466),i=n(16550),s=n(91980),c=n(67392),u=n(50012);function p(e){return function(e){return r.Children.map(e,(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:t,label:n,attributes:a,default:r}}=e;return{value:t,label:n,attributes:a,default:r}}))}function d(e){const{values:t,children:n}=e;return(0,r.useMemo)((()=>{const e=t??p(n);return function(e){const t=(0,c.l)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,n])}function m(e){let{value:t,tabValues:n}=e;return n.some((e=>e.value===t))}function X(e){let{queryString:t=!1,groupId:n}=e;const a=(0,i.k6)(),o=function(e){let{queryString:t=!1,groupId:n}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:t,groupId:n});return[(0,s._X)(o),(0,r.useCallback)((e=>{if(!o)return;const t=new URLSearchParams(a.location.search);t.set(o,e),a.replace({...a.location,search:t.toString()})}),[o,a])]}function f(e){const{defaultValue:t,queryString:n=!1,groupId:a}=e,o=d(e),[l,i]=(0,r.useState)((()=>function(e){let{defaultValue:t,tabValues:n}=e;if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!m({value:t,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const a=n.find((e=>e.default))??n[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:t,tabValues:o}))),[s,c]=X({queryString:n,groupId:a}),[p,f]=function(e){let{groupId:t}=e;const n=function(e){return e?`docusaurus.tab.${e}`:null}(t),[a,o]=(0,u.Nk)(n);return[a,(0,r.useCallback)((e=>{n&&o.set(e)}),[n,o])]}({groupId:a}),b=(()=>{const e=s??p;return m({value:e,tabValues:o})?e:null})();(0,r.useLayoutEffect)((()=>{b&&i(b)}),[b]);return{selectedValue:l,selectValue:(0,r.useCallback)((e=>{if(!m({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);i(e),c(e),f(e)}),[c,f,o]),tabValues:o}}var b=n(72389);const k={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function y(e){let{className:t,block:n,selectedValue:i,selectValue:s,tabValues:c}=e;const u=[],{blockElementScrollPositionUntilNextRender:p}=(0,l.o5)(),d=e=>{const t=e.currentTarget,n=u.indexOf(t),a=c[n].value;a!==i&&(p(t),s(a))},m=e=>{let t=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const n=u.indexOf(e.currentTarget)+1;t=u[n]??u[0];break}case"ArrowLeft":{const n=u.indexOf(e.currentTarget)-1;t=u[n]??u[u.length-1];break}}t?.focus()};return r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.Z)("tabs",{"tabs--block":n},t)},c.map((e=>{let{value:t,label:n,attributes:l}=e;return r.createElement("li",(0,a.Z)({role:"tab",tabIndex:i===t?0:-1,"aria-selected":i===t,key:t,ref:e=>u.push(e),onKeyDown:m,onClick:d},l,{className:(0,o.Z)("tabs__item",k.tabItem,l?.className,{"tabs__item--active":i===t})}),n??t)})))}function g(e){let{lazy:t,children:n,selectedValue:a}=e;const o=(Array.isArray(n)?n:[n]).filter(Boolean);if(t){const e=o.find((e=>e.props.value===a));return e?(0,r.cloneElement)(e,{className:"margin-top--md"}):null}return r.createElement("div",{className:"margin-top--md"},o.map(((e,t)=>(0,r.cloneElement)(e,{key:t,hidden:e.props.value!==a}))))}function h(e){const t=f(e);return r.createElement("div",{className:(0,o.Z)("tabs-container",k.tabList)},r.createElement(y,(0,a.Z)({},e,t)),r.createElement(g,(0,a.Z)({},e,t)))}function v(e){const t=(0,b.Z)();return r.createElement(h,(0,a.Z)({key:String(t)},e))}},66644:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>u,contentTitle:()=>s,default:()=>X,frontMatter:()=>i,metadata:()=>c,toc:()=>p});var a=n(87462),r=(n(67294),n(3905)),o=n(74866),l=n(85162);const i={},s="Connections",c={unversionedId:"configuration/connections",id:"configuration/connections",title:"Connections",description:"Connections are defined in the connections section under the root attribute application.",source:"@site/docs/0500-configuration/0110-connections.mdx",sourceDirName:"0500-configuration",slug:"/configuration/connections",permalink:"/starlake/docs/next/configuration/connections",draft:!1,editUrl:"https://github.com/starlake-ai/starlake/edit/master/docs/docs/0500-configuration/0110-connections.mdx",tags:[],version:"current",sidebarPosition:110,frontMatter:{},sidebar:"starlakeSidebar",previous:{title:"Environment",permalink:"/starlake/docs/next/configuration/environment"},next:{title:"Platforms",permalink:"/starlake/docs/next/category/platforms"}},u={},p=[{value:"Local File System",id:"local-file-system",level:2},{value:"Google BigQuery",id:"google-bigquery",level:2},{value:"Apache Spark / Databricks",id:"apache-spark--databricks",level:2},{value:"Snowflake",id:"snowflake",level:2},{value:"Amazon Redshift",id:"amazon-redshift",level:2},{value:"Postgres",id:"postgres",level:2}],d={toc:p},m="wrapper";function X(e){let{components:t,...n}=e;return(0,r.kt)(m,(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"connections"},"Connections"),(0,r.kt)("p",null,"Connections are defined in the ",(0,r.kt)("inlineCode",{parentName:"p"},"connections")," section under the root attribute ",(0,r.kt)("inlineCode",{parentName:"p"},"application"),"."),(0,r.kt)("p",null,"The following types of connections are supported:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#local-file-system"},"Local File System")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#google-bigquery"},"BigQuery")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#apache-spark--databricks"},"Spark / Databricks")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#snowflake"},"Snowflake")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#amazon-redshift"},"Redshift")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#postgres"},"Postgres"))),(0,r.kt)("h2",{id:"local-file-system"},"Local File System"),(0,r.kt)("p",null,"The local file system connection is used to read and write files to the local file system."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"application:\n    connections:\n    local:\n        type: local\n")),(0,r.kt)("p",null,"Files will be stored in the ",(0,r.kt)("inlineCode",{parentName:"p"},"area")," directory under the ",(0,r.kt)("inlineCode",{parentName:"p"},"datasets")," directory.\nD\xe9fault values for ",(0,r.kt)("inlineCode",{parentName:"p"},"area")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"datasets")," can be set in the ",(0,r.kt)("inlineCode",{parentName:"p"},"application")," section."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'application:\n    datasets = "{{root}}/datasets" # or set it through the SL_DATASETS environnement variable.\n    area:\n        pending: "pending" # Location where files of pending load are stored. May be overloaded by the ${SL_AREA_PENDING} environment variable.\n        unresolved: "unresolved" # Location where files that do not match any pattern are moved. May be overloaded by the ${SL_AREA_UNRESOLVED} environment variable.\n        archive: "archive" # Location where files are moved after they have been processed. May be overloaded by the ${SL_AREA_ARCHIVE} environment variable.\n        ingesting: "ingesting" # Location where files are moved while they are being processed. May be overloaded by the ${SL_AREA_INGESTING} environment variable.\n        accepted: "accepted" # Location where files are moved after they have been processed and accepted. May be overloaded by the ${SL_AREA_ACCEPTED} environment variable.\n        rejected: "rejected" # Location where files are moved after they have been processed and rejected. May be overloaded by the ${SL_AREA_REJECTED} environment variable.\n        business: "business" # Location where transform tasks store their result. May be overloaded by the ${SL_AREA_BUSINESS} environment variable.\n        replay: "replay" # Location rejected records are stored in their orginial format. May be overloaded by the ${SL_AREA_REPLAY} environment variable.\n        hiveDatabase: "${domain}_${area}" # Hive database name. May be overloaded by the ${SL_AREA_HIVE_DATABASE} environment variable.\n\n')),(0,r.kt)("h2",{id:"google-bigquery"},"Google BigQuery"),(0,r.kt)("p",null,"Starlake support native and spark / dataproc bigquery connections."),(0,r.kt)(o.Z,{groupId:"bq_connections",mdxType:"Tabs"},(0,r.kt)(l.Z,{label:"BigQuery",value:"bigquery",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'application:\n  connections:\n    bigquery:\n      type: "bigquery"\n      options:\n        location: "us-central1" # EU or US or ...\n        authType: "APPLICATION_DEFAULT"\n        authScopes: "https://www.googleapis.com/auth/cloud-platform" # comma separated list of scopes\n        #authType: SERVICE_ACCOUNT_JSON_KEYFILE\n        #jsonKeyfile: "/Users/me/.gcloud/keys/starlake-me.json"\n        #authType: "ACCESS_TOKEN"\n        #gcpAccessToken: "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"\n  accessPolicies: # Required when applying access policies to table columns (Column Level Security)\n    apply: true\n    location: EU\n    taxonomy: RGPD\n'))),(0,r.kt)(l.Z,{label:"Spark BigQuery Direct",value:"spark-bigquery-direct",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'application:\n  connections:\n    bigquery:\n      type: "bigquery"\n      sparkFormat: "bigquery"\n      options:\n        writeMethod: "direct" # direct or indirect (indirect is required for certain features see https://github.com/GoogleCloudDataproc/spark-bigquery-connector)\n        location: "us-central1" # EU or US or ...\n        authType: "APPLICATION_DEFAULT"\n        authScopes: "https://www.googleapis.com/auth/cloud-platform" # comma separated list of scopes\n        # authType: SERVICE_ACCOUNT_JSON_KEYFILE\n        # jsonKeyfile: "/Users/me/.gcloud/keys/starlake-me.json"\n        # authType: "ACCESS_TOKEN"\n        # gcpAccessToken: "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"\n  spark:\n    datasource:\n      bigquery: # Setting properties here will apply them to all bigquery data sources (connection.type == bigquery)\n        allowFieldAddition: "true" # Allow schema updates. To disable, set it to false\n        allowFieldRelaxation: "true" # Allow schema updates. To disable, set it to false\n\n'))),(0,r.kt)(l.Z,{label:"Spark BigQuery Indirect",value:"spark-bigquery-indirect",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'application:\n  connections:\n    bigquery:\n      type: "bigquery"\n      sparkFormat: "bigquery"\n      options:\n        writeMethod: "indirect" # direct or indirect (indirect is required for certain features see https://github.com/GoogleCloudDataproc/spark-bigquery-connector)\n        gcsBucket: "starlake-app" # Temporary GCS Bucket where intermediary files will be stored. Required in indirect mode only\n        location: "us-central1" # EU or US or ...\n        authType: "APPLICATION_DEFAULT"\n        authScopes: "https://www.googleapis.com/auth/cloud-platform" # comma separated list of scopes\n        materializationDataset: "my-bucket-name" # when sparkFormat is defined, required by the spark-bigquery-connector (https://github.com/GoogleCloudDataproc/spark-bigquery-connector?tab=readme-ov-file#properties)\n        #authType: SERVICE_ACCOUNT_JSON_KEYFILE\n        #jsonKeyfile: "/Users/me/.gcloud/keys/starlake-me.json"\n        #authType: "ACCESS_TOKEN"\n        #gcpAccessToken: "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"\n  spark:\n    datasource:\n      bigquery:\n        allowFieldAddition: "true" # Allow schema updates. To disable, set it to false\n        allowFieldRelaxation: "true" # Allow schema updates. To disable, set it to false\n\n')))),(0,r.kt)("h2",{id:"apache-spark--databricks"},"Apache Spark / Databricks"),(0,r.kt)("p",null,"Spark connections are used to read and write data from Spark."),(0,r.kt)(o.Z,{groupId:"spark_connections",mdxType:"Tabs"},(0,r.kt)(l.Z,{label:"Spark Parquet",value:"spark-parquet",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'application:\n  connections:\n    spark:\n      type: "spark"\n      options:\n        # any spark configuration can be set here\n      \n'))),(0,r.kt)(l.Z,{label:"Spark Delta",value:"spark-delta",mdxType:"TabItem"},(0,r.kt)("p",null,"In addition to the connection defined below, please download the following jars: "),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://repo1.maven.org/maven2/io/delta/delta-spark_2.12"},"delta-spark_2.12-VERSION.jar")," and place it in the ",(0,r.kt)("inlineCode",{parentName:"li"},"bin/deps")," directory of the starlake directory."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://repo1.maven.org/maven2/io/delta/delta-storage"},"delta-storage_2.12-VERSION.jar")," and place it in the ",(0,r.kt)("inlineCode",{parentName:"li"},"bin/deps")," directory of the starlake directory.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'application:\n  connections:\n    spark:\n      type: "spark"\n      options:\n        # any spark configuration can be set here\n  spark:\n    sql:\n      extensions: "io.delta.sql.DeltaSparkSessionExtension"\n      catalog:\n        spark_catalog: "org.apache.spark.sql.delta.catalog.DeltaCatalog"\n\n')))),(0,r.kt)("h2",{id:"snowflake"},"Snowflake"),(0,r.kt)(o.Z,{groupId:"snow_connections",mdxType:"Tabs"},(0,r.kt)(l.Z,{label:"Snowflake JDBC",value:"snow-jdbc",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'\napplication:\n  connectionRef: {{connection}}\n  connections:\n    snowflake:\n      type: jdbc\n      options:\n        url: "jdbc:snowflake://{{SNOWFLAKE_ACCOUNT}}.snowflakecomputing.com"\n        driver: "net.snowflake.client.jdbc.SnowflakeDriver"\n        user: {{SNOWFLAKE_USER}}\n        password: {{SNOWFLAKE_PASSWORD}}\n        warehouse: {{SNOWFLAKE_WAREHOUSE}}\n        db: {{SNOWFLAKE_DB}}\n        keep_column_case: "off"\n        preActions: "alter session set TIMESTAMP_TYPE_MAPPING = \'TIMESTAMP_LTZ\';ALTER SESSION SET QUOTED_IDENTIFIERS_IGNORE_CASE = true"\n      \n'))),(0,r.kt)(l.Z,{label:"Snowflake Spark",value:"snow-spark",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'\napplication:\n  connectionRef: {{connection}}\n  connections:\n    snowflake:\n    spark-snowflake:\n      type: jdbc\n      sparkFormat: snowflake\n      options:\n        sfUrl: "{{SNOWFLAKE_ACCOUNT}}.snowflakecomputing.com" # make sure you do not prefix by jdbc:snowflake://. This is done by the snowflaek driver\n        #sfDriver: "net.snowflake.client.jdbc.SnowflakeDriver"\n        sfUser: {{SNOWFLAKE_USER}}\n        sfPassword: {{SNOWFLAKE_PASSWORD}}\n        sfWarehouse: {{SNOWFLAKE_WAREHOUSE}}\n        sfDatabase: {{SNOWFLAKE_DB}}\n        keep_column_case: "off"\n        autopushdown: on\n        preActions: "alter session set TIMESTAMP_TYPE_MAPPING = \'TIMESTAMP_LTZ\';ALTER SESSION SET QUOTED_IDENTIFIERS_IGNORE_CASE = true"\n\n')))),(0,r.kt)("h2",{id:"amazon-redshift"},"Amazon Redshift"),(0,r.kt)(o.Z,{groupId:"redshift_connections",mdxType:"Tabs"},(0,r.kt)(l.Z,{label:"Redshift JDBC",value:"redshift-jdbc",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'\napplication:\n  connections:\n    redshift:\n      options:\n        url: "jdbc:redshift://account.region.redshift.amazonaws.com:5439/database",\n        driver: com.amazon.redshift.Driver\n        password: "{{REDSHIFT_PASSWORD}}"\n        tempdir: "s3a://bucketName/data",\n        tempdir_region: "eu-central-1" # required only if running from outside AWS (your laptop ...)\n        aws_iam_role: "arn:aws:iam::aws_count_id:role/role_name"\n  \n'))),(0,r.kt)(l.Z,{label:"Redshift Spark",value:"redshift-spark",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'\napplication:\n  connections:\n    redshift:\n      sparkFormat: "io.github.spark_redshift_community.spark.redshift" # if running on top of Spark or else  "redshift" if running on top of Databricks\n      options:\n        url: "jdbc:redshift://account.region.redshift.amazonaws.com:5439/database",\n        driver: com.amazon.redshift.Driver\n        password: "{{REDSHIFT_PASSWORD}}"\n        tempdir: "s3a://bucketName/data",\n        tempdir_region: "eu-central-1" # required only if running from outside AWS (your laptop ...)\n        aws_iam_role: "arn:aws:iam::aws_count_id:role/role_name"\n\n')))),(0,r.kt)("h2",{id:"postgres"},"Postgres"),(0,r.kt)(o.Z,{groupId:"pg_connections",mdxType:"Tabs"},(0,r.kt)(l.Z,{label:"Postgres JDBC",value:"postgres-jdbc",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'application:\n  connectionRef: "postgresql"\n  connections:\n    postgresql:\n      type: jdbc\n      options:\n        url: "jdbc:postgresql://{{POSTGRES_HOST}}:{{POSTGRES_PORT}}/{{POSTGRES_DATABASE}}"\n        driver: "org.postgresql.Driver"\n        user: "{{DATABASE_USER}}"\n        password: "{{DATABASE_PASSWORD}}"\n        quoteIdentifiers: false\n'))),(0,r.kt)(l.Z,{label:"Postgres Spark",value:"pg-spark",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'\napplication:\n  connectionRef: "postgresql"\n  connections:\n    postgresql:\n      type: jdbc\n      sparkFormat: jdbc\n      options:\n        url: "jdbc:postgresql://{{POSTGRES_HOST}}:{{POSTGRES_PORT}}/{{POSTGRES_DATABASE}}"\n        driver: "org.postgresql.Driver"\n        user: "{{DATABASE_USER}}"\n        password: "{{DATABASE_PASSWORD}}"\n        quoteIdentifiers: false\n')))))}X.isMDXComponent=!0}}]);