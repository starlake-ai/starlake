"use strict";(self.webpackChunkstarlake_docs=self.webpackChunkstarlake_docs||[]).push([[3685],{3905:(e,t,r)=>{r.d(t,{Zo:()=>u,kt:()=>f});var n=r(7294);function a(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function l(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function s(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?l(Object(r),!0).forEach((function(t){a(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):l(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function o(e,t){if(null==e)return{};var r,n,a=function(e,t){if(null==e)return{};var r,n,a={},l=Object.keys(e);for(n=0;n<l.length;n++)r=l[n],t.indexOf(r)>=0||(a[r]=e[r]);return a}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(n=0;n<l.length;n++)r=l[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(a[r]=e[r])}return a}var i=n.createContext({}),p=function(e){var t=n.useContext(i),r=t;return e&&(r="function"==typeof e?e(t):s(s({},t),e)),r},u=function(e){var t=p(e.components);return n.createElement(i.Provider,{value:t},e.children)},m="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var r=e.components,a=e.mdxType,l=e.originalType,i=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),m=p(r),c=a,f=m["".concat(i,".").concat(c)]||m[c]||d[c]||l;return r?n.createElement(f,s(s({ref:t},u),{},{components:r})):n.createElement(f,s({ref:t},u))}));function f(e,t){var r=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var l=r.length,s=new Array(l);s[0]=c;var o={};for(var i in t)hasOwnProperty.call(t,i)&&(o[i]=t[i]);o.originalType=e,o[m]="string"==typeof e?e:a,s[1]=o;for(var p=2;p<l;p++)s[p]=r[p];return n.createElement.apply(null,s)}return n.createElement.apply(null,r)}c.displayName="MDXCreateElement"},2655:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>i,contentTitle:()=>s,default:()=>d,frontMatter:()=>l,metadata:()=>o,toc:()=>p});var n=r(7462),a=(r(7294),r(3905));const l={sidebar_position:300},s="Transform",o={unversionedId:"userguide/transform",id:"userguide/transform",title:"Transform",description:"Once your data is ingested, you may start to expose insights by joining them and / or create meaningful aggregates.",source:"@site/docs/userguide/300.transform.md",sourceDirName:"userguide",slug:"/userguide/transform",permalink:"/starlake/docs/userguide/transform",draft:!1,editUrl:"https://github.com/starlake-ai/starlake/edit/master/docs/docs/userguide/300.transform.md",tags:[],version:"current",sidebarPosition:300,frontMatter:{sidebar_position:300},sidebar:"starlakeSidebar",previous:{title:"Load",permalink:"/starlake/docs/userguide/load"},next:{title:"Configuration",permalink:"/starlake/docs/reference/configuration"}},i={},p=[{value:"Parquet Job",id:"parquet-job",level:2},{value:"BigQuery Job",id:"bigquery-job",level:2},{value:"Externalize SQL Requests",id:"externalize-sql-requests",level:2}],u={toc:p},m="wrapper";function d(e){let{components:t,...r}=e;return(0,a.kt)(m,(0,n.Z)({},u,r,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"transform"},"Transform"),(0,a.kt)("p",null,"Once your data is ingested, you may start to expose insights by joining them and / or create meaningful aggregates."),(0,a.kt)("p",null,"In the example below, we join the ",(0,a.kt)("inlineCode",{parentName:"p"},"sellers")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"orders")," tables to compute the total amount sold by each seller."),(0,a.kt)("p",null,"We want to do it on parquet files and on BigQuery. We need to create 2 env files, one for environment."),(0,a.kt)("h2",{id:"parquet-job"},"Parquet Job"),(0,a.kt)("p",null,"Create the ",(0,a.kt)("inlineCode",{parentName:"p"},"env.FS.comet.yml")," file in the ",(0,a.kt)("inlineCode",{parentName:"p"},"metadata")," folder as follows:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},'env:\n  sink_type: "FS"\n  engine: Spark\n  sellers_view: "FS:${root_path}/datasets/accepted/hr/sellers"\n  orders_view: "FS:${root_path}/datasets/accepted/sales/orders"\n')),(0,a.kt)("p",null,"Create the YAML file that describe the job and name it ",(0,a.kt)("inlineCode",{parentName:"p"},"sellers_kpi.comet.yml"),":"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},'transform:\n  name: kpi\n  engine: ${engine}\n  views:\n    sellers: "${sellers_view}"\n    orders: "${orders_view}"\n  tasks:\n    - sql: "select seller_email, sum(amount) as sum from sellers, orders where sellers.id = orders.seller_id group by sellers.seller_email"\n      name: byseller\n      domain: sales_kpi\n      dataset: byseller_kpi\n      write: OVERWRITE\n  area: business\n')),(0,a.kt)("p",null,"Before executing the job, we set the ",(0,a.kt)("inlineCode",{parentName:"p"},"SL_ENV")," variable to ",(0,a.kt)("inlineCode",{parentName:"p"},"FS")," to make sure variables are instantiated correctly:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},"export SL_ENV=FS\n$ starlake.sh transform --name sellers_kpi\n")),(0,a.kt)("h2",{id:"bigquery-job"},"BigQuery Job"),(0,a.kt)("p",null,"To execute the same request on BigQuery, we need to create views on the BigQuery tables where data were ingested.\nCreate the ",(0,a.kt)("inlineCode",{parentName:"p"},"env.BQ.comet.yml")," file in the ",(0,a.kt)("inlineCode",{parentName:"p"},"metadata")," folder as follows:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},'env:\n  sink_type: "BQ"\n  engine: BQ\n  sellers_view: "BQ:select * from hr.sellers" # or simply sellers_view: hr.sellers\n  orders_view: "BQ:select * from sales.orders" # or simply sellers_view: sales.orders\n')),(0,a.kt)("p",null,"Before executing the job, we set the ",(0,a.kt)("inlineCode",{parentName:"p"},"SL_ENV")," variable to ",(0,a.kt)("inlineCode",{parentName:"p"},"BQ")," to make sure variables are instantiated correctly:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},"export SL_ENV=BQ\n$ starlake.sh transform --name sellers_kpi\n")),(0,a.kt)("h2",{id:"externalize-sql-requests"},"Externalize SQL Requests"),(0,a.kt)("p",null,"Most of the time you may want to use your favorite SQL Deve env (I personally use Jetbrains IntelliJ / DataGrip).\nYou may externalize the SQL Code in a file named after the name of the job if you have only one task in the job file\nor after the name and the task for each task in the job file."),(0,a.kt)("p",null,"Create a SQL File with the name ",(0,a.kt)("inlineCode",{parentName:"p"},"ki.byseller.sql")," (or simply ",(0,a.kt)("inlineCode",{parentName:"p"},"ki.sql")," since we have only one task in the job file)"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sql"},"select seller_email, sum(amount) as sum from sellers, orders where sellers.id = orders.seller_id\ngroup by sellers.seller_email\n")),(0,a.kt)("p",null,"And leave the task.sql field blank in the YAML file."))}d.isMDXComponent=!0}}]);