"use strict";(self.webpackChunkstarlake=self.webpackChunkstarlake||[]).push([[8888],{5985:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"polars-vs-spark","metadata":{"permalink":"/starlake/blog/polars-vs-spark","editUrl":"https://github.com/starlake-ai/starlake/edit/master/docs/blog/2024-05-28-polars-vs-spark.mdx","source":"@site/blog/2024-05-28-polars-vs-spark.mdx","title":"Polars versus Spark","description":"Introduction","date":"2024-05-28T00:00:00.000Z","tags":[{"inline":true,"label":"Polars","permalink":"/starlake/blog/tags/polars"},{"inline":true,"label":"Spark","permalink":"/starlake/blog/tags/spark"},{"inline":true,"label":"Databricks","permalink":"/starlake/blog/tags/databricks"},{"inline":true,"label":"Data Engineering","permalink":"/starlake/blog/tags/data-engineering"},{"inline":true,"label":"Analytics","permalink":"/starlake/blog/tags/analytics"}],"readingTime":5.715,"hasTruncateMarker":false,"authors":[{"name":"Hayssam Saleh","title":"Starlake Core Team","url":"https://www.linkedin.com/in/hayssams/","imageURL":"https://s.gravatar.com/avatar/04aa2a859a66b52787bcba8c36beba8c.png"}],"frontMatter":{"slug":"polars-vs-spark","title":"Polars versus Spark","author":"Hayssam Saleh","author_title":"Starlake Core Team","author_url":"https://www.linkedin.com/in/hayssams/","author_image_url":"https://s.gravatar.com/avatar/04aa2a859a66b52787bcba8c36beba8c.png","tags":["Polars","Spark","Databricks","Data Engineering","Analytics"]},"unlisted":false,"nextItem":{"title":"Starlake OSS - Bringing Declarative Programming to Data Engineering and Analytics","permalink":"/starlake/blog/starlake-oss"}},"content":"<head>\\n    <link rel=\\"canonical\\" href=\\"https://www.linkedin.com/pulse/polars-vs-spark-good-bad-ugly-hayssam-saleh-wpwee\\" />\\n</head>\\n\\n\\n## Introduction\\n\\nPolars is often compared to Spark. In this post, I will highlight the main differences and the best use cases for each in my data engineering activities.\\n\\nAs a Data Engineer, I primarily focus on the following goals:\\n1. Parsing files, validating their input, and loading the data into the target data warehouse.\\n2. Once the data is loaded, applying transformations by joining and aggregating the data to build KPIs.\\n\\nHowever, on a daily basis, I also need to develop on my laptop and test my work locally before delivering it to the CI pipeline and then to production.\\n\\nWhat about my fellow data scientist colleagues? They need to run their workload on production data through their favorite notebook environment.\\n\\n\\nThis post addresses the following points:\\n\\n- How suitable each tool is for loading files into your data warehouse.\\n- How easy and powerful each tool is for performing transformations.\\n- How easy it is to test your code locally before deploying to the cloud\\n\\n\\n\\n## Load tasks\\n\\nData loading seems easy at first glance, as almost all databases and APIs offer some sort of single-line command to load a few lines or millions of records quickly. However, this simplicity disappears when you encounter real-world cases such as:\\n\\n- Fixed-width fields: These files are typically exported from mainframe databases.\\n- XML files: I sometimes work in  the finance industry where SWIFT is a common XML file format that will be around for some time.\\n- Multi-character CSV: For example, where the separator consists of two characters like ||.\\n- File validation: You cannot trust the files you receive and need to check their content thoroughly.\\n\\nLoading correct CSV or JSON files using Spark or Polars is straightforward. However, it is also straightforward using your favorite database command line utility, making this capabilities somewhat redundant and even slower than the native data warehouse load feature.\\n\\nHowever, in real-world scenarios, you want to ensure the incoming file adheres to the expected schema and load a variety of file formats not supported natively. This is where Spark excels compared to Polars, as it allows for the parallel loading of your JSONL or CSV files and offers through map operations local and distributed validation of your incoming file.\\n\\nAs XML and multichar and multiline CSV are only supported by Spark, dealing with file parsing for data loading, Spark is definitely the most suitable solution.\\n\\n\\n## Transform tasks\\n\\nThree interfaces are provided to transform data: SQL, Dafarames and Datasets.\\n\\n__SQL__\\n\\nSQL is the preferred language for most data analysts when it comes to computing aggregations. One of its key benefits is its widespread understanding, which eliminates the learning curve.\\n\\nYou can use SQL with both Spark and Polars, but Polars has significant limitations. It does not support SQL Window functions nor does it support the SQL statements for update, insert, delete, or merge operations.\\n\\n__Dataframes__\\n\\nBoth Polars and Spark offer excellent support for DataFrames. The main added value of using DataFrames is the ability to reuse portions of code and access features not available in standard SQL, such as lambda functions, JSON handling and array manipulation.\\n\\n__Datasets__\\n\\nAs a software engineer, I particularly appreciate Datasets. Datasets are typed DataFrames, meaning that syntax, column names, and types are checked at compile time. If you believe that statically typed languages greatly enhance code quality, then you understand the added value of datasets.\\n\\nDatasets are only supported by Spark, allowing you to write clean, reusable, and statically typed transformations. They are available exclusively to statically typed languages such as Java or Scala.\\n\\nSpark stands out as the only tool with complete support for SQL, DataFrames, and Datasets. Polars\u2019 limited support for SQL makes it less suitable for my data engineering tasks.\\n\\n## Cloud Era\\n\\nAt least 60% of the projects I have worked on are cloud-based, primarily targeting Amazon Redshift, Google BigQuery, Databricks, Snowflake, or Azure Synapse. All of these platforms offer serverless support for Spark, making it incredibly easy to run workloads by simply providing your PySpark script and letting the cloud provider handle the rest.\\n\\nIn the cloud environment, Spark is definitely the tool of choice as I see it.\\n\\n## Single node\\n\\nThere has been much discussion about Spark being slow or too heavy for single-node computers. I was particularly interested in running this test since I currently execute most of my workloads on a single-node Google Cloud Run job with Spark embedded in my Docker image.\\n\\nI decided to conduct this test on my almost 3-year-old MacBook Pro M1 Max with 64GB of memory. The test involved loading 27GB of CSV data, selecting a few attributes, computing metrics on those selected attributes, and then saving the results to a Parquet file.\\n\\nI ran Spark with default settings and without any fine-tuning. This means it utilized all 10 cores of my MacBook Pro M1 Max but only 1 gigabyte of memory for execution.\\n\\n::: note\\nI could have optimized my Spark workload, but given the small size of the dataset (27GB of CSV), it didn\'t make sense. The default settings were sufficient.\\n:::\\n\\nHere are the results after a cold restart  of my laptop before each test  to ensure the test did not benefit from any operating system cache.\\n\\n- Apache Spark pipeline took: 29 seconds\\n\\n- Polars pipeline took: 56 seconds\\n\\n::: note\\nRerunning Polars gave me 23 seconds instead of 56 seconds. This discrepancy is probably due to filesystem caching by the operating system.\\n:::\\n\\n\\nLoad and Save Test: Load a 27GB CSV file with 193 columns per record and save  the result as parquet.\\n\\n- Apache Spark pipeline took: 2mn 18s\\n\\n- Polars pipeline took: 2mn 32s\\n\\nLoad parquet and filter on column value then return count : Load a 74 millions records parquet file with 193 columns, filter on \'model\' column and return count.\\n\\n- Apache Spark pipeline took: 3 seconds\\n\\n- Polars pipeline took: 28 seconds\\n\\nThe table below summarises the results\\n\\n| Task | Spark 1GB memory | Polars All the available memory |\\n| --- | --- | --- |\\n| Load CSV, aggregate and save the aggregation as parquet | 29s | 56s |\\n| Load CSV and Save parquet | 2mn 18s | 2mn 32s |\\n| Load Parquet, filter and count | 3s | 28s |\\n\\n\\n\\n## Conclusion\\n\\nI don\u2019t think it is time to switch from Spark to Polars, at least for those of us accustomed to the JVM, running workloads in the cloud, or even working on small datasets. However, Polars may be a perfect fit for those familiar with pandas.\\n\\nAs of today, Spark is the only framework I see that can handle both local and distributed workloads, adapt to on-premise and cloud serverless jobs, and provide the complete SQL support required for most of our transformations."},{"id":"starlake-oss","metadata":{"permalink":"/starlake/blog/starlake-oss","editUrl":"https://github.com/starlake-ai/starlake/edit/master/docs/blog/2024-05-14-declarative.mdx","source":"@site/blog/2024-05-14-declarative.mdx","title":"Starlake OSS - Bringing Declarative Programming to Data Engineering and Analytics","description":"Introduction","date":"2024-05-14T00:00:00.000Z","tags":[{"inline":true,"label":"Starlake","permalink":"/starlake/blog/tags/starlake"},{"inline":true,"label":"Data Engineering","permalink":"/starlake/blog/tags/data-engineering"},{"inline":true,"label":"Analytics","permalink":"/starlake/blog/tags/analytics"}],"readingTime":5.07,"hasTruncateMarker":false,"authors":[{"name":"Hayssam Saleh","title":"Starlake Core Team","url":"https://www.linkedin.com/in/hayssams/","imageURL":"https://s.gravatar.com/avatar/04aa2a859a66b52787bcba8c36beba8c.png"}],"frontMatter":{"slug":"starlake-oss","title":"Starlake OSS - Bringing Declarative Programming to Data Engineering and Analytics","author":"Hayssam Saleh","author_title":"Starlake Core Team","author_url":"https://www.linkedin.com/in/hayssams/","author_image_url":"https://s.gravatar.com/avatar/04aa2a859a66b52787bcba8c36beba8c.png","tags":["Starlake","Data Engineering","Analytics"]},"unlisted":false,"prevItem":{"title":"Polars versus Spark","permalink":"/starlake/blog/polars-vs-spark"},"nextItem":{"title":"Column  and Row Level Security in BigQuery","permalink":"/starlake/blog/rls-cls-big-query"}},"content":"<head>\\n    <link rel=\\"canonical\\" href=\\"https://www.linkedin.com/pulse/starlake-oss-bringing-declarative-programming-data-engineering-5fdde\\" />\\n</head>\\n\\n## Introduction\\n\\nThe advent of declarative programming through tools like Ansible and Terraform, has revolutionized infrastructure deployment by allowing developers to achieve intended goals without specifying the order of code execution.\\n\\nThis paradigm shift brings forth benefits such as reduced error rates, significantly shortened development cycles, enhanced code readability, and increased accessibility for developers of all levels.\\n\\nThis is the story of how a small team of developers crafted a platform that goes beyond the boundaries  of conventional data engineering by applying a declarative approach to data extraction, loading, transformation and orchestration.\\n\\n![Starlake](/img/starlake-draw.png)\\n\\n## The Genesis\\n\\nBack in 2015, at the helm of ebiznext, a boutique data engineering company, we faced a daunting challenge. Our client, a prominent entity in need of a robust big data solution, sought to harness the power of Hadoop and Spark. Despite our modest size (20 people), we dared to compete against industry giants with tenfold resources (100.000+ headcount).\\n\\nOur only chance to succeed was to innovate: we needed a data platform that could exponentially outperform the traditional ETL solutions pushed by our competitors. To build data pipelines these GUI based ETLs  require an effort that is proportional to the number and complexity of the sources.\\n\\nDetermined to disrupt this norm, we embarked on a quest to devise a DevOps friendly platform capable of lightning-fast data ingestion from any source, without the drawbacks of ETLs or specialized engineering skills.\\n\\nThe day of the tender, our ability to deliver a solution that could load data in a few weeks instead of many months allowed us to stand out from the competition and win the project.\\n\\n## Expisode 1: Smartlake Emerges\\n\\n:::note\\n\\nThe basic idea behind Smartlake was that no datawarehouse would stay clean if data quality is checked after the data has been loaded and this pre-load quality checks needed to be handled by data owners.\\n\\n:::\\n\\nThis left us with little choice but to embrace the declarative approach. Empowering business users, we devised a system where data formats and transformations could be described in simple JSON files. Smartlake wasn\u2019t merely a code generator; it was a versatile engine, seamlessly ingesting diverse data formats, executing transformations, and orchestrating operations with unparalleled efficiency.\\n\\nTo streamline user interaction, we devised an intuitive Excel-to-JSON converter, enabling effortless specification of input formats. Thanks to Smartlake and its declarative approach, the business users were able to define load and transformation operations in a matter of minutes.\\n\\n__Smartlake Standout features__\\n\\n- Load almost any file format at Spark speed (CSV, JSON, XML, FIXED WITH, Multi-record types \u2026) or Kafka topic\\n\\n- Validate fields using user-defined schemas with user defined semantic types\\n\\n- Apply transformations on the fly to data being loaded (GDPR, normalisation, computed fields ...) with and without schema evolution\\n\\n- Sink to almost any target including Spark, Kafka, Elasticsearch.\\n\\n## Episode 2: Evolution to Starlake\\n\\n:::note\\n\\nThe basic idea behind Starlake was to bring in all Smartlake benefits to the cloud by leveraging serverless services and Cloud Datawarehouses capabilities while minimising development and execution costs.\\n\\n:::\\n\\nAs the data landscape evolved, so did our vision. Cloud data warehouses emerged as formidable competitors to Spark for query execution. Recognizing this shift, we evolved Smartlake into Starlake, preserving its declarative essence while embracing YAML for enhanced readability. We maintained Spark\u2019s prowess to run inside single or multiple container(s) for data ingestion, leveraging cloud data warehouses for query execution.\\n\\nThis strategic blend allowed us to optimize performance and cost-effectiveness based on specific workload requirements. The result was a reimagined platform, tailored for the cloud era, yet grounded in the principles of efficiency and simplicity that defined its inception.\\n\\nThe result is the Starlake OSS project that you can find on [Github](https://github.com/starlake-ai/starlake).\\n\\nThe capabilities of Starlake are extensively described [here](https://github.com/starlake-ai/starlake).\\n\\n## The people behind Starlake\\n\\nSmartlake, the precursor to Starlake, owes its existence to the collective efforts of numerous individuals, but a select few stand out for their exceptional contributions:\\n\\n- [Sam Bessalah](https://www.linkedin.com/in/samklr/?lipi=urn%3Ali%3Apage%3Ad_flagship3_publishing_post_edit%3BErFcjpiuROmx%2F%2BQ9qmKozQ%3D%3D) With Sam\u2019s presence, rallying others became effortless. His visionary outlook and knack for simplifying complexities proved transformative, setting a new standard for implementation.\\n\\n- [Olivier Girardot](https://www.linkedin.com/in/oliviergirardot/?lipi=urn%3Ali%3Apage%3Ad_flagship3_pulse_read%3By9rNw1mSTEqx928hK46n9w%3D%3D) Every team has its coding wizard, and Olivier filled that role impeccably. From leveraging Spark codegen to exploring mathematical frameworks like matryoshka, he pushed the boundaries, mentoring the team with his expertise spanning Docker, Ansible, Python, Scala and Spark internals.\\n\\n- [Valentin Kasas](https://www.linkedin.com/in/valentin-kasas-937a5837/?lipi=urn%3Ali%3Apage%3Ad_flagship3_pulse_read%3By9rNw1mSTEqx928hK46n9w%3D%3D) Valentin championed functional programming in Scala. Introducing concepts like recursion schemes, he empowered the team to craft code that was not just functional but also elegant and maintainable.\\n\\nAs the journey progressed towards the cloud, long time data experts joined and made Starlake what it is today:\\n\\n- [Bounkong Khamphousone](https://www.linkedin.com/in/bounkong/) The speed and efficiency of Starlake\u2019s extraction and load engines owe much to his contributions.\\n\\n- [Mohamad Kassir](https://www.linkedin.com/in/mohamad-kassir-\ud83d\udcca\u2601%EF%B8%8F-399b562b/) His direct involvement with customer projects and his in-depth knowledge of cloud platforms and business needs have been major assets in the evolution of Starlake.\\n\\n- [Abdelhamide EL ARIB](https://www.linkedin.com/in/elarib/?lipi=urn%3Ali%3Apage%3Ad_flagship3_publishing_post_edit%3BErFcjpiuROmx%2F%2BQ9qmKozQ%3D%3D) An early contributor to the load engine, this foresight and execution prowess played a significant role in shaping the platform\u2019s today capabilities.\\n\\n- [Stephane Manciot](https://www.linkedin.com/in/smanciot/?lipi=urn%3Ali%3Apage%3Ad_flagship3_publishing_post_edit%3BErFcjpiuROmx%2F%2BQ9qmKozQ%3D%3D) The developer behind Starlake\u2019s declarative workflows on top of Airflow and Dagster, was pivotal in shaping its operational backbone.\\n\\n- [Cyrille Ch\xe9p\xe9lov](https://www.linkedin.com/in/cyrille-ch\xe9p\xe9lov/?lipi=urn%3Ali%3Apage%3Ad_flagship3_publishing_post_edit%3BErFcjpiuROmx%2F%2BQ9qmKozQ%3D%3D) A master of codebase optimisation, Cyrille\u2019s rewrite efforts were instrumental in ensuring the reentrant nature of Starlake\u2019s API.\\n\\n\\n## The next journey\\n\\nToday with hundreds of gigabytes of data loaded and transformed daily into thousands of tables in various data warehouses, we can confidently say that Starlake is battle tested and ready for the most demanding data engineering & analytics challenges.\\n\\nAs Starlake is, and will always be open source, join us in building a supportive community. Your insights and feature requests aren\u2019t just welcome, they guide our roadmap.\\n\\nGet Started with Starlake:\\n\\n- Check-out the other [features](https://starlake-ai.github.io/starlake/)\\n\\n- Explore our [documentation](https://starlake-ai.github.io/starlake/docs/next/intro)\\n\\nJoin our community on [GitHub](https://github.com/starlake-ai/starlake)\\n\\n\\n\\nP.S. Please star the repository: https://github.com/starlake-ai/starlake. Also, any issue or enhancement with Starlake, please just report it. It will fall under the scope of gracious care taking of course."},{"id":"rls-cls-big-query","metadata":{"permalink":"/starlake/blog/rls-cls-big-query","editUrl":"https://github.com/starlake-ai/starlake/edit/master/docs/blog/2022-02-15-rls-cls-big-query.mdx","source":"@site/blog/2022-02-15-rls-cls-big-query.mdx","title":"Column  and Row Level Security in BigQuery","description":"Data exposition strategies","date":"2022-02-15T00:00:00.000Z","tags":[{"inline":true,"label":"Spark","permalink":"/starlake/blog/tags/spark"},{"inline":true,"label":"BigQuery","permalink":"/starlake/blog/tags/big-query"},{"inline":true,"label":"Dataproc","permalink":"/starlake/blog/tags/dataproc"},{"inline":true,"label":"Google Cloud","permalink":"/starlake/blog/tags/google-cloud"},{"inline":true,"label":"ETL","permalink":"/starlake/blog/tags/etl"},{"inline":true,"label":"Starlake","permalink":"/starlake/blog/tags/starlake"}],"readingTime":2.815,"hasTruncateMarker":false,"authors":[{"name":"Hayssam Saleh","title":"Starlake Core Team Member","url":"https://www.linkedin.com/in/hayssams/","imageURL":"https://s.gravatar.com/avatar/04aa2a859a66b52787bcba8c36beba8c.png"}],"frontMatter":{"slug":"rls-cls-big-query","title":"Column  and Row Level Security in BigQuery","author":"Hayssam Saleh","author_title":"Starlake Core Team Member","author_url":"https://www.linkedin.com/in/hayssams/","author_image_url":"https://s.gravatar.com/avatar/04aa2a859a66b52787bcba8c36beba8c.png","tags":["Spark","BigQuery","Dataproc","Google Cloud","ETL","Starlake"]},"unlisted":false,"prevItem":{"title":"Starlake OSS - Bringing Declarative Programming to Data Engineering and Analytics","permalink":"/starlake/blog/starlake-oss"},"nextItem":{"title":"Handling Dynamic Partitioning and Merge with Spark on BigQuery","permalink":"/starlake/blog/spark-big-query-partitioning"}},"content":"## Data exposition strategies\\n\\nData may be exposed using views or authorized views and more recently using Row / Column level security.\\n\\nHistorically, to restrict access on specific columns or rows in BigQuery, one can create a (authorized) view with a SQL request like the one below:\\n\\n![CLS / RLS using Views]( /img/blog/cls-rls-bigquery/cls-rls.png \\"CLS / RLS using Views\\")\\n\\n\\nBigQuery **Views** require to grant access for the end users to the table on top of which the view is created. To bypass that limitation,\\nBigQuery provide **Authorized views**. However, Authorized views come with the following restrictions:\\n\\n1.  The underlying table is accessed through the authorized view where the end user is impersonated,\\nloosing thus at the table level, the identity of the user making the request.\\n![Impersonation]( /img/blog/cls-rls-bigquery/cls-rls-impersonation.png \\"Impersonation\\")\\n\\n\\n2. Each restriction policy require to define a specific authorized view making it difficult to identify who has access to what ?\\n![Multiplication of Authorized Views]( /img/blog/cls-rls-bigquery/cls-rls-multiple-authorized.png \\"Multiplication of Authorized Views\\")\\n\\n\\n3. Authorized views need to be updated whenever a schema evolution on the underlying table bring in a sensitive field that need to be excluded or a field that need to be included in the view.\\nIn the example below, the new column \\"description\\" need to be added to the authorized view if we want it .\\n![Multiplication of Authorized Views]( /img/blog/cls-rls-bigquery/cls-rls-schema-evolution.png \\"Multiplication of Authorized Views\\")\\n\\n\\nThat\'s where Row Level Security and Column Level security features natively supported by BigQuery come in.\\n\\n## BigQuery Row Level Security\\n\\nRow Level Security restrict access to the rows based on the conditions set in the where clause using the custom SQL statement below:\\n\\n![RLS]( /img/blog/cls-rls-bigquery/rls-syntax.png \\"RLS\\")\\n\\n## Big Query Column Level Security\\n\\nColumn level security in BigQuery is managed using a taxonomy. This taxonomy is a hierarchy of policy tags\\ndescribing the table attributes or other resources. By assigning access rights to a tag,\\nwe restrict access to any resource tagged using this specific tag and this applies to BigQuery table fields.\\n\\nIn our example, restricting access to specific user/group/sa to the column `price` require the following steps:\\n\\n1. In Cloud Data Catalog/Policy Tags, create a Taxonomy. Note that `Enfore access control` should be checked.\\n\\n![CLS Taxonomy]( /img/blog/cls-rls-bigquery/cls-taxonomy.png \\"CLS Taxonomy\\")\\n\\n2. Assign permissions for each policy tag you defined\\n\\n![CLS Access]( /img/blog/cls-rls-bigquery/cls-tag-access.png \\"CLS Access\\")\\n\\n3. Tag restricted columns in the BigQuery schema editor.\\n![CLS Assign]( /img/blog/cls-rls-bigquery/cls-tag-assign.png \\"CLS Assign\\")\\n\\n:::tip\\nAssigning policy tags may be done using the `bq load/update` command line tool\\n:::\\n\\n## BigQuery RLS/CLS benefits\\n\\nUsing BigQuery row and column level security features bring several benefits:\\n- There is no need to create extra views\\n- Users use the same name for the table but with different access rights\\n- A company-wide taxonomy is defined allowing better Data Management\\n- Access rights to a new column in the table are automatically handled\\n\\n## A word about RLS and CLS in Starlake\\n\\nIngesting Data into BigQuery cannot be considered complete without taking into account the access level restrictions on the target table.\\nStarlake will handle for you all the scripting required to secure BigQuery rows and columns using a YAML declarative syntax to make sure\\nthat your tables are secured in BigQuery:\\n```yaml {2,11} title=\\"Declarative Row Level & Column Level Security\\"\\n  - name: \\"PRODUCT\\"\\n    rls:\\n      - name: \\"my-rls\\"\\n        predicate: \\"category like \'Food\'\\"\\n        grants:\\n          - \\"user:me@company.com\\"\\n          - \\"group:financegroup@company.com\\"\\n          - \\"sa:serviceacount@gserviceaccount.com\\"\\n    attributes:\\n      - name: \\"id\\"\\n        accessPolicy: PII\\n```"},{"id":"spark-big-query-partitioning","metadata":{"permalink":"/starlake/blog/spark-big-query-partitioning","editUrl":"https://github.com/starlake-ai/starlake/edit/master/docs/blog/2021-12-15-spark-big-query-partitioning.mdx","source":"@site/blog/2021-12-15-spark-big-query-partitioning.mdx","title":"Handling Dynamic Partitioning and Merge with Spark on BigQuery","description":"Data Loading strategies","date":"2021-12-15T00:00:00.000Z","tags":[{"inline":true,"label":"Spark","permalink":"/starlake/blog/tags/spark"},{"inline":true,"label":"BigQuery","permalink":"/starlake/blog/tags/big-query"},{"inline":true,"label":"Dataproc","permalink":"/starlake/blog/tags/dataproc"},{"inline":true,"label":"Google Cloud","permalink":"/starlake/blog/tags/google-cloud"},{"inline":true,"label":"ETL","permalink":"/starlake/blog/tags/etl"},{"inline":true,"label":"Starlake","permalink":"/starlake/blog/tags/starlake"}],"readingTime":6.28,"hasTruncateMarker":false,"authors":[{"name":"Hayssam Saleh","title":"Starlake Core Team Member","url":"https://www.linkedin.com/in/hayssams/","imageURL":"https://s.gravatar.com/avatar/04aa2a859a66b52787bcba8c36beba8c.png"}],"frontMatter":{"slug":"spark-big-query-partitioning","title":"Handling Dynamic Partitioning and Merge with Spark on BigQuery","author":"Hayssam Saleh","author_title":"Starlake Core Team Member","author_url":"https://www.linkedin.com/in/hayssams/","author_image_url":"https://s.gravatar.com/avatar/04aa2a859a66b52787bcba8c36beba8c.png","tags":["Spark","BigQuery","Dataproc","Google Cloud","ETL","Starlake"]},"unlisted":false,"prevItem":{"title":"Column  and Row Level Security in BigQuery","permalink":"/starlake/blog/rls-cls-big-query"},"nextItem":{"title":"Bonjour","permalink":"/starlake/blog/bonjour"}},"content":"import Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n\\n\\n## Data Loading strategies\\n\\nWhen loading data into BigQuery, you may want to:\\n- **Overwrite** the existing data and replace it with the incoming data.\\n- **Append** incoming data to existing\\n- **Dynamic partition Overwrite** where only the partitions to which the incoming data belong to are overwritten.\\n- **Merge** incoming data with existing data by keeping the newest version of each record.\\n\\nFor performance reasons, when having huge amount of data, tables are usually split\\ninto multiple partitions. BigQuery supports range partitioning which are uncommon and date/time partitioning\\nwhich is the most widely used type of partitioning.\\nThe diagram below shows our initial table partitioned by the `date` field.\\n\\n![Initial data]( /img/blog/spark-bigquery/initial-data.png \\"Initial data\\")\\n\\n\\nLet\'s assume we receive the following data that we need to ingest into the table:\\n\\n![Incoming data]( /img/blog/spark-bigquery/incoming-data.png \\"Incoming data\\")\\n\\nThe strategies above will produce respectively the results below:\\n\\n<Tabs groupId=\\"modes\\">\\n<TabItem value=\\"overwrite\\" label=\\"Overwrite\\">\\n\\nThe table ends up with the 2 incoming records.\\nAll existing partitions are deleted.\\n\\n![Overwrite data]( /img/blog/spark-bigquery/overwrite-data.png \\"Overwrite data\\")\\n\\n</TabItem>\\n<TabItem value=\\"append\\" label=\\"Append\\">\\n\\nThe table ends up with 7 records. Note that a new `\xectem 1` record is added while the older one is kept.\\n\\n![Append data]( /img/blog/spark-bigquery/append-data.png \\"Append data\\")\\n\\n</TabItem>\\n<TabItem value=\\"partitioned-overwrite\\" label=\\"Dynamic partition Overwrite\\">\\n\\nThe table ends up with 4 records.\\nThe second partition remains untouched while the first partition is erased and overwritten by with the incoming data.\\n\\n![Merge data]( /img/blog/spark-bigquery/partitioned-overwrite.png \\"Merge data\\")\\n\\n</TabItem>\\n<TabItem value=\\"merge\\" label=\\"Merge\\">\\n\\nThe table ends up with 4 records.\\nIncoming and existing records are added up but only the newest version of each product in the kept in the resulting table.\\n\\n![Merge data]( /img/blog/spark-bigquery/merge-data.png \\"Merge data\\")\\n\\n</TabItem>\\n</Tabs>\\n\\nThere is no good or bad strategy, the use of one of the strategies above depends on the use case. Some use case examples for each of the strategies are:\\n- Overwrite mode may be useful when you receive every day the list of all product names.\\n- Append mode may be useful when you receive daily sales.\\n- Dynamic Partition Overwrite mode may be useful when you ingested the first time a partition, and you need to ingest it again with a different set of data and thus alter only that partition.\\n- Merge mode may be useful when you receive product updates every day and that you need to keep only the last version of each product.\\n\\n\\n# Spark How-to\\n\\n[Apache Spark SQL connector for Google BigQuery](https://github.com/GoogleCloudDataproc/spark-bigquery-connector) makes BigQuery a first class citizen as a source and sink for Spark jobs.\\n\\n## Append and Overwrite modes in Spark\\n\\nBigQuery is supported by Spark as a source and sink through the [Spark BigQuery connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector)\\n\\nSpark comes out of the box with the ability to append or overwrite existing data using a predefined save mode:\\n\\n```scala\\n\\nval incomingDF = ... // Incoming data loaded with the correct schema\\nval bqTable = \\"project-id.dataset.table\\"\\nval saveMode = SaveMode.Overwrite // or SaveMode.Append fot he appending data\\nincomingDF.write\\n    .mode(saveMode)\\n    .partitionBy(\\"date\\")\\n    .format(\\"com.google.cloud.spark.bigquery\\")\\n    .option(\\"table\\", bqTable)\\n    .save()\\n\\n```\\n\\n### Dynamic Partition Overwrite mode in Spark\\n\\nTo activate dynamic partitioning, you need to set the configuration below before saving the data using the exact same code above:\\n```scala\\nspark.conf.set(\\"spark.sql.sources.partitionOverwriteMode\\",\\"DYNAMIC\\")\\n```\\n\\nUnfortunately, the BigQuery Spark connector does not support this feature (at the time of writing).\\nWe need to manually delete the partitions we want to overwrite first and then append the incoming data.\\n\\nAssuming the table is partitioned by the field `date` and the incoming data loaded in the incomingDF dataframe, the code below will\\nremove existing partitions that need to be overwritten.\\n\\n```scala {3,8} title=\\"Delete partitions that need to be updated\\"\\nval incomingDF = ... // Incoming data loaded with the correct schema\\nincomingDF\\n  .select(date_format(col(\\"date\\"), \\"yyyyMMdd\\").cast(\\"string\\"))\\n  .distinct()\\n  .collect()\\n  .map(_.getString(0))\\n  .foreach { partition =>\\n    bigQueryClient.deleteTable(TableId.of(datasetName, s\\"$table\\\\$$partition\\"));\\n  }\\n```\\n\\n:::tip\\nTo drop a table partition using the Google Cloud `bq` command line tool, you may use the following syntax:\\n\\n```shell\\nbq rm -t \'project-id.dataset.table$YYYYMMDD\'\\n```\\n:::\\n\\nWe now need to append the incomingDF to mimic the `dynamic partition overwrite` feature:\\n\\n```scala {3} title=\\"Append incoming partitions\\"\\nval incomingDF = ... // Incoming data loaded with the correct schema\\nval bqTable = \\"project-id.dataset.table\\"\\nval saveMode = SaveMode.Append\\nincomingDF.write\\n    .mode(saveMode)\\n    .partitionBy(\\"date\\")\\n    .format(\\"com.google.cloud.spark.bigquery\\")\\n    .option(\\"table\\", bqTable)\\n    .save()\\n```\\n\\n:::caution\\nThe issue with this approach is that if the program crashes during the \\"appending\\" of the incoming data, partitions will have been deleted and data would be lost.\\nHowever, you can still ingest the same file again in case of failure and the end result will be the same.\\n:::\\n\\n\\n\\n\\n### Dynamic Partition Merge in Spark\\n\\nWhen you need to keep the last version of the record for each product, both BigQuery and Databricks (the company behind Spark in case you lived on the moon the last ten years) support\\nthe merge SQL statement:\\n\\n```sql {5,7} title=\\"Merge records using SQL statement\\"\\nMERGE INTO target_table\\nUSING incoming_table\\nON target_table.product = incoming_table.product\\n    WHEN NOT MATCHED\\n\\t\\tTHEN INSERT *\\n\\tWHEN MATCHED AND incoming_table.date > target_table.date THEN\\n\\t\\tUPDATE SET *\\n/*\\n    WHEN MATCHED AND incoming_table.timestamp <= target_table.timestamp THEN\\n\\t\\tSKIP\\n*/\\n```\\n\\nUnfortunately the `MERGE` statement is not supported by Apache Spark. It is only supported by Databricks, its commercial version.\\n\\nTo do a merge using the Spark BigQuery connector, we need to do it by following the steps below:\\n\\n\\n**Step 1: Create a dataframe with all the rows**\\n\\n```scala\\nval allRowsDF =\\n    incomingDF\\n    .unionByName(existingDF)\\n```\\n\\n![Step 1]( /img/blog/spark-bigquery/step1-union-data.png \\"Step 1\\")\\n\\n\\n**Step 2: group by product and order each product occurrence by date descending**\\n\\n```scala\\nval orderingWindow =\\n    Window\\n      .partitionBy(\\"product\\")\\n      .orderBy(col(\\"date\\").desc, col(\\"product\\")))\\n\\nval orderedDF =\\n    allRowsDF\\n        .withColumn(\\"rownum\\", row_number.over(orderingWindow))\\n```\\n\\n![Step 2]( /img/blog/spark-bigquery/step2-ordering-data.png \\"Step 2\\")\\n\\n\\n\\nIn the step 2 above, each product is ordered by date with the most recent one first (descending order).\\nWe identify it by the `rownum` column.\\n\\n\\n**Step 3: Keep the most recent product**\\n\\n```scala {3}\\nval toKeepDF =\\n    orderedDF\\n        .where(col(\\"rownum\\") === 1)\\n        .drop(\\"rownum\\")\\n```\\n\\n![Step 3]( /img/blog/spark-bigquery/step3-keep-data.png \\"Step 3\\")\\n\\n\\n**Step 4: Overwrite existing partitions with the data we want to keep**\\n\\n```scala {3}\\n\\nval bqTable = \\"project-id.dataset.table\\"\\nval saveMode = SaveMode.Overwrite\\ntoKeepDF.write\\n    .mode(saveMode)\\n    .partitionBy(\\"date\\")\\n    .format(\\"com.google.cloud.spark.bigquery\\")\\n    .option(\\"table\\", bqTable)\\n    .save()\\n```\\n\\n![Step 4]( /img/blog/spark-bigquery/step4-save-data.png \\"Step 4\\")\\n\\n## Starlake How-to\\n\\n[Starlake](https://starlake.io) is a declarative Ingestion Framework based on YAML description files.<br/>\\nThe 4 ingestion strategies described above are supported through the settings below:\\n\\n<Tabs groupId=\\"modes\\">\\n<TabItem value=\\"overwrite\\" label=\\"Overwrite\\">\\n\\n\\n```yaml {8,9} title=\\"Schema Definition File\\"\\n     name: \\"mydb\\"\\n     directory: \\"...\\"\\n+    metadata:\\n    schemas:\\n      - name: \\"mytable\\"\\n        pattern: \\"data-.*.csv\\"\\n        metadata:\\n          writeStrategy:\\n            type: \\"OVERWRITE\\"\\n       attributes:\\n          - name: \\"date\\"\\n            type: \\"date\\"\\n            rename: \\"id\\"\\n          - name: \\"product\\"\\n            type: \\"string\\"\\n          - name: \\"price\\"\\n            type: \\"decimal\\"\\n```\\n\\n[See again manual Spark overwrite](#append-and-overwrite-modes-in-spark)\\n\\n</TabItem>\\n<TabItem value=\\"append\\" label=\\"Append\\">\\n\\n\\n```yaml {8,9} title=\\"Schema Definition File\\"\\n     name: \\"mydb\\"\\n     directory: \\"...\\"\\n+    metadata:\\n    schemas:\\n      - name: \\"mytable\\"\\n        pattern: \\"data-.*.csv\\"\\n        metadata:\\n          writeStrategy:\\n            type: \\"APPEND\\"\\n        attributes:\\n          - name: \\"date\\"\\n            type: \\"date\\"\\n            rename: \\"id\\"\\n          - name: \\"product\\"\\n            type: \\"string\\"\\n          - name: \\"price\\"\\n            type: \\"decimal\\"\\n```\\n\\n[See again manual Spark append](#append-and-overwrite-modes-in-spark)\\n\\n</TabItem>\\n<TabItem value=\\"partitioned-overwrite\\" label=\\"Dynamic partition Overwrite\\">\\n\\n\\n```yaml {8-12} title=\\"Schema Definition File\\"\\n    name: \\"mydb\\"\\n    directory: \\"...\\"\\n+   metadata:\\n    schemas:\\n      - name: \\"mytable\\"\\n        pattern: \\"data-.*.csv\\"\\n        metadata:\\n          strategy:\\n            type: \\"OVERWRITE_BY_PARTITION\\"\\n          sink:\\n            partition:\\n              - \\"date\\"\\n       attributes:\\n          - name: \\"date\\"\\n            type: \\"date\\"\\n            rename: \\"id\\"\\n          - name: \\"product\\"\\n            type: \\"string\\"\\n          - name: \\"price\\"\\n            type: \\"decimal\\"\\n```\\n\\n[See again manual Spark dynamic partition overwrite](#dynamic-partition-overwrite-mode-in-spark)\\n\\n</TabItem>\\n<TabItem value=\\"merge\\" label=\\"Merge\\">\\n\\n\\n```yaml {8,11} title=\\"Schema Definition File\\"\\n     name: \\"mydb\\"\\n     directory: \\"...\\"\\n+    metadata:\\n    schemas:\\n      - name: \\"mytable\\"\\n        pattern: \\"data-.*.csv\\"\\n        metadata:\\n          strategy:\\n            type: \\"OVERWRITE_BY_KEY_AND_TIMESTAMP\\"\\n            key: [\\"product\\"]\\n            timestamp: \\"date\\"\\n        attributes:\\n          - name: \\"date\\"\\n            type: \\"date\\"\\n          - name: \\"product\\"\\n            type: \\"string\\"\\n          - name: \\"price\\"\\n            type: \\"decimal\\"\\n```\\n\\n[See again manual Spark Merge](#dynamic-partition-merge-in-spark)\\n\\n</TabItem>\\n</Tabs>"},{"id":"bonjour","metadata":{"permalink":"/starlake/blog/bonjour","editUrl":"https://github.com/starlake-ai/starlake/edit/master/docs/blog/2021-09-18-bonjour.md","source":"@site/blog/2021-09-18-bonjour.md","title":"Bonjour","description":"Pipelining fast data is big. Pipelining big data fast is bigger. :)","date":"2021-09-18T00:00:00.000Z","tags":[{"inline":true,"label":"bonjour","permalink":"/starlake/blog/tags/bonjour"},{"inline":true,"label":"Starlake","permalink":"/starlake/blog/tags/starlake"}],"readingTime":0.06,"hasTruncateMarker":false,"authors":[{"name":"Starlake Team","title":"Starlake Core Team","url":"https://github.com/starlake-ai","imageURL":"https://avatars.githubusercontent.com/u/89859410?s=200&v=4"}],"frontMatter":{"slug":"bonjour","title":"Bonjour","author":"Starlake Team","author_title":"Starlake Core Team","author_url":"https://github.com/starlake-ai","author_image_url":"https://avatars.githubusercontent.com/u/89859410?s=200&v=4","tags":["bonjour","Starlake"]},"unlisted":false,"prevItem":{"title":"Handling Dynamic Partitioning and Merge with Spark on BigQuery","permalink":"/starlake/blog/spark-big-query-partitioning"}},"content":"Pipelining fast data is big. Pipelining big data fast is bigger. :)"}]}}')}}]);