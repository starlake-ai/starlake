"use strict";(self.webpackChunkstarlake_docs=self.webpackChunkstarlake_docs||[]).push([[9868],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>f});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var i=n.createContext({}),u=function(e){var t=n.useContext(i),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},p=function(e){var t=u(e.components);return n.createElement(i.Provider,{value:t},e.children)},c="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,i=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),c=u(a),m=r,f=c["".concat(i,".").concat(m)]||c[m]||d[m]||o;return a?n.createElement(f,l(l({ref:t},p),{},{components:a})):n.createElement(f,l({ref:t},p))}));function f(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,l=new Array(o);l[0]=m;var s={};for(var i in t)hasOwnProperty.call(t,i)&&(s[i]=t[i]);s.originalType=e,s[c]="string"==typeof e?e:r,l[1]=s;for(var u=2;u<o;u++)l[u]=a[u];return n.createElement.apply(null,l)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},5162:(e,t,a)=>{a.d(t,{Z:()=>l});var n=a(7294),r=a(6010);const o={tabItem:"tabItem_Ymn6"};function l(e){let{children:t,hidden:a,className:l}=e;return n.createElement("div",{role:"tabpanel",className:(0,r.Z)(o.tabItem,l),hidden:a},t)}},4866:(e,t,a)=>{a.d(t,{Z:()=>v});var n=a(7462),r=a(7294),o=a(6010),l=a(2466),s=a(6550),i=a(1980),u=a(7392),p=a(12);function c(e){return function(e){return r.Children.map(e,(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:t,label:a,attributes:n,default:r}}=e;return{value:t,label:a,attributes:n,default:r}}))}function d(e){const{values:t,children:a}=e;return(0,r.useMemo)((()=>{const e=t??c(a);return function(e){const t=(0,u.l)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,a])}function m(e){let{value:t,tabValues:a}=e;return a.some((e=>e.value===t))}function f(e){let{queryString:t=!1,groupId:a}=e;const n=(0,s.k6)(),o=function(e){let{queryString:t=!1,groupId:a}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:t,groupId:a});return[(0,i._X)(o),(0,r.useCallback)((e=>{if(!o)return;const t=new URLSearchParams(n.location.search);t.set(o,e),n.replace({...n.location,search:t.toString()})}),[o,n])]}function k(e){const{defaultValue:t,queryString:a=!1,groupId:n}=e,o=d(e),[l,s]=(0,r.useState)((()=>function(e){let{defaultValue:t,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!m({value:t,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const n=a.find((e=>e.default))??a[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:t,tabValues:o}))),[i,u]=f({queryString:a,groupId:n}),[c,k]=function(e){let{groupId:t}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(t),[n,o]=(0,p.Nk)(a);return[n,(0,r.useCallback)((e=>{a&&o.set(e)}),[a,o])]}({groupId:n}),h=(()=>{const e=i??c;return m({value:e,tabValues:o})?e:null})();(0,r.useLayoutEffect)((()=>{h&&s(h)}),[h]);return{selectedValue:l,selectValue:(0,r.useCallback)((e=>{if(!m({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);s(e),u(e),k(e)}),[u,k,o]),tabValues:o}}var h=a(2389);const X={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function b(e){let{className:t,block:a,selectedValue:s,selectValue:i,tabValues:u}=e;const p=[],{blockElementScrollPositionUntilNextRender:c}=(0,l.o5)(),d=e=>{const t=e.currentTarget,a=p.indexOf(t),n=u[a].value;n!==s&&(c(t),i(n))},m=e=>{let t=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const a=p.indexOf(e.currentTarget)+1;t=p[a]??p[0];break}case"ArrowLeft":{const a=p.indexOf(e.currentTarget)-1;t=p[a]??p[p.length-1];break}}t?.focus()};return r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.Z)("tabs",{"tabs--block":a},t)},u.map((e=>{let{value:t,label:a,attributes:l}=e;return r.createElement("li",(0,n.Z)({role:"tab",tabIndex:s===t?0:-1,"aria-selected":s===t,key:t,ref:e=>p.push(e),onKeyDown:m,onClick:d},l,{className:(0,o.Z)("tabs__item",X.tabItem,l?.className,{"tabs__item--active":s===t})}),a??t)})))}function y(e){let{lazy:t,children:a,selectedValue:n}=e;const o=(Array.isArray(a)?a:[a]).filter(Boolean);if(t){const e=o.find((e=>e.props.value===n));return e?(0,r.cloneElement)(e,{className:"margin-top--md"}):null}return r.createElement("div",{className:"margin-top--md"},o.map(((e,t)=>(0,r.cloneElement)(e,{key:t,hidden:e.props.value!==n}))))}function g(e){const t=k(e);return r.createElement("div",{className:(0,o.Z)("tabs-container",X.tabList)},r.createElement(b,(0,n.Z)({},e,t)),r.createElement(y,(0,n.Z)({},e,t)))}function v(e){const t=(0,h.Z)();return r.createElement(g,(0,n.Z)({key:String(t)},e))}},7217:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>i,default:()=>f,frontMatter:()=>s,metadata:()=>u,toc:()=>c});var n=a(7462),r=(a(7294),a(3905)),o=a(4866),l=a(5162);const s={},i="Load",u={unversionedId:"quickstart/load",id:"quickstart/load",title:"Load",description:"In this section you will learn how to load and transform data using the samples files created when bootstrapping a new project.",source:"@site/docs/quickstart/100.load.mdx",sourceDirName:"quickstart",slug:"/quickstart/load",permalink:"/starlake/docs/next/quickstart/load",draft:!1,editUrl:"https://github.com/starlake-ai/starlake/edit/master/docs/docs/quickstart/100.load.mdx",tags:[],version:"current",sidebarPosition:100,frontMatter:{},sidebar:"starlakeSidebar",previous:{title:"Infer schemas",permalink:"/starlake/docs/next/quickstart/infer-schema"},next:{title:"Transform",permalink:"/starlake/docs/next/quickstart/transform"}},p={},c=[{value:"Load files",id:"load-files",level:2},{value:"Import step",id:"import-step",level:3},{value:"Load step",id:"load-step",level:3},{value:"Check the result",id:"check-the-result",level:3},{value:"Loading the data into a different store",id:"loading-the-data-into-a-different-store",level:3}],d={toc:c},m="wrapper";function f(e){let{components:t,...a}=e;return(0,r.kt)(m,(0,n.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"load"},"Load"),(0,r.kt)("p",null,"In this section you will learn how to load and transform data using the samples files created when ",(0,r.kt)("a",{parentName:"p",href:"bootstrap"},"bootstrapping a new project"),".\nYou will learn how to:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"load: validate file records and load data into you warehouse"),(0,r.kt)("li",{parentName:"ul"},"transform: apply transformation on your previously loaded data")),(0,r.kt)("h2",{id:"load-files"},"Load files"),(0,r.kt)("p",null,"Loading is a two step process: The optional ",(0,r.kt)("inlineCode",{parentName:"p"},"import")," step and the ",(0,r.kt)("inlineCode",{parentName:"p"},"load")," step."),(0,r.kt)("h3",{id:"import-step"},"Import step"),(0,r.kt)("p",null,"In this first step, Starlake will look at the ",(0,r.kt)("em",{parentName:"p"},"directory")," attribute value in the YAML files and look at the file that matches the expected patterns defined in the table definition files.\nIn our example, the directories are ",(0,r.kt)("inlineCode",{parentName:"p"},"{{incoming_path}}/sales")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"{{incoming_path}}/hr"),"."),(0,r.kt)("p",null,"the ",(0,r.kt)("inlineCode",{parentName:"p"},"import")," command moves the files that satisfy one table pattern from the ",(0,r.kt)("em",{parentName:"p"},"incoming")," folder to the ",(0,r.kt)("inlineCode",{parentName:"p"},"datasets/pending")," folder.\nFiles that do not satisfy any pattern won't be loaded and are moved to the ",(0,r.kt)("inlineCode",{parentName:"p"},"datasets/unresolved")," directory."),(0,r.kt)(o.Z,{groupId:"platforms",mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"linux_macos",label:"Linux/MacOS",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"$ cd $HOME/quickstart\n$ starlake import\n"))),(0,r.kt)(l.Z,{value:"windows",label:"Windows",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-powershell"},"c:\\users\\me\\quickstart> starlake import\n"))),(0,r.kt)(l.Z,{value:"docker",label:"Docker",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"$ docker run                                                  \\\n    -e SL_ROOT=/app/quickstart                                \\\n    -v $HOME/quickstart:/app/quickstart -it starlake import\n")))),(0,r.kt)("p",null,"The sample data files has now been moved to the ",(0,r.kt)("inlineCode",{parentName:"p"},"quickstart/datasets/pending/sales")," directory."),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"This step is optional and does not need to be run if your files directly arrive in the ",(0,r.kt)("inlineCode",{parentName:"p"},"datasets/pending/sales")," folder.\nAlso note that all these source directories may be ",(0,r.kt)("a",{parentName:"p",href:"../reference/configuration"},"redefined"),".")),(0,r.kt)("h3",{id:"load-step"},"Load step"),(0,r.kt)("p",null,"In this second step, each line in the file present in the ",(0,r.kt)("inlineCode",{parentName:"p"},"datasets/pending")," folder is checked against the schema described in the YAML file\nand its result is stored in the warehouse."),(0,r.kt)(o.Z,{groupId:"platforms",mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"linux_macos",label:"Linux/MacOS",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"$ cd $HOME/quickstart\n$ starlake load\n"))),(0,r.kt)(l.Z,{value:"windows",label:"Windows",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-powershell"},"c:\\users\\me\\starlake> starlake load\n"))),(0,r.kt)(l.Z,{value:"docker",label:"Docker",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"$ docker run                                                      \\\n    -e SL_ROOT=/app/quickstart                                    \\\n    -v $HOME/myproject:/app/quickstart -it starlake load\n")))),(0,r.kt)("p",null,"This will load the data files and since we chose the localFilesystem connection, they will be stored them as parquet files into the following folders:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"datasets/accepted")," for valid records"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"datasets/rejected")," for invalid records"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"datasets/unresolved")," for unrecognized files")),(0,r.kt)("admonition",{type:"caution"},(0,r.kt)("p",{parentName:"admonition"},"Starlake validate the data against the table's schema of the first pattern that match with the file name.\nHence, you must be careful regarding the pattern you set.\nMake sure that there is no overlap.")),(0,r.kt)("h3",{id:"check-the-result"},"Check the result"),(0,r.kt)("p",null,"You can check the result by running the following python script to read the parquet file from the project directory:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"import pandas as pd\nfilepath = 'datasets/accepted/sales/customers/'\ndf = pd.read_parquet(filepath)\ndf.head\n")),(0,r.kt)("h3",{id:"loading-the-data-into-a-different-store"},"Loading the data into a different store"),(0,r.kt)("p",null,"We just loaded our text file as a parquet file. This is a very common format for data scientists and analysts.\nThrough minimum extra configuration, we are able to run Starlake on top of any warehouse and have these datasets available as tables."),(0,r.kt)("p",null,"The examples below describe the extra configuration required in the metadata/application.sl.yml configuration file\nto load the data into  bigquery, databricks, redshift, snowflake or postgresql."),(0,r.kt)(o.Z,{groupId:"datawarehouse",mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"bigquery",label:"bigquery",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'application:\n  connectionRef: "bigquery"\n  loader: native\n  connections:\n    bigquery:\n      type: "bigquery"\n      options:\n        location: "EU" # EU or US or ..\n        authType: "APPLICATION_DEFAULT"\n        authScopes: "https://www.googleapis.com/auth/cloud-platform" # comma separated list of scopes\n        #authType: SERVICE_ACCOUNT_JSON_KEYFILE\n        #jsonKeyfile: "/Users/me/.gcloud/keys/starlake-me.json"\n        #authType: "ACCESS_TOKEN"\n        #gcpAccessToken: "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"\n'))),(0,r.kt)(l.Z,{value:"databricks",label:"databricks",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'application:\n  connectionRef: "databricks"\n  loader: spark # note the spark loader here\n  connections:\n    localFilesystem:\n      type: "databricks"\n'))),(0,r.kt)(l.Z,{value:"postgresql",label:"postgresql",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'application:\n  connectionRef: "postgresql"\n  loader: spark # note the spark loader here\n  connections:\n    postgresql:\n      type: jdbc\n      options:\n        url: "jdbc:postgresql://{{POSTGRES_HOST}}:{{POSTGRES_PORT}}/{{POSTGRES_DATABASE}}"\n        driver: "org.postgresql.Driver"\n        supportTruncateOnInsert: false\n        user: "{{DATABASE_USER}}"\n        password: "{{DATABASE_PASSWORD}}"\n        quoteIdentifiers: false\n'))),(0,r.kt)(l.Z,{value:"redshift",label:"redshift",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'application:\n  connectionRef: "reshift"\n  loader: spark # note the spark loader here\n  connections:\n    redshift:\n      sparkFormat: "com.databricks.spark.redshift"\n      options:\n        url: "jdbc:redshift://redshifthost:5439/database",\n        user: "username",\n        password: "pass",\n        tempdir: "s3n://path/for/temp/data",\n        aws_iam_role: "arn:aws:iam::123456789000:role/redshift_iam_role"\n'))),(0,r.kt)(l.Z,{value:"snowflake",label:"snowflake",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'application:\n  connectionRef: "snowflake"\n  loader: native\n  connections:\n    snowflake:\n      type: jdbc\n      options:\n        url: "jdbc:snowflake://{{SNOWFLAKE_ACCOUNT}}.snowflakecomputing.com/"\n        driver: "net.snowflake.client.jdbc.SnowflakeDriver"\n        user: "{{SNOWFLAKE_USER}}"\n        password: "{{SNOWFLAKE_PASSWORD}}"\n        warehouse: "{{SNOWFLAKE_WAREHOUSE}}"\n        db: "{{SNOWFLAKE_DB}}"\n        keep_column_case: "off"\n        preactions: "alter session set TIMESTAMP_TYPE_MAPPING = \'TIMESTAMP_LTZ\';ALTER SESSION SET QUOTED_IDENTIFIERS_IGNORE_CASE = true"\n'))),(0,r.kt)(l.Z,{value:"spark-bigquery",label:"spark bigquery",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'application:\n  connectionRef: "spark-bigquery"\n  loader: spark # note the spark loader here\n  connections:\n    spark-bigquery:\n      type: "bigquery"\n      options:\n        location: "EU" # EU or US or ..\n        authType: "APPLICATION_DEFAULT"\n        authScopes: "https://www.googleapis.com/auth/cloud-platform" # comma separated list of scopes\n        writeMethod: "direct" # direct or indirect (indirect is required for certain features see https://github.com/GoogleCloudDataproc/spark-bigquery-connector)\n        #gcsBucket: "starlake-app" # Temporary GCS Bucket where intermediary files will be stored. Required in indirect mode only\n        #authType: SERVICE_ACCOUNT_JSON_KEYFILE\n        #jsonKeyfile: "/Users/me/.gcloud/keys/starlake-me.json"\n        #authType: "ACCESS_TOKEN"\n        #gcpAccessToken: "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"\n'))),(0,r.kt)(l.Z,{value:"spark-snowflake",label:"spark snowflake",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'application:\n  connectionRef: "spark-snowflake"\n  loader: spark # note the spark loader here\n  connections:\n      spark-snowflake:\n        type: jdbc\n        sparkFormat: snowflake\n        options:\n          sfUrl: "{{SNOWFLAKE_ACCOUNT}}.snowflakecomputing.com" # make sure you do not prefix by jdbc:snowflake://. This is done by the snowflaek driver\n          #sfDriver: "net.snowflake.client.jdbc.SnowflakeDriver"\n          sfUser: "{{SNOWFLAKE_USER}}"\n          sfPassword: "{{SNOWFLAKE_PASSWORD}}"\n          sfWarehouse: "{{SNOWFLAKE_WAREHOUSE}}"\n          sfDatabase: "{{SNOWFLAKE_DB}}"\n          keep_column_case: "off"\n          autopushdown: on\n          preactions: "alter session set TIMESTAMP_TYPE_MAPPING = \'TIMESTAMP_LTZ\';ALTER SESSION SET QUOTED_IDENTIFIERS_IGNORE_CASE = true"\n\n')))),(0,r.kt)("p",null,"Using Spark instead of the BigQuery Load API may slow down the ingestion process but it has among others the following advantages:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"It allows to load data from any source supported by Spark including Fixed Width Files, XML files, JSON Arrays files ..."),(0,r.kt)("li",{parentName:"ul"},"It allows to load data into any destination supported by Spark including Snowflake, Amazon Redshift ..."),(0,r.kt)("li",{parentName:"ul"},"It allows to apply any transformation supported by Spark"),(0,r.kt)("li",{parentName:"ul"},"It allows to report any number of errors instead of 5 errors max with the BigQuery Load API (This is a BigQuery API Limitation)")),(0,r.kt)("p",null,"To load the data into BigQuery, simply put back the samples data files now archived in the datasets folder back\nto the sample-data/hr and sample-data/sales folders and run the ",(0,r.kt)("em",{parentName:"p"},"import")," and ",(0,r.kt)("em",{parentName:"p"},"load")," commands again."),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"Spark is not required to load data into any of the target datawarehouse, nor is it required to run transformations on the data."),(0,r.kt)("p",{parentName:"admonition"},"Even when using Spark, you do not need to instantiate a cluster.  Spark becomes useful for advanced data validation at load time.")))}f.isMDXComponent=!0}}]);