"use strict";(self.webpackChunkstarlake_docs=self.webpackChunkstarlake_docs||[]).push([[386],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>k});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function l(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?l(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},l=Object.keys(e);for(n=0;n<l.length;n++)a=l[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(n=0;n<l.length;n++)a=l[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),p=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},d=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,l=e.originalType,s=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),m=p(a),c=r,k=m["".concat(s,".").concat(c)]||m[c]||u[c]||l;return a?n.createElement(k,i(i({ref:t},d),{},{components:a})):n.createElement(k,i({ref:t},d))}));function k(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var l=a.length,i=new Array(l);i[0]=c;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o[m]="string"==typeof e?e:r,i[1]=o;for(var p=2;p<l;p++)i[p]=a[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},5867:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>u,frontMatter:()=>l,metadata:()=>o,toc:()=>p});var n=a(7462),r=(a(7294),a(3905));const l={sidebar_position:1,title:"Configuration"},i=void 0,o={unversionedId:"reference/configuration",id:"version-1.0.0/reference/configuration",title:"Configuration",description:"To run it with the default configuration, you simply launch it as follows :",source:"@site/versioned_docs/version-1.0.0/reference/1.configuration.md",sourceDirName:"reference",slug:"/reference/configuration",permalink:"/starlake/docs/reference/configuration",draft:!1,editUrl:"https://github.com/starlake-ai/starlake/edit/master/docs/versioned_docs/version-1.0.0/reference/1.configuration.md",tags:[],version:"1.0.0",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Configuration"},sidebar:"starlakeSidebar",previous:{title:"On Premise Hadoop Cluster",permalink:"/starlake/docs/platform/hadoop"},next:{title:"Environment",permalink:"/starlake/docs/reference/environment"}},s={},p=[{value:"Configuration",id:"configuration",level:2},{value:"application.sl.yml",id:"applicationslyml",level:3},{value:"Environment variables",id:"environment-variables",level:3},{value:"Configuration sections",id:"configuration-sections",level:2},{value:"Filesystem",id:"filesystem",level:3},{value:"Ingestion",id:"ingestion",level:3},{value:"Validation",id:"validation",level:3},{value:"Privacy",id:"privacy",level:3},{value:"Sinks",id:"sinks",level:3},{value:"BigQuery Sink",id:"bigquery-sink",level:4},{value:"Elasticsearch Sink",id:"elasticsearch-sink",level:4},{value:"Filesystem Sink",id:"filesystem-sink",level:4},{value:"Audit",id:"audit",level:3},{value:"Metrics",id:"metrics",level:3},{value:"Expectations",id:"expectations",level:3},{value:"Elasticsearch",id:"elasticsearch",level:3},{value:"Spark",id:"spark",level:3},{value:"Kafka",id:"kafka",level:3},{value:"JDBC",id:"jdbc",level:3},{value:"\xb5-service",id:"\xb5-service",level:3},{value:"Airflow",id:"airflow",level:3},{value:"Airflow DAGs",id:"airflow-dags",level:2},{value:"Import DAG",id:"import-dag",level:3},{value:"Watch DAG",id:"watch-dag",level:3},{value:"Ingestion DAG",id:"ingestion-dag",level:3}],d={toc:p},m="wrapper";function u(e){let{components:t,...a}=e;return(0,r.kt)(m,(0,n.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"To run it with the default configuration, you simply launch it as follows :"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"$ starlake.sh COMMAND [ARGS]\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"COMMAND: Any of the command described in the CLI section followed by optional arguments"),(0,r.kt)("li",{parentName:"ul"},"ARGS: Option list of command arguments")),(0,r.kt)("h2",{id:"configuration"},"Configuration"),(0,r.kt)("h3",{id:"applicationslyml"},"application.sl.yml"),(0,r.kt)("p",null,"You may also pass any Spark arguments as usual but also pass a custom ",(0,r.kt)("inlineCode",{parentName:"p"},"application.sl.yml")," file .\ndefault settings are found in the ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/starlake-ai/starlake/blob/master/src/main/resources/reference.conf"},"reference.conf"),"\nand ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/starlake-ai/starlake/tree/master/src/main/resources"},"reference-*.conf")," files. In your ",(0,r.kt)("inlineCode",{parentName:"p"},"application.sl.yml"),"file you only\nneed to redefine the variables you want to customize."),(0,r.kt)("p",null,"Some of those configurations may also be redefined through environment variables."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"In client mode: To pass those env vars, simply export / set them before calling starlake."),(0,r.kt)("li",{parentName:"ul"},"In cluster mode, you need to pass them as extra driver options.")),(0,r.kt)("p",null,"An ",(0,r.kt)("inlineCode",{parentName:"p"},"application.sl.yml")," file stored in the metadata subdirectory will be automatically loaded by Starlake. "),(0,r.kt)("h3",{id:"environment-variables"},"Environment variables"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"On Premise: To pass Starlake Data Pipeline env vars in cluster mode, you'll have to put them in the spark-defaults.conf file or pass them as arguments to your\nSpark job as described in this ",(0,r.kt)("a",{parentName:"p",href:"https://stackoverflow.com/questions/37887168/how-to-pass-environment-variables-to-spark-driver-in-cluster-mode-with-spark-sub"},"article"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"On Google Cloud: To make it available for all your jobs, you need to pass them in the ",(0,r.kt)("inlineCode",{parentName:"p"},"DataprocClusterCreateOperator")," using the ",(0,r.kt)("inlineCode",{parentName:"p"},"spark-env:"),"prefix\nas described in the example below:"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'    create_cluster = DataprocClusterCreateOperator(\n        task_id=\'create_dataproc_cluster\',\n        cluster_name=CLUSTER_NAME,\n        num_workers= \'${dataproc_cluster_size}\',\n        zone=ZONE,\n        region="${region}",\n        tags = ["dataproc"],\n        storage_bucket = "dataproc-${project_id}",\n        image_version=\'2.0.1-debian10\',\n        master_machine_type=MASTER_MACHINE_TYPE,\n        worker_machine_type=WORKER_MACHINE_TYPE,\n        service_account = "${service_account}",\n        internal_ip_only = True,\n        subnetwork_uri = "projects/${project_id}/regions/${region}/subnetworks/${subnet}",\n        properties = {\n            "spark-env:SL_FS": "gs://${my_bucket}",\n            "spark-env:SL_HIVE": "false",\n            "spark-env:SL_GROUPED": "false",\n            "spark-env:SL_AUDIT_SINK_TYPE": "BigQuerySink"\n            }\n    )\n')),(0,r.kt)("p",null,"In the example above, the variables are available in all the tasks that will be started on this cluster."),(0,r.kt)("p",null,"To set variables for specific tasks only, use a syntax similar to this one:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"t1 = dataproc_operator.DataProcSparkOperator(\n  task_id ='my_task',\n  dataproc_spark_jars='gs://my-bucket/starlake-spark3_2.12-VERSION-assembly.jar',\n  cluster_name='cluster',\n  main_class = 'ai.starlake.job.Main',\n  arguments=['import'],\n  project_id='my-project-id',\n  dataproc_spark_properties={'spark.driver.extraJavaOptions':'-DSL_FS=gs://${my_bucket} -DSL_HIVE=false -DSL_GROUPED=false'},\n  dag=dag)\n")),(0,r.kt)("p",null,"Starlake allows you to override some configurations properties using predefined env variables prefixed by ",(0,r.kt)("inlineCode",{parentName:"p"},"SL_"),".\nIn addition, you may override any configuration option by setting the JVM property ",(0,r.kt)("inlineCode",{parentName:"p"},"-Dconfig.override_with_env_vars=true")," and using the prefix\n",(0,r.kt)("inlineCode",{parentName:"p"},"CONFIG_FORCE_")," as explained below:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"prefix the property name with ",(0,r.kt)("inlineCode",{parentName:"li"},"CONFIG_FORCE_")),(0,r.kt)("li",{parentName:"ul"},"use single underscore ",(0,r.kt)("inlineCode",{parentName:"li"},"_")," for a dot ",(0,r.kt)("inlineCode",{parentName:"li"},".")),(0,r.kt)("li",{parentName:"ul"},"use double underscore ",(0,r.kt)("inlineCode",{parentName:"li"},"__")," for a dash ",(0,r.kt)("inlineCode",{parentName:"li"},"-")),(0,r.kt)("li",{parentName:"ul"},"use triple undercore ",(0,r.kt)("inlineCode",{parentName:"li"},"___"),"for a single underscore ",(0,r.kt)("inlineCode",{parentName:"li"},"_"))),(0,r.kt)("p",null,"For example, to redefine the property ",(0,r.kt)("inlineCode",{parentName:"p"},"metrics.discrete-max-cardinality")," with the value ",(0,r.kt)("inlineCode",{parentName:"p"},"100"),",\nyou need to set it as a JVM property using the syntax ",(0,r.kt)("inlineCode",{parentName:"p"},"-DCONFIG_FORCE_metrics_discrete__max__cardinality=100")),(0,r.kt)("p",null,"For more details, please refer to the official ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/lightbend/config#optional-system-or-env-variable-overrides"},"typesafeconfig")," documentation. "),(0,r.kt)("h2",{id:"configuration-sections"},"Configuration sections"),(0,r.kt)("h3",{id:"filesystem"},"Filesystem"),(0,r.kt)("p",null,"A filesystem is the location where datasets and Starlake Data Pipeline metadata used for ingestion are stored."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"On premise this reference the folder where datasets and metadata are stored, eq.",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"On a local filesystem: file://"),(0,r.kt)("li",{parentName:"ul"},"On a HDFS: hdfs://localhost:9000"))),(0,r.kt)("li",{parentName:"ul"},"In the cloud:",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"On Google Cloud Platform: gs://my-bucket"),(0,r.kt)("li",{parentName:"ul"},"On Microsoft Azure: abfs://",(0,r.kt)("a",{parentName:"li",href:"mailto:my-bucket@starlake.dfs.core.windows.net"},"my-bucket@starlake.dfs.core.windows.net")),(0,r.kt)("li",{parentName:"ul"},"On Amazon Web Service: s3a://my_bucket")))),(0,r.kt)("p",null,"By default, Starlake expect metadata in the /tmp/metadata folder and will store ingested datasets in the /tmp/datasets folder.\nBelow is how the folders look like by default for the provided quickstart sample."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    /tmp\n    |-- datasets (Root folder of ingested datasets)\n    |   |-- accepted (Root folder of all valid records)\n    |   |   |-- hr (domain name as specified in the name attribute of the /tmp/metadata/hr.yml)\n    |   |   |   `-- sellers (Schema name as specified in the /tmp/metadata/hr.yml)\n    |   |   |       |-- _SUCCESS\n    |   |   |       `-- part-00000-292c081b-7291-4797-b935-17bc9409b03b.snappy.parquet\n    |   |   `-- sales\n    |   |       |-- customers (valid records for this schema as specified in the /tmp/metadata/sales.yml)\n    |   |       |   |-- _SUCCESS\n    |   |       |   `-- part-00000-562501a1-34ef-4b94-b527-8e93bcbb5f89.snappy.parquet\n    |   |       `-- orders (valid records for this schema as specified in the /tmp/metadata/sales.yml)\n    |   |           |-- _SUCCESS\n    |   |           `-- part-00000-92544093-4ae2-4a98-8df8-a5aba19a1b27.snappy.parquet\n    |   |-- archive (Source files as found in the incoming folder are saved here after processing)\n    |   |   |-- hr (Domain name)\n    |   |   |   `-- sellers-2018-01-01.json\n    |   |   `-- sales\n    |   |       |-- customers-2018-01-01.psv\n    |   |       `-- orders-2018-01-01.csv\n    |   |-- business\n    |   |   |-- hr\n    |   |   `-- sales\n    |   |-- metrics\n    |   |   |-- discrete\n    |   |   |-- continuous\n    |   |   `-- frequencies\n    |   |-- ingesting (Temporary folder used during ingestion by Starlake)\n    |   |   |-- hr (One temporary subfolder / domain)\n    |   |   `-- sales\n    |   |-- pending (Source files are copied here from the incoming folder before processing)\n    |   |   |-- hr (one folder / domain)\n    |   |   `-- sales\n    |   |-- rejected (invalid records in processed datasets are stored here)\n    |   |   |-- hr (Domain name)\n    |   |   |   `-- sellers (Schema name)\n    |   |   |       |-- _SUCCESS\n    |   |   |       `-- part-00000-aef2dde6-af24-4e20-ad88-3e5238916e57.snappy.parquet\n    |   |   `-- sales\n    |   |       |-- customers\n    |   |       |   |-- _SUCCESS\n    |   |       |   `-- part-00000-e6fa5ff9-ad29-4e5f-a5ff-549dd331fafd.snappy.parquet\n    |   |       `-- orders\n    |   |           |-- _SUCCESS\n    |   |           `-- part-00000-6f7ba5d4-960b-4ac6-a123-87a7ab2d212f.snappy.parquet\n    |   `-- unresolved (Files found in the incoming folder but do not match any schema)\n    |       `-- hr\n    |           `-- dummy.json\n    `-- metadata (Root of metadata files)\n        |-- load (all domain definition files are located in this folder)\n        |   |-- hr/_config.sl.yml (One definition file / domain)\n        |   `-- hr/sales.yml\n        `-- assertions (All assertion definitions go here)\n        |   |-- default.sl.yml (Predefined assertion definitions)\n        |   `-- assertions.sl.yml (assertion definitions defined here are accessible throughout the project)\n        `-- types (All semantic types are defined here)\n        |   |-- default.sl.yml (Default semantic types)\n        |   `-- types.sl.yml (User defined semantic types, overwrite default ones)\n        `-- transform (All transform jobs go here)\n            `-- hr/sales_by_name.sql (Compute sales by )\n")),(0,r.kt)("p",null,"Starlake Data Pipeline allows you to store datasets and metadata in two different filesystems. Thi is useful if you want to define a specific lifecycle\nfor your datasets.\nAlmost all options are customizable through environnement variables.\nThe main env vars are described below, you may change default settings. The exhaustive list of predefined env vars can be found in the reference.conf file."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:"left"},"HOCON Variable"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Env variable"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Default Value"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"root"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_ROOT"),(0,r.kt)("td",{parentName:"tr",align:"left"},"/tmp"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Root directory of the datasets and metadata files in the defined filesystem above")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"datasets"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_DATASETS"),(0,r.kt)("td",{parentName:"tr",align:"left"},'${root}"/datasets"'),(0,r.kt)("td",{parentName:"tr",align:"left"},"Folder where datasets are located in the datasets ",(0,r.kt)("inlineCode",{parentName:"td"},"file-system"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"metadata"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_METADATA"),(0,r.kt)("td",{parentName:"tr",align:"left"},'${root}"/metadata" otherwise'),(0,r.kt)("td",{parentName:"tr",align:"left"},"Folder where metadata are located in the metadata ",(0,r.kt)("inlineCode",{parentName:"td"},"metadata-file-system"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"area.pending"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_AREA_PENDING"),(0,r.kt)("td",{parentName:"tr",align:"left"},"pending"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Source files are copied here from the incoming folder before processing")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"area.unresolved"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_AREA_UNRESOLVED"),(0,r.kt)("td",{parentName:"tr",align:"left"},"unresolved"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Files found in the incoming folder but do not match any schema")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"area.archive"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_AREA_ARCHIVE"),(0,r.kt)("td",{parentName:"tr",align:"left"},"archive"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Source files as found in the incoming folder are saved here after processing")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"area.ingesting"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_AREA_INGESTING"),(0,r.kt)("td",{parentName:"tr",align:"left"},"ingesting"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Temporary folder used during ingestion by Starlake")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"area.accepted"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_AREA_ACCEPTED"),(0,r.kt)("td",{parentName:"tr",align:"left"},"accepted"),(0,r.kt)("td",{parentName:"tr",align:"left"},"root folder of all valid records")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"area.rejected"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_AREA_REJECTED"),(0,r.kt)("td",{parentName:"tr",align:"left"},"rejected"),(0,r.kt)("td",{parentName:"tr",align:"left"},"invalid records in processed datasets are stored here")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"area.business"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_AREA_BUSINESS"),(0,r.kt)("td",{parentName:"tr",align:"left"},"business"),(0,r.kt)("td",{parentName:"tr",align:"left"},"root folder for all datasets produced by autojobs")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"archive"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_ARCHIVE"),(0,r.kt)("td",{parentName:"tr",align:"left"},"true"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Should we archive the incoming files once they are ingested")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"default-write-format"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_DEFAULT_WRITE_FORMAT"),(0,r.kt)("td",{parentName:"tr",align:"left"},"parquet"),(0,r.kt)("td",{parentName:"tr",align:"left"},"How accepted records are stored (parquet / orc / json / csv / avro)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"default-rejected-write-format"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_DEFAULT_REJECTED_WRITE_FORMAT"),(0,r.kt)("td",{parentName:"tr",align:"left"},"parquet"),(0,r.kt)("td",{parentName:"tr",align:"left"},"How rejected records are stored (parquet / orc / json / csv / avro)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"default-audit-write-format"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_DEFAULT_AUDIT_WRITE_FORMAT"),(0,r.kt)("td",{parentName:"tr",align:"left"},"parquet"),(0,r.kt)("td",{parentName:"tr",align:"left"},"How audit is stored (parquet / orc / json / csv / avro)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"hive"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_HIVE"),(0,r.kt)("td",{parentName:"tr",align:"left"},"true"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Should we create external Hive tables for ingested files ?")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"analyze"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_ANALYZE"),(0,r.kt)("td",{parentName:"tr",align:"left"},"true"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Should we computed basic statistics ? (requires SL_HIVE to be set to true)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"launcher"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_LAUNCHER"),(0,r.kt)("td",{parentName:"tr",align:"left"},"simple"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Which orchestrator to use ? Valid values are airflow or simple (direct call)")))),(0,r.kt)("p",null,"To make sure, the same schema is not ingested by two concurrent Starlake processes, Starlake Data Pipeline uses a file lock when necessary."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:"left"},"HOCON Variable"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Env variable"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Default Value"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"lock.path"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_LOCK_PATH"),(0,r.kt)("td",{parentName:"tr",align:"left"},'${root}"/locks"'),(0,r.kt)("td",{parentName:"tr",align:"left"},"Root folder where lock file is created")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"lock.timeout"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_LOCK_TIMEOUT"),(0,r.kt)("td",{parentName:"tr",align:"left"},"-1"),(0,r.kt)("td",{parentName:"tr",align:"left"},"How long to wait for the file lock to be available (in seconds)")))),(0,r.kt)("h3",{id:"ingestion"},"Ingestion"),(0,r.kt)("p",null,"When many files that have the same pattern and thus belong to the same schema, it is possible to ingest them one after the other using an ingestion policy\nor ingest all of them at once."),(0,r.kt)("p",null,"When ingesting the files with the same schema one after the other, it is possible to use a custom ordering policy by settings the ",(0,r.kt)("inlineCode",{parentName:"p"},"SL_LOAD_STRATEGY")," environment variable. Currently, the following ordering policies are defined:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"ai.starlake.job.load.IngestionTimeStrategy")," : Order the files by modification date"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"ai.starlake.job.load.IngestionNameStrategy")," : Order  the files by name")),(0,r.kt)("p",null,"If you want to use another custom strategy, you'll have to implement the trait below, make it available in the classpath and set the ",(0,r.kt)("inlineCode",{parentName:"p"},"SL_LOAD_STRATEGY")," environment variable"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'package ai.starlake.job.load\n\nimport java.time.LocalDateTime\n\nimport org.apache.hadoop.fs.{FileSystem, Path}\n\ntrait LoadStrategy {\n\n  /** List all files in folder\n    *\n    * @param fs        FileSystem\n    * @param path      Absolute folder path\n    * @param extension Files should end with this string. To list all files, simply provide an empty string\n    * @param since     Minimum modification time of list files. To list all files, simply provide the beginning of all times\n    * @param recursive List files recursively\n    * @return List of Path\n    */\n  def list(\n    fs: FileSystem,\n    path: Path,\n    extension: String = "",\n    since: LocalDateTime = LocalDateTime.MIN,\n    recursive: Boolean\n  ): List[Path]\n}\n')),(0,r.kt)("p",null,"To ingest all the files at once, set the ",(0,r.kt)("inlineCode",{parentName:"p"},"SL_GROUPED")," variable to true."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:"left"},"HOCON Variable"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Env variable"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Default Value"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"grouped"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_GROUPED"),(0,r.kt)("td",{parentName:"tr",align:"left"},"false"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Should files with the same schema be ingested all at once ?")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"load-strategy-class"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_LOAD_STRATEGY"),(0,r.kt)("td",{parentName:"tr",align:"left"},"ai.starlake.job.load.IngestionTimeStrategy"),(0,r.kt)("td",{parentName:"tr",align:"left"},"When ",(0,r.kt)("inlineCode",{parentName:"td"},"grouped")," is false, which ingestion order strategy to use")))),(0,r.kt)("p",null,"Below is an example of HOCON file with the default values."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-hocon"},'load-strategy-class = "ai.starlake.job.load.IngestionTimeStrategy"\nload-strategy-class = ${?SL_LOAD_STRATEGY}\n\ngrouped = false\ngrouped = ${?SL_GROUPED}\n')),(0,r.kt)("p",null,"The YAML/HOCON file describing the schema and ingestion rules may also define a custom sink (FS / JDBC / BigQuery / Redshift ...). "),(0,r.kt)("p",null,"In test mode, we need to sink the files to the filesystem. To enable sinking the resulting parquet file even when another sink type is desired, simply\nset the ",(0,r.kt)("inlineCode",{parentName:"p"},"SL_SINK_TO_FILE")," environment variable to ",(0,r.kt)("inlineCode",{parentName:"p"},"true"),"."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:"left"},"HOCON Variable"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Env variable"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Default Value"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"sink-to-file"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_SINK_TO_FILE"),(0,r.kt)("td",{parentName:"tr",align:"left"},"false"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Should ingested files be stored on the filesystem on only in the sink defined in the YAML file ?")))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-hocon"},"sink-to-file = false\nsink-to-file = ${?SL_SINK_TO_FILE}\n")),(0,r.kt)("p",null,"When ",(0,r.kt)("inlineCode",{parentName:"p"},"sink to file")," or a filesystem sink (SinkType.FS) is requested, and you want to output the result in a single file in the csv file format, set the ",(0,r.kt)("inlineCode",{parentName:"p"},"SL_CSV_OUTPUT"),"\nenvironment variable to ",(0,r.kt)("inlineCode",{parentName:"p"},"true"),"."),(0,r.kt)("h3",{id:"validation"},"Validation"),(0,r.kt)("p",null,"During ingestion, the input file is validated up to the attribute level. Three default row validators are defined:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"ai.starlake.job.validator.FlatRowValidator: to validate flat files, eq. DSV, Position and single level Json files."),(0,r.kt)("li",{parentName:"ul"},"ai.starlake.job.validator.TreeRowValidator:  used for tree like documents, eq. XML and JSON files"),(0,r.kt)("li",{parentName:"ul"},"ai.starlake.job.validator.AcceptAllValidator: used for any document type (flat and tree like) and accept the input without any validation")),(0,r.kt)("p",null,"The validtor to use is configurable as follows:"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:"left"},"HOCON Variable"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Env. variable"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Default value"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"row-validator-class"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_ROW_VALIDATOR_CLASS"),(0,r.kt)("td",{parentName:"tr",align:"left"},"ai.starlake.job.validator.FlatRowValidator")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"tree-validator-class"),(0,r.kt)("td",{parentName:"tr",align:"left"},"SL_TREE_VALIDATOR_CLASS"),(0,r.kt)("td",{parentName:"tr",align:"left"},"ai.starlake.job.validator.TreeRowValidator")))),(0,r.kt)("h3",{id:"privacy"},"Privacy"),(0,r.kt)("p",null,"Default valid values are NONE, HIDE, MD5, SHA1, SHA256, SHA512, AES(not implemented).\nCustom values may also be defined by adding a new privacy option in the application.conf.\nThe default reference.conf file defines the following valid privacy strategies:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-hocon"},'privacy {\n  options = {\n    "none": "ai.starlake.privacy.No",\n    "hide": "ai.starlake.privacy.Hide",\n    "hide10X": "ai.starlake.privacy.Hide(\\"X\\",10)",\n    "approxLong20": "ai.starlake.privacy.ApproxLong(20)",\n    "md5": "ai.starlake.privacy.Md5",\n    "sha1": "ai.starlake.privacy.Sha1",\n    "sha256": "ai.starlake.privacy.Sha256",\n    "sha512": "ai.starlake.privacy.Sha512",\n    "initials": "ai.starlake.privacy.Initials"\n  }\n}\n')),(0,r.kt)("p",null,"In the YAML/HOCON file, reference, you reference the option name. This will apply the function defined in the class referenced by the option value."),(0,r.kt)("p",null,"Below the predefined strategies:"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:"left"},"Privacy Strategy"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Privacy class"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"none"),(0,r.kt)("td",{parentName:"tr",align:"left"},"ai.starlake.privacy.No"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Return the input string itself")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"hide"),(0,r.kt)("td",{parentName:"tr",align:"left"},'ai.starlake.privacy.Hide(\\"X\\", 10)'),(0,r.kt)("td",{parentName:"tr",align:"left"},"Without a parameter, return the empty string. Otherwise, replace with 10 occurrences of the character 'X'")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"md5"),(0,r.kt)("td",{parentName:"tr",align:"left"},"ai.starlake.privacy.Md5"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Return the md5 of the input string")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"sha1"),(0,r.kt)("td",{parentName:"tr",align:"left"},"ai.starlake.privacy.Sha1"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Return the sha1 of the input string")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"sha256"),(0,r.kt)("td",{parentName:"tr",align:"left"},"ai.starlake.privacy.Sha256"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Return the sha256 of the input string")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"sha512"),(0,r.kt)("td",{parentName:"tr",align:"left"},"ai.starlake.privacy.Sha512"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Return the sha256 of the input string")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"initials"),(0,r.kt)("td",{parentName:"tr",align:"left"},"ai.starlake.privacy.Initials"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Return the first char of each word (usually applied to user names)")))),(0,r.kt)("p",null,"The following startegies are also defined and may be declared in the custom configuration file."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:"left"},"Privacy class"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"ai.starlake.privacy.IPv4(8)"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Return the IPv4 address with the last 8 bytes masked")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"ai.starlake.privacy.IPv6(8"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Return the IPv6 address with the last 8 bytes masked")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"ai.starlake.privacy.RandomDouble"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Return a random double number")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"ai.starlake.privacy.RandomDouble(10,20)"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Return a random double between 10.0 and 20.0")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"ai.starlake.privacy.RandomLong"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Return a random long number")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"ai.starlake.privacy.RandomLong(10, 20)"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Return a random long number between 10 and 20")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"ai.starlake.privacy.RandomInt"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Return a random int number")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"ai.starlake.privacy.RandomInt(10, 20)"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Return a random int number between 10 and 20")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"ai.starlake.privacy.ApproxDouble(70)"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Return a double value with a variation up to 70% applied to the input value")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"ai.starlake.privacy.ApproxLong(70)"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Return a double long with a variation up to 70% applied to the input value")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},'ai.starlake.privacy.Mask(\\"*\\", 4, 1, 3)'),(0,r.kt)("td",{parentName:"tr",align:"left"},"Partially mask the input value with 4 occurrences of the '*' character, 1 on the left side and 3 on the right side.")))),(0,r.kt)("p",null,"Any new privacy strategy should implement the following trait :"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},"/** @param s: String  => Input string to encrypt\n  * @param colMap : Map[String, Option[String]] => Map of all the attributes and their corresponding values\n  * @param params: List[Any]  => Parameters passed to the algorithm as defined in the conf file.\n  *                               Parameter starting with '\"' is converted to a string\n  *                               Parameter containing a '.' is converted to a double\n  *                               Parameter equals to true of false is converted a boolean\n  *                               Anything else is converted to an int\n  * @return The encrypted string\n  */\n")),(0,r.kt)("h3",{id:"sinks"},"Sinks"),(0,r.kt)("h4",{id:"bigquery-sink"},"BigQuery Sink"),(0,r.kt)("p",null,"When type field is set to ",(0,r.kt)("inlineCode",{parentName:"p"},"BigQuerySink")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:"left"},"Property"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Type"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Default Value"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"name"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Optional String"),(0,r.kt)("td",{parentName:"tr",align:"left"},"None"),(0,r.kt)("td",{parentName:"tr",align:"left"})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"location"),(0,r.kt)("td",{parentName:"tr",align:"left"},"String"),(0,r.kt)("td",{parentName:"tr",align:"left"},"EU"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Database location (EU, US, ...)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"timestamp"),(0,r.kt)("td",{parentName:"tr",align:"left"},"String"),(0,r.kt)("td",{parentName:"tr",align:"left"},"None"),(0,r.kt)("td",{parentName:"tr",align:"left"},"The timestamp column to use for table partitioning if any. No partitioning by default")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"clustering"),(0,r.kt)("td",{parentName:"tr",align:"left"},"List"),(0,r.kt)("td",{parentName:"tr",align:"left"},"None"),(0,r.kt)("td",{parentName:"tr",align:"left"},"List of ordered columns to use for table clustering")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"days"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Int"),(0,r.kt)("td",{parentName:"tr",align:"left"},"None"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Number of days before this table is set as expired and deleted. Never by default.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"requirePartitionFilter"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Boolean"),(0,r.kt)("td",{parentName:"tr",align:"left"},"false"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Should be require a partition filter on every request ? No by default.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"options"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Map"),(0,r.kt)("td",{parentName:"tr",align:"left"},"None"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Spark or BigQuery (depend on the selected engine) options to be set on the BigQuery connection")))),(0,r.kt)("h4",{id:"elasticsearch-sink"},"Elasticsearch Sink"),(0,r.kt)("p",null,"When type field is set to ",(0,r.kt)("inlineCode",{parentName:"p"},"EsSink")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:"left"},"Property"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Type"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Default Value"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"name"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Optional String"),(0,r.kt)("td",{parentName:"tr",align:"left"},"None"),(0,r.kt)("td",{parentName:"tr",align:"left"})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"id"),(0,r.kt)("td",{parentName:"tr",align:"left"},"String"),(0,r.kt)("td",{parentName:"tr",align:"left"},"None"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Attribute to use as id of the document. Generated by Elasticseach if not specified.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"timestamp"),(0,r.kt)("td",{parentName:"tr",align:"left"},"String"),(0,r.kt)("td",{parentName:"tr",align:"left"},"None"),(0,r.kt)("td",{parentName:"tr",align:"left"},'Timestamp field format as expected by Elasticsearch ("{beginTs',"|",'yyyy.MM.dd}" for example).')),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"options"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Map"),(0,r.kt)("td",{parentName:"tr",align:"left"},"None"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Elasticsearch options to be set on the ES connection")))),(0,r.kt)("h4",{id:"filesystem-sink"},"Filesystem Sink"),(0,r.kt)("p",null,"When type field is set to ",(0,r.kt)("inlineCode",{parentName:"p"},"FsSink"),". FsSink est the default sink type when ingesting data.\nThe file where data is saved is computed using the domain and schema name. See ",(0,r.kt)("a",{parentName:"p",href:"/starlake/docs/userguide/load"},"Load")," and ",(0,r.kt)("a",{parentName:"p",href:"/starlake/docs/userguide/transform"},"Transform")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:"left"},"Property"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Type"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Default Value"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"name"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Optional String"),(0,r.kt)("td",{parentName:"tr",align:"left"},"None"),(0,r.kt)("td",{parentName:"tr",align:"left"})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"connection"),(0,r.kt)("td",{parentName:"tr",align:"left"},"String"),(0,r.kt)("td",{parentName:"tr",align:"left"},"None"),(0,r.kt)("td",{parentName:"tr",align:"left"},"JDBC Connection String")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"options"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Map"),(0,r.kt)("td",{parentName:"tr",align:"left"},"None"),(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("a",{parentName:"td",href:"https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html"},"JDBC Options"))))),(0,r.kt)("h3",{id:"audit"},"Audit"),(0,r.kt)("h3",{id:"metrics"},"Metrics"),(0,r.kt)("p",null,"During ingestion, Starlake may produce metrics for any attribute in the dataset. Currently, only top level attributes are supported.\nOne of the two available metric type may be specified on an attribute: continuous and discrete.\nWhen the ",(0,r.kt)("inlineCode",{parentName:"p"},"metric")," property is set to ",(0,r.kt)("inlineCode",{parentName:"p"},"continuous"),", Starlake will compute for this attribute the following metrics:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"minimum value"),(0,r.kt)("li",{parentName:"ul"},"maximum value"),(0,r.kt)("li",{parentName:"ul"},"sum of all values"),(0,r.kt)("li",{parentName:"ul"},"mean: The arithmetic average"),(0,r.kt)("li",{parentName:"ul"},'median: the value separating the higher half from the lower half, may be thought of as "the middle" value'),(0,r.kt)("li",{parentName:"ul"},"variance: How far the values are spread out from their average value."),(0,r.kt)("li",{parentName:"ul"},"standard deviation: square root of the variance, the standard deviation measures how spread out numbers are in a data set"),(0,r.kt)("li",{parentName:"ul"},"missing values"),(0,r.kt)("li",{parentName:"ul"},"skewness: The measure of the asymmetry of the probability distribution. Negative skew commonly indicates that the tail is on the left side of the distribution, and positive skew indicates that the tail is on the right."),(0,r.kt)("li",{parentName:"ul"},"kurtosis: It tells us the extent to which the distribution is more or less outlier-prone (heavier or light-tailed) than the normal distribution. The greater the kurtosis, the less precise the standard deviation and variance become."),(0,r.kt)("li",{parentName:"ul"},"25th percentile: Returns the approximate ",(0,r.kt)("inlineCode",{parentName:"li"},"25 percentile")," of this attribute which is the smallest value in the ordered attribute values (sorted from least to greatest) such that no more than ",(0,r.kt)("inlineCode",{parentName:"li"},"25%")," of attribute values is less than the value or equal to that value"),(0,r.kt)("li",{parentName:"ul"},"75 percentile: Returns the approximate ",(0,r.kt)("inlineCode",{parentName:"li"},"75 percentile")," of this attribute which is the smallest value in the ordered attribute values (sorted from least to greatest) such that no more than ",(0,r.kt)("inlineCode",{parentName:"li"},"75%")," of attribute values is less than the value or equal to that value"),(0,r.kt)("li",{parentName:"ul"},"row count ")),(0,r.kt)("p",null,"When the ",(0,r.kt)("inlineCode",{parentName:"p"},"metric")," property is set to ",(0,r.kt)("inlineCode",{parentName:"p"},"discrete"),", Starlake will compute for this attribute the following metrics:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"count distinct: The number of distinct values for this attribute"),(0,r.kt)("li",{parentName:"ul"},"category frequency: The frequency (percentage) for each distinct value for this attribute"),(0,r.kt)("li",{parentName:"ul"},"category count: The number of occurrences for each distinct value for this attribute"),(0,r.kt)("li",{parentName:"ul"},"row count")),(0,r.kt)("p",null,"Each metric is computed for each attribute only on the incoming dataset and stored in a table with the ingestion time allowing to compare metric values between loads."),(0,r.kt)("p",null,"Assuming we are ingesting a file with the following schema:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"|-- business_id: string (nullable = false)\n|-- name: string (nullable = true)\n|-- address: string (nullable = true)\n|-- city: string (nullable = true) \n|-- state: string (nullable = true)\n|-- postal_code: string (nullable = true)\n|-- latitude: double (nullable = true)\n|-- longitude: double (nullable = true)\n|-- stars: double (nullable = true)\n|-- review_count: long (nullable = true)\n|-- is_open: long (nullable = true)\n")),(0,r.kt)("p",null,"with the attributes ",(0,r.kt)("inlineCode",{parentName:"p"},"city")," is marked as discrete and ",(0,r.kt)("inlineCode",{parentName:"p"},"review_count")," is marked as continuous"),(0,r.kt)("p",null,"The following tables would be generated:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"+-----------+-------------+---------------------+-----------+-------------------+------+--------+-----+-------------+----------+\n|attribute  |countDistinct|missingValuesDiscrete|cometMetric|jobId              |domain|schema  |count|cometTime    |cometStage|\n+-----------+-------------+---------------------+-----------+-------------------+------+--------+-----+-------------+----------+\n|city       |53           |0                    |Discrete   |local-1650471634299|yelp  |business|200  |1650471642737|UNIT      |\n+-----------+-------------+---------------------+-----------+-------------------+------+--------+-----+-------------+----------+\n\n+------------+---+-----+------+-------------+--------+-----------+------+--------+--------+------------+------+------------+-----------+-------------------+------+--------+-----+-------------+----------+\n|attribute   |min|max  |mean  |missingValues|variance|standardDev|sum   |skewness|kurtosis|percentile25|median|percentile75|cometMetric|jobId              |domain|schema  |count|cometTime    |cometStage|\n+------------+---+-----+------+-------------+--------+-----------+------+--------+--------+------------+------+------------+-----------+-------------------+------+--------+-----+-------------+----------+\n|review_count|3.0|664.0|38.675|0            |7974.944|89.303     |7735.0|4.359   |21.423  |5.0         |9.0   |25.0        |Continuous |local-1650471634299|yelp  |business|200  |1650471642737|UNIT      |\n+------------+---+-----+------+-------------+--------+-----------+------+--------+--------+------------+------+------------+-----------+-------------------+------+--------+-----+-------------+----------+\n\n+---------+---------------+-----+---------+-------------------+------+--------+-------------+----------+\n|attribute|category       |count|frequency|jobId              |domain|schema  |cometTime    |cometStage|\n+---------+---------------+-----+---------+-------------------+------+--------+-------------+----------+\n|city     |Tempe          |200  |0.01     |local-1650471634299|yelp  |business|1650471642737|UNIT      |\n|city     |North Las Vegas|200  |0.01     |local-1650471634299|yelp  |business|1650471642737|UNIT      |\n|city     |Phoenix        |200  |0.085    |local-1650471634299|yelp  |business|1650471642737|UNIT      |\n|city     |West Mifflin   |200  |0.005    |local-1650471634299|yelp  |business|1650471642737|UNIT      |\n|city     |Newmarket      |200  |0.005    |local-1650471634299|yelp  |business|1650471642737|UNIT      |\n|city     |Wickliffe      |200  |0.005    |local-1650471634299|yelp  |business|1650471642737|UNIT      |\n|city     |McKeesport     |200  |0.005    |local-1650471634299|yelp  |business|1650471642737|UNIT      |\n|city     |Scottsdale     |200  |0.06     |local-1650471634299|yelp  |business|1650471642737|UNIT      |\n|city     |Scarborough    |200  |0.005    |local-1650471634299|yelp  |business|1650471642737|UNIT      |\n|city     |Wexford        |200  |0.005    |local-1650471634299|yelp  |business|1650471642737|UNIT      |\n|city     |Willoughby     |200  |0.005    |local-1650471634299|yelp  |business|1650471642737|UNIT      |\n|city     |Chandler       |200  |0.02     |local-1650471634299|yelp  |business|1650471642737|UNIT      |\n|city     |Surprise       |200  |0.005    |local-1650471634299|yelp  |business|1650471642737|UNIT      |\n|city     |Cleveland      |200  |0.005    |local-1650471634299|yelp  |business|1650471642737|UNIT      |\n|city     |Litchfield Park|200  |0.005    |local-1650471634299|yelp  |business|1650471642737|UNIT      |\n|city     |Verona         |200  |0.005    |local-1650471634299|yelp  |business|1650471642737|UNIT      |\n|city     |Richmond Hill  |200  |0.01     |local-1650471634299|yelp  |business|1650471642737|UNIT      |\n|city     |Hudson         |200  |0.005    |local-1650471634299|yelp  |business|1650471642737|UNIT      |\n|city     |Etobicoke      |200  |0.01     |local-1650471634299|yelp  |business|1650471642737|UNIT      |\n|city     |Cuyahoga Falls |200  |0.005    |local-1650471634299|yelp  |business|1650471642737|UNIT      |\n|city     |.............. |...  |.....    |local-1650471634299|yelp  |business|1650471642737|UNIT      |\n+---------+---------------+-----+---------+-------------------+------+--------+-------------+----------+\n")),(0,r.kt)("h3",{id:"expectations"},"Expectations"),(0,r.kt)("h3",{id:"elasticsearch"},"Elasticsearch"),(0,r.kt)("h3",{id:"spark"},"Spark"),(0,r.kt)("h3",{id:"kafka"},"Kafka"),(0,r.kt)("h3",{id:"jdbc"},"JDBC"),(0,r.kt)("h3",{id:"\xb5-service"},"\xb5-service"),(0,r.kt)("h3",{id:"airflow"},"Airflow"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:"left"},"HOCON Variable"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Env variable"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Default Value"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"}),(0,r.kt)("td",{parentName:"tr",align:"left"},"AIRFLOW_ENDPOINT"),(0,r.kt)("td",{parentName:"tr",align:"left"},"Airflow endpoint. Used when SL_LAUNCHER is set to airflow"),(0,r.kt)("td",{parentName:"tr",align:"left"},(0,r.kt)("a",{parentName:"td",href:"http://127.0.0.1:8080/api/experimental"},"http://127.0.0.1:8080/api/experimental"))))),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Env. Var"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"),(0,r.kt)("th",{parentName:"tr",align:null},"Default value")))),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},"When running Spark on YARN in cluster mode,\nenvironment variables need to be set using the syntax spark.yarn.appMasterEnv.","[EnvironmentVariableName]")),(0,r.kt)("admonition",{type:"note"},(0,r.kt)("p",{parentName:"admonition"},'When running Dataproc on GCP, environment variables need to be set\nin the DataprocClusterCreateOperator in the properties attributes\nusing the syntax "spark-env:',"[EnvironmentVariableName]",'":"',"[Value]",'"')),(0,r.kt)("h2",{id:"airflow-dags"},"Airflow DAGs"),(0,r.kt)("p",null,"Starlake Data Pipeline comes with native  Airflow support.\nBelow are DAG definitions for each of the three ingestion steps on an kerberized cluster."),(0,r.kt)("h3",{id:"import-dag"},"Import DAG"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\n\n\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2018, 11, 2),\n    'email': ['me@here.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n    'retry_delay': timedelta(minutes=5),\n\n}\n\ndag = DAG('comet_import',max_active_runs=1, catchup=False, default_args=default_args, schedule_interval='*/1 * * * *')\n\n\n\nSL_SPARK_CMD = \"spark2-submit \\\n                        --keytab /etc/keytabs/importhdfs.keytab \\\n                        --principal importhdfs@MY.BIGDATA \\\n                        --conf spark.jars.packages=\\\"\\\" \\\n                        --master yarn \\\n                        --deploy-mode client /home/airflow/program/comet-assembly-0.1.jar\"\n\nCometImport = BashOperator(\n    task_id='comet_import',\n    bash_command= SL_SPARK_CMD + ' import',\n    env={\n        'SL_DATASETS':\"/project/data\",\n        'SL_METADATA':\"/project/metadata\",\n        'SL_AREA_ACCEPTED':\"working\",\n        'SL_AREA_PENDING':\"staging\",\n        'SL_ARCHIVE':\"true\",\n        'SL_LAUNCHER':\"airflow\",\n        'SL_HIVE':\"true\",\n        'SL_ANALYZE':\"true\"\n    },\n    dag=dag)\n")),(0,r.kt)("h3",{id:"watch-dag"},"Watch DAG"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"import os\nfrom airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom datetime import datetime, timedelta\nfrom airflow.operators.slack_operator import SlackAPIPostOperator\n\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2018, 11, 2),\n    'email': ['me@here.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n    'retry_delay': timedelta(minutes=5),\n    # 'queue': 'bash_queue',\n    # 'pool': 'backfill',\n    # 'priority_weight': 10,\n    # 'end_date': datetime(2016, 1, 1),\n}\n\ndag = DAG('comet_watcher',max_active_runs=1 , catchup=False, default_args=default_args, schedule_interval='*/1 * * * *')\n\ndef slack_task(msg):\n    slack_alert = SlackAPIPostOperator(\n        task_id='slack_alert',\n        channel=\"#airflow\",\n        token=\"xoxp-64071012534-475450904118-524897638692-f9a90d49fd7fb312a574b4570d557b9a\",\n        text = msg,\n        username = 'airflow',)\n    return slack_alert.execute(msg=msg)\n\nSL_SPARK_CMD = \"spark2-submit \\\n                        --keytab /etc/keytabs/importhdfs.keytab \\\n                        --principal importhdfs@MY.BIGDATA \\\n                        --conf spark.jars.packages=\\\"\\\" \\\n                        --master yarn \\\n                        --deploy-mode client /home/airflow/program/comet-assembly-0.1.jar\"\n\nSL_DOMAIN = os.environ.get('SL_DOMAIN', '')\nCometWatch = BashOperator(\n    task_id='comet_watcher',\n    bash_command= SL_SPARK_CMD + ' watch '+ SL_DOMAIN,\n    #on_failure_callback=slack_task(\":red_circle: Task Starlake Watch Failed\"),\n    #on_success_callback=slack_task(\":ok_hand: Task Starlake Watch Success\"),\n    env={\n        'AIRFLOW_ENDPOINT':\"https://airflow.my.server.com/api/experimental\",\n        'SL_DATASETS':\"/project/data\",\n        'SL_METADATA':\"/project/metadata\",\n        'SL_AREA_ACCEPTED':\"working\",\n        'SL_AREA_PENDING':\"staging\",\n        'SL_ARCHIVE':\"true\",\n        'SL_LAUNCHER':\"airflow\",\n        'SL_HIVE':\"true\",\n        'SL_ANALYZE':\"true\"\n    },\n    dag=dag)\n")),(0,r.kt)("h3",{id:"ingestion-dag"},"Ingestion DAG"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom datetime import datetime, timedelta\nfrom airflow.operators.slack_operator import SlackAPIPostOperator\n\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2018, 11, 2),\n    'email': ['me@here.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG('comet_ingest',max_active_runs=1 , catchup=False, default_args=default_args, schedule_interval = None)\n\ndef slack_task(msg):\n    slack_alert = SlackAPIPostOperator(\n        task_id='slack_alert',\n        channel=\"#airflow\",\n        token=\"xoxp-64071012534-475450904118-524897638692-f9a90d49fd7fb312a574b4570d557b9a\",\n        text = msg,\n        username = 'airflow',)\n    return slack_alert.execute(msg=msg)\n\nSL_SPARK_CMD = \"spark2-submit \\\n                        --keytab /etc/keytabs/importhdfs.keytab \\\n                        --principal importhdfs@MY.BIGDATA \\\n                        --conf spark.jars.packages=\\\"\\\" \\\n                        --conf spark.yarn.appMasterEnv.SL_METADATA=/project/metadata \\\n                        --conf spark.yarn.appMasterEnv.SL_ACCEPTED=working \\\n                        --conf spark.yarn.appMasterEnv.SL_DATASETS=/project/data \\\n                        --master yarn \\\n                        --deploy-mode cluster /home/airflow/program/comet-assembly-0.1.jar\"\n\ntemplated_command = SL_SPARK_CMD + \"\"\" {{ dag_run.conf['command'] }}\"\"\"\n\nCometIngest = BashOperator(\n    task_id='comet_ingest',\n    bash_command=templated_command,\n    #on_failure_callback=slack_task(\":red_circle: Task Starlake Ingest Failed: \"),\n    #on_success_callback=slack_task(\":ok_hand: Task Starlake Ingest Success: \"),\n    env={\n        'SL_DATASETS':\"/project/data\",\n        'SL_METADATA':\"/project/metadata\",\n        'SL_AREA_ACCEPTED':\"working\",\n        'SL_AREA_PENDING':\"staging\",\n        'SL_ARCHIVE':\"true\",\n        'SL_LAUNCHER':\"airflow\",\n        'SL_HIVE':\"true\",\n        'SL_ANALYZE':\"true\"\n    },\n    dag=dag)\n")))}u.isMDXComponent=!0}}]);