"use strict";(self.webpackChunkstarlake=self.webpackChunkstarlake||[]).push([[2330],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>k});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var i=n.createContext({}),p=function(e){var t=n.useContext(i),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},d=function(e){var t=p(e.components);return n.createElement(i.Provider,{value:t},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,i=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),m=p(a),c=r,k=m["".concat(i,".").concat(c)]||m[c]||u[c]||o;return a?n.createElement(k,l(l({ref:t},d),{},{components:a})):n.createElement(k,l({ref:t},d))}));function k(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,l=new Array(o);l[0]=c;var s={};for(var i in t)hasOwnProperty.call(t,i)&&(s[i]=t[i]);s.originalType=e,s[m]="string"==typeof e?e:r,l[1]=s;for(var p=2;p<o;p++)l[p]=a[p];return n.createElement.apply(null,l)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},85162:(e,t,a)=>{a.d(t,{Z:()=>l});var n=a(67294),r=a(86010);const o={tabItem:"tabItem_Ymn6"};function l(e){let{children:t,hidden:a,className:l}=e;return n.createElement("div",{role:"tabpanel",className:(0,r.Z)(o.tabItem,l),hidden:a},t)}},74866:(e,t,a)=>{a.d(t,{Z:()=>y});var n=a(87462),r=a(67294),o=a(86010),l=a(12466),s=a(16550),i=a(91980),p=a(67392),d=a(50012);function m(e){return function(e){return r.Children.map(e,(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:t,label:a,attributes:n,default:r}}=e;return{value:t,label:a,attributes:n,default:r}}))}function u(e){const{values:t,children:a}=e;return(0,r.useMemo)((()=>{const e=t??m(a);return function(e){const t=(0,p.l)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,a])}function c(e){let{value:t,tabValues:a}=e;return a.some((e=>e.value===t))}function k(e){let{queryString:t=!1,groupId:a}=e;const n=(0,s.k6)(),o=function(e){let{queryString:t=!1,groupId:a}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:t,groupId:a});return[(0,i._X)(o),(0,r.useCallback)((e=>{if(!o)return;const t=new URLSearchParams(n.location.search);t.set(o,e),n.replace({...n.location,search:t.toString()})}),[o,n])]}function g(e){const{defaultValue:t,queryString:a=!1,groupId:n}=e,o=u(e),[l,s]=(0,r.useState)((()=>function(e){let{defaultValue:t,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!c({value:t,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const n=a.find((e=>e.default))??a[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:t,tabValues:o}))),[i,p]=k({queryString:a,groupId:n}),[m,g]=function(e){let{groupId:t}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(t),[n,o]=(0,d.Nk)(a);return[n,(0,r.useCallback)((e=>{a&&o.set(e)}),[a,o])]}({groupId:n}),_=(()=>{const e=i??m;return c({value:e,tabValues:o})?e:null})();(0,r.useLayoutEffect)((()=>{_&&s(_)}),[_]);return{selectedValue:l,selectValue:(0,r.useCallback)((e=>{if(!c({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);s(e),p(e),g(e)}),[p,g,o]),tabValues:o}}var _=a(72389);const h={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function f(e){let{className:t,block:a,selectedValue:s,selectValue:i,tabValues:p}=e;const d=[],{blockElementScrollPositionUntilNextRender:m}=(0,l.o5)(),u=e=>{const t=e.currentTarget,a=d.indexOf(t),n=p[a].value;n!==s&&(m(t),i(n))},c=e=>{let t=null;switch(e.key){case"Enter":u(e);break;case"ArrowRight":{const a=d.indexOf(e.currentTarget)+1;t=d[a]??d[0];break}case"ArrowLeft":{const a=d.indexOf(e.currentTarget)-1;t=d[a]??d[d.length-1];break}}t?.focus()};return r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.Z)("tabs",{"tabs--block":a},t)},p.map((e=>{let{value:t,label:a,attributes:l}=e;return r.createElement("li",(0,n.Z)({role:"tab",tabIndex:s===t?0:-1,"aria-selected":s===t,key:t,ref:e=>d.push(e),onKeyDown:c,onClick:u},l,{className:(0,o.Z)("tabs__item",h.tabItem,l?.className,{"tabs__item--active":s===t})}),a??t)})))}function N(e){let{lazy:t,children:a,selectedValue:n}=e;const o=(Array.isArray(a)?a:[a]).filter(Boolean);if(t){const e=o.find((e=>e.props.value===n));return e?(0,r.cloneElement)(e,{className:"margin-top--md"}):null}return r.createElement("div",{className:"margin-top--md"},o.map(((e,t)=>(0,r.cloneElement)(e,{key:t,hidden:e.props.value!==n}))))}function b(e){const t=g(e);return r.createElement("div",{className:(0,o.Z)("tabs-container",h.tabList)},r.createElement(f,(0,n.Z)({},e,t)),r.createElement(N,(0,n.Z)({},e,t)))}function y(e){const t=(0,_.Z)();return r.createElement(b,(0,n.Z)({key:String(t)},e))}},41135:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>d,contentTitle:()=>i,default:()=>k,frontMatter:()=>s,metadata:()=>p,toc:()=>m});var n=a(87462),r=(a(67294),a(3905)),o=a(74866),l=a(85162);const s={toc_min_heading_level:2,toc_max_heading_level:4},i="Orchestration",p={unversionedId:"user-guide/orchestration",id:"version-1.1.0/user-guide/orchestration",title:"Orchestration",description:"Now that we have seen how to load and transform data, we will see how to orchestrate the tasks using the starlake command line tool.",source:"@site/versioned_docs/version-1.1.0/0400-user-guide/350-orchestration.mdx",sourceDirName:"0400-user-guide",slug:"/user-guide/orchestration",permalink:"/starlake/docs/user-guide/orchestration",draft:!1,editUrl:"https://github.com/starlake-ai/starlake/edit/master/docs/versioned_docs/version-1.1.0/0400-user-guide/350-orchestration.mdx",tags:[],version:"1.1.0",sidebarPosition:350,frontMatter:{toc_min_heading_level:2,toc_max_heading_level:4},sidebar:"starlakeSidebar",previous:{title:"Access Control",permalink:"/starlake/docs/user-guide/security"},next:{title:"About Metrics",permalink:"/starlake/docs/user-guide/metrics"}},d={},m=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Command",id:"command",level:2},{value:"Configuration",id:"configuration",level:2},{value:"References",id:"references",level:3},{value:"DAG configuration for loading data",id:"dag-configuration-for-loading-data",level:4},{value:"DAG configuration for transforming data",id:"dag-configuration-for-transforming-data",level:4},{value:"Properties",id:"properties",level:3},{value:"Comment",id:"comment",level:4},{value:"Template",id:"template",level:4},{value:"Filename",id:"filename",level:4},{value:"Options",id:"options",level:4},{value:"Starlake env vars",id:"starlake-env-vars",level:5},{value:"Pre-load strategy",id:"pre-load-strategy",level:5},{value:"NONE",id:"none",level:6},{value:"IMPORTED",id:"imported",level:6},{value:"PENDING",id:"pending",level:6},{value:"ACK",id:"ack",level:6},{value:"Load dependencies",id:"load-dependencies",level:5},{value:"Additional options",id:"additional-options",level:3},{value:"IStarlakeJob",id:"istarlakejob",level:4},{value:"Concrete factory classes",id:"concrete-factory-classes",level:4},{value:"StarlakeDagsterShellJob",id:"starlakedagstershelljob",level:6},{value:"StarlakeDagsterDataprocJob",id:"starlakedagsterdataprocjob",level:6},{value:"StarlakeDagsterCloudRunJob",id:"starlakedagstercloudrunjob",level:6},{value:"Templates",id:"templates",level:2},{value:"Starlake templates",id:"starlake-templates",level:3},{value:"Data loading",id:"data-loading",level:4},{value:"Data transformation",id:"data-transformation",level:4},{value:"Customize existing templates",id:"customize-existing-templates",level:3},{value:"Transform parameters",id:"transform-parameters",level:4},{value:"jobs variable",id:"jobs-variable",level:5},{value:"Airflow user defined macros",id:"airflow-user-defined-macros",level:4},{value:"Dataproc cluster configuration",id:"dataproc-cluster-configuration",level:4},{value:"Spark configuration",id:"spark-configuration",level:4},{value:"Dependencies",id:"dependencies",level:2},{value:"Inline",id:"inline",level:3},{value:"External state change",id:"external-state-change",level:3},{value:"Airflow Data-aware scheduling",id:"airflow-data-aware-scheduling",level:4}],u={toc:m},c="wrapper";function k(e){let{components:t,...s}=e;return(0,r.kt)(c,(0,n.Z)({},u,s,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"orchestration"},"Orchestration"),(0,r.kt)("p",null,"Now that we have seen how to load and transform data, we will see how to orchestrate the tasks using the ",(0,r.kt)("inlineCode",{parentName:"p"},"starlake")," command line tool.",(0,r.kt)("br",null),"\nstarlake is not an orchestration tool, but it can be used to generate your DAG based on templates and to run your transforms in the right\norder on your tools of choice for scheduling and monitoring batch oriented workflows.",(0,r.kt)("br",null)),(0,r.kt)("br",null),"Starlake DAG generation relies on:",(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"starlake")," command line tool"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"DAG")," ",(0,r.kt)("strong",{parentName:"li"},"configuration"),"(s) and their ",(0,r.kt)("strong",{parentName:"li"},"references")," within the loads and tasks"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"template"),"(s) that may be customized"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"starlake-orchestration")," framework to dynamically generate the tasks that will be run"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"managing dependencies")," between tasks to execute transforms in the ",(0,r.kt)("strong",{parentName:"li"},"correct order"))),(0,r.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,r.kt)("p",null,"Before using Starlake dag generation, ensure the following minimum versions are installed on your system:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"starlake"),": 1.0.1 or higher")),(0,r.kt)(o.Z,{groupId:"schedulers",mdxType:"Tabs"},(0,r.kt)(l.Z,{label:"Airflow",value:"airflow",default:!0,mdxType:"TabItem"},(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"Additional requirements for Airflow")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Apache Airflow"),": 2.4.0 or higher (2.6.0 or higher is recommended with cloud-run)"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"starlake-airflow"),": 0.1.2.1 or higher"))),(0,r.kt)(l.Z,{label:"Dagster",value:"dagster",mdxType:"TabItem"},(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"Additional requirements for Dagster")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Dagster"),": 1.6.0 or higher"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"starlake-dagster"),": 0.1.2 or higher")))),(0,r.kt)("h2",{id:"command"},"Command"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"starlake dag-generate [options]\n")),(0,r.kt)("p",null,"where options are:"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"parameter"),(0,r.kt)("th",{parentName:"tr",align:null},"cardinality"),(0,r.kt)("th",{parentName:"tr",align:null},"description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"--outputDir ",(0,r.kt)("inlineCode",{parentName:"td"},"<value>")),(0,r.kt)("td",{parentName:"tr",align:null},"optional"),(0,r.kt)("td",{parentName:"tr",align:null},"Path for saving the resulting DAG file(s) (",(0,r.kt)("em",{parentName:"td"},"${SL_ROOT}/metadata/dags/generated")," by default).")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"--clean"),(0,r.kt)("td",{parentName:"tr",align:null},"optional"),(0,r.kt)("td",{parentName:"tr",align:null},"Should the existing DAG file(s) be removed first (",(0,r.kt)("em",{parentName:"td"},"false")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"--domains"),(0,r.kt)("td",{parentName:"tr",align:null},"optional"),(0,r.kt)("td",{parentName:"tr",align:null},"Wether to generate DAG file(s) to load schema(s) or not (",(0,r.kt)("em",{parentName:"td"},"true")," by default if ",(0,r.kt)("em",{parentName:"td"},"--tasks")," option has not been specified)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"--tasks"),(0,r.kt)("td",{parentName:"tr",align:null},"optional"),(0,r.kt)("td",{parentName:"tr",align:null},"Whether to generate DAG file(s) for tasks or not  (",(0,r.kt)("em",{parentName:"td"},"true")," by default if ",(0,r.kt)("em",{parentName:"td"},"--domains")," option has not been specified)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"--tags ",(0,r.kt)("inlineCode",{parentName:"td"},"<value>")),(0,r.kt)("td",{parentName:"tr",align:null},"optional"),(0,r.kt)("td",{parentName:"tr",align:null},"Whether to generate DAG file(s) for the specified tags only (no tags by default)")))),(0,r.kt)("h2",{id:"configuration"},"Configuration"),(0,r.kt)("p",null,"All DAG configuration files are located in ",(0,r.kt)("em",{parentName:"p"},"${SL_ROOT}/metadata/dags")," directory. The root element is ",(0,r.kt)("strong",{parentName:"p"},"dag"),"."),(0,r.kt)("h3",{id:"references"},"References"),(0,r.kt)("p",null,"We reference a DAG configuration by using the configuration file name without its extension"),(0,r.kt)("h4",{id:"dag-configuration-for-loading-data"},"DAG configuration for loading data"),(0,r.kt)("admonition",{title:"DAG configuration for loading data",type:"note"},(0,r.kt)("p",{parentName:"admonition"},"The configuration files to use for ",(0,r.kt)("em",{parentName:"p"},"loading")," data can be defined:"),(0,r.kt)("ul",{parentName:"admonition"},(0,r.kt)("li",{parentName:"ul"},"at the ",(0,r.kt)("strong",{parentName:"li"},"project")," level, in the ",(0,r.kt)("strong",{parentName:"li"},"application")," file ",(0,r.kt)("em",{parentName:"li"},"${SL_ROOT}/metadata/application.sl.yml")," under the ",(0,r.kt)("em",{parentName:"li"},"application.dagRef.load")," property.",(0,r.kt)("br",null),"\nIn this case the same configuration file will be used as the default DAG configuration for all the tables in the project.")),(0,r.kt)("pre",{parentName:"admonition"},(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"application:\n  dagRef:\n    load: load_cloud_run_domain\n#...\n")),(0,r.kt)("ul",{parentName:"admonition"},(0,r.kt)("li",{parentName:"ul"},"at the ",(0,r.kt)("strong",{parentName:"li"},"domain")," level, in the ",(0,r.kt)("strong",{parentName:"li"},"domain")," configuration file ",(0,r.kt)("em",{parentName:"li"},"${SL_ROOT}/metadata/load/{domain}/_config.sl.yml")," under the ",(0,r.kt)("em",{parentName:"li"},"load.metadata.dagRef")," property.",(0,r.kt)("br",null),"\nIn this case the configuration file will be used as the default DAG configuration for all the tables in the domain.")),(0,r.kt)("pre",{parentName:"admonition"},(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"load:\n  metadata:\n    dagRef:load_dataproc_domain\n#...\n")),(0,r.kt)("ul",{parentName:"admonition"},(0,r.kt)("li",{parentName:"ul"},"at the ",(0,r.kt)("strong",{parentName:"li"},"table")," level, in the ",(0,r.kt)("strong",{parentName:"li"},"table")," configuration file ",(0,r.kt)("em",{parentName:"li"},"${SL_ROOT}/metadata/load/{domain}/{table}.sl.yml")," under the ",(0,r.kt)("em",{parentName:"li"},"table.metadata.dagRef")," property.",(0,r.kt)("br",null),"\nIn this case the configuration file will be used as the default DAG configuration for the table only.")),(0,r.kt)("pre",{parentName:"admonition"},(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"table:\n  metadata:\n    dagRef:load_bash_domain\n#...\n"))),(0,r.kt)("h4",{id:"dag-configuration-for-transforming-data"},"DAG configuration for transforming data"),(0,r.kt)("admonition",{title:"DAG configuration for transforming data",type:"note"},(0,r.kt)("p",{parentName:"admonition"},"The configuration files to use for ",(0,r.kt)("em",{parentName:"p"},"transforming")," data can be defined"),(0,r.kt)("ul",{parentName:"admonition"},(0,r.kt)("li",{parentName:"ul"},"at the ",(0,r.kt)("strong",{parentName:"li"},"project")," level, in the ",(0,r.kt)("strong",{parentName:"li"},"application")," file ",(0,r.kt)("em",{parentName:"li"},"${SL_ROOT}/metadata/application.sl.yml")," under the ",(0,r.kt)("em",{parentName:"li"},"application.dagRef.transform")," property.",(0,r.kt)("br",null),"\nIn this case the same configuration file will be used as the default DAG configuration for all the transformations in the project.")),(0,r.kt)("pre",{parentName:"admonition"},(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"application:\n  dagRef:\n    transform: norm_cloud_run_domain\n#...\n")),(0,r.kt)("ul",{parentName:"admonition"},(0,r.kt)("li",{parentName:"ul"},"at the ",(0,r.kt)("strong",{parentName:"li"},"transformation")," level, in the ",(0,r.kt)("strong",{parentName:"li"},"transformation")," configuration file ",(0,r.kt)("em",{parentName:"li"},"${SL_ROOT}/metadata/transform/{domain}/{transformation}.sl.yml")," under the ",(0,r.kt)("em",{parentName:"li"},"task.dagRef")," property.",(0,r.kt)("br",null),"\nIn this case the configuration file will be used as the default DAG configuration for the transformation only.")),(0,r.kt)("pre",{parentName:"admonition"},(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"task:\n  dagRef: agr_cloud_run_domain\n#...\n"))),(0,r.kt)("h3",{id:"properties"},"Properties"),(0,r.kt)("p",null,"A DAG configuration defines four properties: ",(0,r.kt)("a",{parentName:"p",href:"#comment"},"comment"),", ",(0,r.kt)("a",{parentName:"p",href:"#template"},"template"),", ",(0,r.kt)("a",{parentName:"p",href:"#filename"},"filename")," and ",(0,r.kt)("a",{parentName:"p",href:"#options"},"options"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'dag:\n  comment: "dag for transforming tables for domain {{domain}} with cloud run" # will appear as a description of the dag\n  template: "custom_scheduled_task_cloud_run.py.j2" # the dag template to use\n  filename: "{{domain}}_norm_cloud_run.py" # the relative path to the outputDir specified as a parameter of the `dag-generate` command where the generated dag file will be copied\n  options:\n    sl_env_var: "{\\"SL_ROOT\\": \\"${root_path}\\", \\"SL_DATASETS\\": \\"${root_path}/datasets\\", \\"SL_TIMEZONE\\": \\"Europe/Paris\\"}"\n\n #...\n')),(0,r.kt)("h4",{id:"comment"},"Comment"),(0,r.kt)("admonition",{title:"Comment",type:"note"},(0,r.kt)("p",{parentName:"admonition"},"A short ",(0,r.kt)("strong",{parentName:"p"},"description")," to describe the generated DAG.")),(0,r.kt)("h4",{id:"template"},"Template"),(0,r.kt)("admonition",{title:"Template",type:"note"},(0,r.kt)("p",{parentName:"admonition"},"The ",(0,r.kt)("strong",{parentName:"p"},"path")," to the template that will generate the DAG(s), either:"),(0,r.kt)("ul",{parentName:"admonition"},(0,r.kt)("li",{parentName:"ul"},"an ",(0,r.kt)("strong",{parentName:"li"},"absolute")," path"),(0,r.kt)("li",{parentName:"ul"},"a ",(0,r.kt)("strong",{parentName:"li"},"relative")," path name to the ",(0,r.kt)("em",{parentName:"li"},"${SL_ROOT}metadata/dags/template")," directory"),(0,r.kt)("li",{parentName:"ul"},"a ",(0,r.kt)("strong",{parentName:"li"},"relative")," path name to the ",(0,r.kt)("em",{parentName:"li"},"src/main/templates/dags")," starlake resource directory"))),(0,r.kt)("h4",{id:"filename"},"Filename"),(0,r.kt)("admonition",{title:"Filename",type:"note"},(0,r.kt)("p",{parentName:"admonition"},"The filename defines the ",(0,r.kt)("strong",{parentName:"p"},"relative path")," to the DAG(s) that will be generated. The specified path is relative to the ",(0,r.kt)("strong",{parentName:"p"},"outputDir")," option that was specified on the command line (or its default value if not specified)."),(0,r.kt)("p",{parentName:"admonition"},"The value of this property may include ",(0,r.kt)("strong",{parentName:"p"},"special variable"),"s that will have a direct ",(0,r.kt)("strong",{parentName:"p"},"impact")," on the ",(0,r.kt)("strong",{parentName:"p"},"number of dags")," that will be ",(0,r.kt)("strong",{parentName:"p"},"generated"),":"),(0,r.kt)("ul",{parentName:"admonition"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"domain"),": a single DAG for all tables within the domain affected by this configuration")),(0,r.kt)("pre",{parentName:"admonition"},(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'dag:\n  filename: "{{domain}}_norm_cloud_run.py" # one DAG per domain\n #...\n')),(0,r.kt)("ul",{parentName:"admonition"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"table")," : as many dags as there are tables in the domain affected by this configuration")),(0,r.kt)("pre",{parentName:"admonition"},(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'dag:\n  filename: "{{domain}}_{{table}}_norm_cloud_run.py" # one DAG per table\n #...\n')),(0,r.kt)("p",{parentName:"admonition"},"Otherwise, a single DAG will be generated for all tables affected by this configuration.")),(0,r.kt)("h4",{id:"options"},"Options"),(0,r.kt)("admonition",{title:"Options",type:"note"},(0,r.kt)("p",{parentName:"admonition"},"This property allows you to pass a certain number of options to the template in the form of a ",(0,r.kt)("em",{parentName:"p"},"dictionary"),".")),(0,r.kt)("p",null,"Some of these ",(0,r.kt)("strong",{parentName:"p"},"options")," are ",(0,r.kt)("strong",{parentName:"p"},"common")," to all templates."),(0,r.kt)("h5",{id:"starlake-env-vars"},"Starlake env vars"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"sl_en_var")," defines starlake environment variables passed as an encoded json string"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'dag:\n  options:\n    sl_env_var: "{\\"SL_ROOT\\": \\"${root_path}\\", \\"SL_DATASETS\\": \\"${root_path}/datasets\\", \\"SL_TIMEZONE\\": \\"Europe/Paris\\"}"\n #...\n')),(0,r.kt)("h5",{id:"pre-load-strategy"},"Pre-load strategy"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"pre_load_strategy")," defines the strategy that can be used to conditionaly load the tables of a domain within the DAG."),(0,r.kt)("p",null,"Four possible strategies:"),(0,r.kt)("h6",{id:"none"},"NONE"),(0,r.kt)("admonition",{title:"NONE",type:"note"},(0,r.kt)("p",{parentName:"admonition"},"The load of the domain will not be conditionned and no pre-load tasks will be executed (the default strategy).")),(0,r.kt)(o.Z,{groupId:"schedulers",mdxType:"Tabs"},(0,r.kt)(l.Z,{label:"Airflow",value:"airflow",default:!0,mdxType:"TabItem"},(0,r.kt)("p",null,(0,r.kt)("img",{src:a(6430).Z,width:"1288",height:"726"}))),(0,r.kt)(l.Z,{label:"Dagster",value:"dagster",mdxType:"TabItem"},(0,r.kt)("p",null,(0,r.kt)("img",{src:a(72646).Z,width:"1988",height:"742"})))),(0,r.kt)("h6",{id:"imported"},"IMPORTED"),(0,r.kt)("admonition",{title:"IMPORTED",type:"note"},(0,r.kt)("p",{parentName:"admonition"},"This strategy implies that at least one file is present in the ",(0,r.kt)("strong",{parentName:"p"},"landing area")," (",(0,r.kt)("em",{parentName:"p"},"${SL_ROOT}/incoming/{domain}")," by default, if option ",(0,r.kt)("strong",{parentName:"p"},"incoming_path")," has not been specified). If there is one or more files to load, the method ",(0,r.kt)("strong",{parentName:"p"},"sl_import")," will be called to import the domain before loading it, otherwise the loading of the domain will be skipped."),(0,r.kt)("pre",{parentName:"admonition"},(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'dag:\n  options:\n    pre_load_strategy: "imported"\n #...\n'))),(0,r.kt)(o.Z,{groupId:"schedulers",mdxType:"Tabs"},(0,r.kt)(l.Z,{label:"Airflow",value:"airflow",default:!0,mdxType:"TabItem"},(0,r.kt)("p",null,(0,r.kt)("img",{src:a(29058).Z,width:"2026",height:"242"}))),(0,r.kt)(l.Z,{label:"Dagster",value:"dagster",mdxType:"TabItem"},(0,r.kt)("p",null,(0,r.kt)("img",{src:a(94056).Z,width:"1138",height:"1054"})))),(0,r.kt)("h6",{id:"pending"},"PENDING"),(0,r.kt)("admonition",{title:"PENDING",type:"note"},(0,r.kt)("p",{parentName:"admonition"},"This strategy implies that at least one file is present in the ",(0,r.kt)("strong",{parentName:"p"},"pending")," ",(0,r.kt)("strong",{parentName:"p"},"datasets area")," of the ",(0,r.kt)("strong",{parentName:"p"},"domain")," (",(0,r.kt)("em",{parentName:"p"},"${SL_ROOT}/datasets/pending/{domain}")," by default if option ",(0,r.kt)("strong",{parentName:"p"},"pending_path")," has not been specified), otherwise the loading of the domain will be skipped."),(0,r.kt)("pre",{parentName:"admonition"},(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'dag:\n  options:\n    pre_load_strategy: "pending"\n #...\n'))),(0,r.kt)(o.Z,{groupId:"schedulers",mdxType:"Tabs"},(0,r.kt)(l.Z,{label:"Airflow",value:"airflow",default:!0,mdxType:"TabItem"},(0,r.kt)("p",null,(0,r.kt)("img",{src:a(21204).Z,width:"1798",height:"242"}))),(0,r.kt)(l.Z,{label:"Dagster",value:"dagster",mdxType:"TabItem"},(0,r.kt)("p",null,(0,r.kt)("img",{src:a(46253).Z,width:"1138",height:"868"})))),(0,r.kt)("h6",{id:"ack"},"ACK"),(0,r.kt)("admonition",{title:"ACK",type:"note"},(0,r.kt)("p",{parentName:"admonition"},"This strategy implies that an ",(0,r.kt)("strong",{parentName:"p"},"ack file")," is present at the specified path (",(0,r.kt)("em",{parentName:"p"},"${SL_ROOT}/datasets/pending/{domain}/{{{{ds}}}}.ack")," by default if option ",(0,r.kt)("strong",{parentName:"p"},"global_ack_file_path")," has not been specified), otherwise the loading of the domain will be skipped."),(0,r.kt)("pre",{parentName:"admonition"},(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'dag:\n  options:\n    pre_load_strategy: "ack"\n #...\n'))),(0,r.kt)(o.Z,{groupId:"schedulers",mdxType:"Tabs"},(0,r.kt)(l.Z,{label:"Airflow",value:"airflow",default:!0,mdxType:"TabItem"},(0,r.kt)("p",null,(0,r.kt)("img",{src:a(14614).Z,width:"1970",height:"242"}))),(0,r.kt)(l.Z,{label:"Dagster",value:"dagster",mdxType:"TabItem"},(0,r.kt)("p",null,(0,r.kt)("img",{src:a(7751).Z,width:"1138",height:"1022"})))),(0,r.kt)("h5",{id:"load-dependencies"},"Load dependencies"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"load_dependencies")," defines wether or not we want to ",(0,r.kt)("strong",{parentName:"p"},"generate recursively")," all the ",(0,r.kt)("strong",{parentName:"p"},"dependencies")," associated to each task for which the transformation DAG was generated (",(0,r.kt)("em",{parentName:"p"},"False")," by default)."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"dag:\n  options:\n    load_dependencies: True\n #...\n")),(0,r.kt)("h3",{id:"additional-options"},"Additional options"),(0,r.kt)("p",null,"Depending on the ",(0,r.kt)("strong",{parentName:"p"},"template")," chosen, a specific ",(0,r.kt)("strong",{parentName:"p"},"concrete")," factory class extending ",(0,r.kt)("inlineCode",{parentName:"p"},"ai.starlake.job.IStarlakeJob")," will be instantiated for which additional options may be required."),(0,r.kt)("h4",{id:"istarlakejob"},"IStarlakeJob"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"ai.starlake.job.IStarlakeJob")," is the ",(0,r.kt)("strong",{parentName:"p"},"generic factory interface")," responsible for ",(0,r.kt)("strong",{parentName:"p"},"generating")," the ",(0,r.kt)("strong",{parentName:"p"},"tasks")," that will run the starlake's ",(0,r.kt)("a",{parentName:"p",href:"load#import-step"},"import"),", ",(0,r.kt)("a",{parentName:"p",href:"../concepts/load"},"load")," and ",(0,r.kt)("a",{parentName:"p",href:"../concepts/transform"},"transform")," commands:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"sl_import")," will generate the task that will run the starlake ",(0,r.kt)("a",{parentName:"li",href:"../cli/import"},"import")," command.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"def sl_import(\n    self, \n    task_id: str, \n    domain: str, \n    **kwargs) -> BaseOperator:\n    #...\n")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"name"),(0,r.kt)("th",{parentName:"tr",align:null},"type"),(0,r.kt)("th",{parentName:"tr",align:null},"description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"task_id"),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional task id (",(0,r.kt)("inlineCode",{parentName:"td"},"{domain}_import")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"domain"),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the required domain to import")))),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"sl_load")," will generate the task that will run the starlake ",(0,r.kt)("a",{parentName:"li",href:"../cli/load"},"load")," command.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"def sl_load(\n    self, \n    task_id: str, \n    domain: str, \n    table: str, \n    spark_config: StarlakeSparkConfig=None,\n    **kwargs) -> BaseOperator:\n    #...\n")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"name"),(0,r.kt)("th",{parentName:"tr",align:null},"type"),(0,r.kt)("th",{parentName:"tr",align:null},"description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"task_id"),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional task id (",(0,r.kt)("inlineCode",{parentName:"td"},"{domain}_{table}_load")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"domain"),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the required domain of the table to load")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"table"),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the required table to load")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"spark_config"),(0,r.kt)("td",{parentName:"tr",align:null},"StarlakeSparkConfig"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional ",(0,r.kt)("inlineCode",{parentName:"td"},"ai.starlake.job.StarlakeSparkConfig"))))),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"sl_transform")," will generate the task that will run the starlake ",(0,r.kt)("a",{parentName:"li",href:"../cli/transform"},"transform")," command.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"def sl_transform(\n    self, \n    task_id: str, \n    transform_name: str, \n    transform_options: str=None, \n    spark_config: StarlakeSparkConfig=None, **kwargs) -> BaseOperator:\n    #...\n")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"name"),(0,r.kt)("th",{parentName:"tr",align:null},"type"),(0,r.kt)("th",{parentName:"tr",align:null},"description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"task_id"),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional task id (",(0,r.kt)("inlineCode",{parentName:"td"},"{transform_name}")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"transform_name"),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the transform to run")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"transform_options"),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional transform options")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"spark_config"),(0,r.kt)("td",{parentName:"tr",align:null},"StarlakeSparkConfig"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional ",(0,r.kt)("inlineCode",{parentName:"td"},"ai.starlake.job.StarlakeSparkConfig"))))),(0,r.kt)("p",null,"Ultimately, all of these methods will call the ",(0,r.kt)("strong",{parentName:"p"},"sl_job")," method that needs to be ",(0,r.kt)("strong",{parentName:"p"},"implemented")," in all ",(0,r.kt)("strong",{parentName:"p"},"concrete")," factory classes."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"def sl_job(\n    self, \n    task_id: str, \n    arguments: list, \n    spark_config: StarlakeSparkConfig=None, \n    **kwargs) -> BaseOperator:\n    #...\n")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"name"),(0,r.kt)("th",{parentName:"tr",align:null},"type"),(0,r.kt)("th",{parentName:"tr",align:null},"description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"task_id"),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the required task id")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"arguments"),(0,r.kt)("td",{parentName:"tr",align:null},"list"),(0,r.kt)("td",{parentName:"tr",align:null},"The required arguments of the starlake command to run")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"spark_config"),(0,r.kt)("td",{parentName:"tr",align:null},"StarlakeSparkConfig"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional ",(0,r.kt)("inlineCode",{parentName:"td"},"ai.starlake.job.StarlakeSparkConfig"))))),(0,r.kt)("h4",{id:"concrete-factory-classes"},"Concrete factory classes"),(0,r.kt)(o.Z,{groupId:"schedulers",mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"airflow",label:"Airflow",mdxType:"TabItem"},(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"Apache Airflow Concrete factory classes")),(0,r.kt)("p",null,"Each ",(0,r.kt)("strong",{parentName:"p"},"concrete")," factory class extends ",(0,r.kt)("inlineCode",{parentName:"p"},"ai.starlake.airflow.StarlakeAirflowJob")," and implements the ",(0,r.kt)("strong",{parentName:"p"},"sl_job")," method that will generate the ",(0,r.kt)("strong",{parentName:"p"},"Airflow task")," that will run the corresponding starlake command."),(0,r.kt)("admonition",{title:"Default pool",type:"note"},(0,r.kt)("p",{parentName:"admonition"},"For all templates instantiating ",(0,r.kt)("inlineCode",{parentName:"p"},"StarlakeAirflowJob")," class, the\n",(0,r.kt)("strong",{parentName:"p"},"default_pool")," option defines the Airflow pool to use for all tasks executed within the DAG.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'dag:\n  options:\n    default_pool: "custom_default_pool"\n #...\n')),(0,r.kt)("admonition",{title:"Bash",type:"note"},(0,r.kt)("p",{parentName:"admonition"},(0,r.kt)("inlineCode",{parentName:"p"},"ai.starlake.airflow.bash.StarlakeAirflowBashJob")," is a concrete implementation of ",(0,r.kt)("inlineCode",{parentName:"p"},"StarlakeAirflowJob")," that generates tasks using ",(0,r.kt)("inlineCode",{parentName:"p"},"airflow.operators.bash.BashOperator"),". Usefull for ",(0,r.kt)("strong",{parentName:"p"},"on premise")," execution.")),(0,r.kt)("p",null,"An additional ",(0,r.kt)("strong",{parentName:"p"},"SL_STARLAKE_PATH")," option is required to specify the ",(0,r.kt)("strong",{parentName:"p"},"path")," to the ",(0,r.kt)("strong",{parentName:"p"},"starlake executable"),"."),(0,r.kt)("p",null,(0,r.kt)("img",{src:a(14847).Z,width:"1970",height:"574"})),(0,r.kt)("admonition",{title:"Dataproc",type:"note"},(0,r.kt)("p",{parentName:"admonition"},(0,r.kt)("inlineCode",{parentName:"p"},"ai.starlake.airflow.gcp.StarlakeAirflowDataprocJob")," is another implementation of ",(0,r.kt)("inlineCode",{parentName:"p"},"StarlakeAirflowJob")," that overrides the ",(0,r.kt)("inlineCode",{parentName:"p"},"sl_job")," method that will run the starlake command by submitting ",(0,r.kt)("strong",{parentName:"p"},"Dataproc job")," to the configured ",(0,r.kt)("strong",{parentName:"p"},"Dataproc cluster"),".")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"StarlakeAirflowDataprocJob")," delegates to an instance of ",(0,r.kt)("inlineCode",{parentName:"p"},"ai.starlake.airflow.gcp.StarlakeAirflowDataprocCluster")," class the responsibility to :"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"create")," the ",(0,r.kt)("strong",{parentName:"li"},"Dataproc cluster")," by instantiating ",(0,r.kt)("inlineCode",{parentName:"li"},"airflow.providers.google.cloud.operators.dataproc.DataprocCreateClusterOperator")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"submit")," the ",(0,r.kt)("strong",{parentName:"li"},"Dataproc job")," to the latter by instantiating ",(0,r.kt)("inlineCode",{parentName:"li"},"airflow.providers.google.cloud.operators.dataproc.DataprocSubmitJobOperator")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"delete")," the ",(0,r.kt)("strong",{parentName:"li"},"Dataproc cluster")," by instantiating ",(0,r.kt)("inlineCode",{parentName:"li"},"airflow.providers.google.cloud.operators.dataproc.DataprocDeleteClusterOperator"))),(0,r.kt)("p",null,"The ",(0,r.kt)("strong",{parentName:"p"},"creation")," of the ",(0,r.kt)("strong",{parentName:"p"},"Dataproc cluster")," can be performed by calling the ",(0,r.kt)("inlineCode",{parentName:"p"},"create_cluster")," method of the ",(0,r.kt)("em",{parentName:"p"},"cluster")," property or by calling the ",(0,r.kt)("inlineCode",{parentName:"p"},"pre_tasks")," method of the StarlakeAirflowDataprocJob (the call to the ",(0,r.kt)("inlineCode",{parentName:"p"},"pre_load")," method will, behind the scene, call the ",(0,r.kt)("inlineCode",{parentName:"p"},"pre_tasks")," method and add the optional resulting task to the group of Airflow tasks)."),(0,r.kt)("p",null,"The ",(0,r.kt)("strong",{parentName:"p"},"submission")," of the ",(0,r.kt)("strong",{parentName:"p"},"Dataproc job")," can be performed by calling the ",(0,r.kt)("inlineCode",{parentName:"p"},"submit_job")," method of the ",(0,r.kt)("em",{parentName:"p"},"cluster")," property or by calling the ",(0,r.kt)("inlineCode",{parentName:"p"},"sl_job")," method of the StarlakeAirflowDataprocJob."),(0,r.kt)("p",null,"The ",(0,r.kt)("strong",{parentName:"p"},"deletion")," of the ",(0,r.kt)("strong",{parentName:"p"},"Dataproc cluster")," can be performed by calling the ",(0,r.kt)("inlineCode",{parentName:"p"},"delete_cluster")," method of the ",(0,r.kt)("em",{parentName:"p"},"cluster")," property or by calling the ",(0,r.kt)("inlineCode",{parentName:"p"},"post_tasks")," method of the StarlakeAirflowDataprocJob."),(0,r.kt)("p",null,"Bellow is the list of ",(0,r.kt)("strong",{parentName:"p"},"additional options")," used to configure the ",(0,r.kt)("strong",{parentName:"p"},"Dataproc cluster"),":"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"name"),(0,r.kt)("th",{parentName:"tr",align:null},"type"),(0,r.kt)("th",{parentName:"tr",align:null},"description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"cluster_id")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional unique id of the cluster that will participate in the definition of the Dataproc cluster name (if not specified)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_name")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional dataproc name of the cluster that will participate in the definition of the Dataproc cluster name (if not specified)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_project_id")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional dataproc project id (the project id on which the composer has been instantiated by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_region")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional region (",(0,r.kt)("inlineCode",{parentName:"td"},"europe-west1")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_subnet")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional subnet (the ",(0,r.kt)("inlineCode",{parentName:"td"},"default")," subnet if not specified)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_service_account")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional service account (",(0,r.kt)("inlineCode",{parentName:"td"},"service-{self.project_id}@dataproc-accounts.iam.gserviceaccount.com")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_image_version")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the image version of the dataproc cluster (",(0,r.kt)("inlineCode",{parentName:"td"},"2.2-debian1")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_master_machine_type")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional master machine type (",(0,r.kt)("inlineCode",{parentName:"td"},"n1-standard-4")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_master_disk_type")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional master disk type (",(0,r.kt)("inlineCode",{parentName:"td"},"pd-standard")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_master_disk_size")),(0,r.kt)("td",{parentName:"tr",align:null},"int"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional master disk size (",(0,r.kt)("inlineCode",{parentName:"td"},"1024")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_worker_machine_type")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional worker machine type (",(0,r.kt)("inlineCode",{parentName:"td"},"n1-standard-4")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_worker_disk_type")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional worker disk size (",(0,r.kt)("inlineCode",{parentName:"td"},"pd-standard")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_worker_disk_size")),(0,r.kt)("td",{parentName:"tr",align:null},"int"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional worker disk size (",(0,r.kt)("inlineCode",{parentName:"td"},"1024")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_num_workers")),(0,r.kt)("td",{parentName:"tr",align:null},"int"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional number of workers (",(0,r.kt)("inlineCode",{parentName:"td"},"4")," by default)")))),(0,r.kt)("p",null,"All of these options will be used by default if no ",(0,r.kt)("em",{parentName:"p"},"StarlakeAirflowDataprocClusterConfig")," was defined when instantiating ",(0,r.kt)("em",{parentName:"p"},"StarlakeAirflowDataprocCluster")," or if the latter was not defined when instantiating ",(0,r.kt)("em",{parentName:"p"},"StarlakeAirflowDataprocJob"),"."),(0,r.kt)("p",null,"Bellow is the list of ",(0,r.kt)("strong",{parentName:"p"},"additional options")," used to configure the ",(0,r.kt)("strong",{parentName:"p"},"Dataproc job"),":"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"name"),(0,r.kt)("th",{parentName:"tr",align:null},"type"),(0,r.kt)("th",{parentName:"tr",align:null},"description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"spark_jar_list")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the required list of spark jars to be used (using ",(0,r.kt)("inlineCode",{parentName:"td"},",")," as separator)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"spark_bucket")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the required bucket to use for spark and biqquery temporary storage")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"spark_job_main_class")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional main class of the spark job (",(0,r.kt)("inlineCode",{parentName:"td"},"ai.starlake.job.Main")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"spark_executor_memory")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional amount of memory to use per executor process (",(0,r.kt)("inlineCode",{parentName:"td"},"11g")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"spark_executor_cores")),(0,r.kt)("td",{parentName:"tr",align:null},"int"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional number of cores to use on each executor (",(0,r.kt)("inlineCode",{parentName:"td"},"4")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"spark_executor_instances")),(0,r.kt)("td",{parentName:"tr",align:null},"int"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional number of executor instances (",(0,r.kt)("inlineCode",{parentName:"td"},"1")," by default)")))),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"spark_executor_memory"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"spark_executor_cores")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"spark_executor_instances")," options will be used by default if no ",(0,r.kt)("strong",{parentName:"p"},"StarlakeSparkConfig")," was passed to the ",(0,r.kt)("inlineCode",{parentName:"p"},"sl_load")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"sl_transform")," methods."),(0,r.kt)("p",null,(0,r.kt)("img",{src:a(43477).Z,width:"2020",height:"536"})),(0,r.kt)("admonition",{title:"Cloud run",type:"note"},(0,r.kt)("p",{parentName:"admonition"},(0,r.kt)("inlineCode",{parentName:"p"},"ai.starlake.airflow.gcp.StarlakeAirflowCloudRunJob")," class is a concrete implementation of ",(0,r.kt)("inlineCode",{parentName:"p"},"StarlakeAirflowJob")," that overrides the ",(0,r.kt)("inlineCode",{parentName:"p"},"sl_job")," method that will run the starlake command by executing ",(0,r.kt)("strong",{parentName:"p"},"Cloud Run job"),".")),(0,r.kt)("p",null,"Bellow is the list of ",(0,r.kt)("strong",{parentName:"p"},"additional options")," used to configure the ",(0,r.kt)("strong",{parentName:"p"},"Cloud run job"),":"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"name"),(0,r.kt)("th",{parentName:"tr",align:null},"type"),(0,r.kt)("th",{parentName:"tr",align:null},"description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"cloud_run_project_id")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional cloud run project id (the project id on which the composer has been instantiated by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"cloud_run_job_name")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the required name of the cloud run job")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"cloud_run_region")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional region (",(0,r.kt)("inlineCode",{parentName:"td"},"europe-west1")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"cloud_run_async")),(0,r.kt)("td",{parentName:"tr",align:null},"bool"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional flag to run the cloud run job asynchronously (",(0,r.kt)("inlineCode",{parentName:"td"},"True")," by default)`")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"retry_on_failure")),(0,r.kt)("td",{parentName:"tr",align:null},"bool"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional flag to retry the cloud run job on failure (",(0,r.kt)("inlineCode",{parentName:"td"},"False")," by default)`")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"retry_delay_in_seconds")),(0,r.kt)("td",{parentName:"tr",align:null},"int"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional delay in seconds to wait before retrying the cloud run job (",(0,r.kt)("inlineCode",{parentName:"td"},"10")," by default)`")))),(0,r.kt)("p",null,(0,r.kt)("img",{src:a(20393).Z,width:"1894",height:"676"})),(0,r.kt)("p",null,"If the execution has been parameterized to be ",(0,r.kt)("strong",{parentName:"p"},"asynchronous"),", an ",(0,r.kt)("inlineCode",{parentName:"p"},"ai.starlake.airflow.gcp.CloudRunJobCompletionSensor")," which extends ",(0,r.kt)("inlineCode",{parentName:"p"},"airflow.sensors.bash.BashSensor")," will be instantiated to ",(0,r.kt)("strong",{parentName:"p"},"wait")," for the ",(0,r.kt)("strong",{parentName:"p"},"completion")," of the ",(0,r.kt)("strong",{parentName:"p"},"Cloud run job execution"),"."),(0,r.kt)("p",null,(0,r.kt)("img",{src:a(26781).Z,width:"2066",height:"676"}))),(0,r.kt)(l.Z,{value:"dagster",label:"Dagster",mdxType:"TabItem"},(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"Dagster Concrete factory classes")),(0,r.kt)("p",null,"Each ",(0,r.kt)("strong",{parentName:"p"},"concrete")," factory class extends ",(0,r.kt)("inlineCode",{parentName:"p"},"ai.starlake.dagster.StarlakeDagsterJob")," and implements the ",(0,r.kt)("strong",{parentName:"p"},"sl_job")," method that will generate the ",(0,r.kt)("strong",{parentName:"p"},"Dagster op")," that will run the corresponding starlake command."),(0,r.kt)("h6",{id:"starlakedagstershelljob"},"StarlakeDagsterShellJob"),(0,r.kt)("admonition",{title:"Shell",type:"note"},(0,r.kt)("p",{parentName:"admonition"},(0,r.kt)("inlineCode",{parentName:"p"},"ai.starlake.dagster.shell.StarlakeDagsterShellJob")," is a concrete implementation of ",(0,r.kt)("inlineCode",{parentName:"p"},"StarlakeDagsterJob")," that generates nodes using dagster-shell library. Usefull for ",(0,r.kt)("strong",{parentName:"p"},"on premise")," execution.")),(0,r.kt)("p",null,"An additional ",(0,r.kt)("strong",{parentName:"p"},"SL_STARLAKE_PATH")," option is required to specify the ",(0,r.kt)("strong",{parentName:"p"},"path")," to the ",(0,r.kt)("strong",{parentName:"p"},"starlake executable"),"."),(0,r.kt)("p",null,(0,r.kt)("img",{src:a(29115).Z,width:"2002",height:"796"})),(0,r.kt)("h6",{id:"starlakedagsterdataprocjob"},"StarlakeDagsterDataprocJob"),(0,r.kt)("admonition",{title:"Dataproc",type:"note"},(0,r.kt)("p",{parentName:"admonition"},(0,r.kt)("inlineCode",{parentName:"p"},"ai.starlake.dagster.gcp.StarlakeDagsterDataprocJob")," concrete factory class executes starlake's commands by submitting ",(0,r.kt)("strong",{parentName:"p"},"Dataproc job")," to the configured ",(0,r.kt)("strong",{parentName:"p"},"Dataproc cluster"),".")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"StarlakeDagsterDataprocJob")," delegates to an instance of the ",(0,r.kt)("inlineCode",{parentName:"p"},"dagster_gcp.DataprocResource")," class the responsibility to :"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"create")," the ",(0,r.kt)("strong",{parentName:"li"},"Dataproc cluster")," "),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"submit")," the ",(0,r.kt)("strong",{parentName:"li"},"Dataproc job")," to the latter"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"delete")," the ",(0,r.kt)("strong",{parentName:"li"},"Dataproc cluster"))),(0,r.kt)("p",null,"This instance is available through the ",(0,r.kt)("inlineCode",{parentName:"p"},"__dataproc__")," property of the ",(0,r.kt)("inlineCode",{parentName:"p"},"StarlakeDagsterDataprocJob")," class and is configured using the ",(0,r.kt)("inlineCode",{parentName:"p"},"ai.starlake.gcp.StarlakeDataprocClusterConfig")," class."),(0,r.kt)("p",null,"The creation of the ",(0,r.kt)("strong",{parentName:"p"},"Dataproc cluster")," can be performed by calling the ",(0,r.kt)("inlineCode",{parentName:"p"},"pre_tasks")," method of the StarlakeDagsterDataprocJob."),(0,r.kt)("p",null,"The ",(0,r.kt)("strong",{parentName:"p"},"submission")," of the ",(0,r.kt)("strong",{parentName:"p"},"Dataproc job")," can be performed by calling the ",(0,r.kt)("inlineCode",{parentName:"p"},"sl_job")," method of the StarlakeDagsterDataprocJob."),(0,r.kt)("p",null,"The deletion of the ",(0,r.kt)("strong",{parentName:"p"},"Dataproc cluster")," can be performed by calling the ",(0,r.kt)("inlineCode",{parentName:"p"},"post_tasks")," method of the StarlakeDagsterDataprocJob."),(0,r.kt)("p",null,"Bellow is the list of ",(0,r.kt)("strong",{parentName:"p"},"additional options")," used to configure the ",(0,r.kt)("strong",{parentName:"p"},"Dataproc cluster"),":"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"name"),(0,r.kt)("th",{parentName:"tr",align:null},"type"),(0,r.kt)("th",{parentName:"tr",align:null},"description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"cluster_id")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional unique id of the cluster that will participate in the definition of the Dataproc cluster name (if not specified)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_name")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional dataproc name of the cluster that will participate in the definition of the Dataproc cluster name (if not specified)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_project_id")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional dataproc project id (the project id on which the composer has been instantiated by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_region")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional region (",(0,r.kt)("inlineCode",{parentName:"td"},"europe-west1")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_subnet")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional subnet (the ",(0,r.kt)("inlineCode",{parentName:"td"},"default")," subnet if not specified)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_service_account")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional service account (",(0,r.kt)("inlineCode",{parentName:"td"},"service-{self.project_id}@dataproc-accounts.iam.gserviceaccount.com")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_image_version")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the image version of the dataproc cluster (",(0,r.kt)("inlineCode",{parentName:"td"},"2.2-debian1")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_master_machine_type")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional master machine type (",(0,r.kt)("inlineCode",{parentName:"td"},"n1-standard-4")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_master_disk_type")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional master disk type (",(0,r.kt)("inlineCode",{parentName:"td"},"pd-standard")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_master_disk_size")),(0,r.kt)("td",{parentName:"tr",align:null},"int"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional master disk size (",(0,r.kt)("inlineCode",{parentName:"td"},"1024")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_worker_machine_type")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional worker machine type (",(0,r.kt)("inlineCode",{parentName:"td"},"n1-standard-4")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_worker_disk_type")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional worker disk size (",(0,r.kt)("inlineCode",{parentName:"td"},"pd-standard")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_worker_disk_size")),(0,r.kt)("td",{parentName:"tr",align:null},"int"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional worker disk size (",(0,r.kt)("inlineCode",{parentName:"td"},"1024")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"dataproc_num_workers")),(0,r.kt)("td",{parentName:"tr",align:null},"int"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional number of workers (",(0,r.kt)("inlineCode",{parentName:"td"},"4")," by default)")))),(0,r.kt)("p",null,"All of these options will be used by default if no ",(0,r.kt)("em",{parentName:"p"},"StarlakeDataprocClusterConfig")," was defined when instantiating ",(0,r.kt)("em",{parentName:"p"},"StarlakeDagsterDataprocJob"),"."),(0,r.kt)("p",null,"Bellow is the list of ",(0,r.kt)("strong",{parentName:"p"},"additional options")," used to configure the ",(0,r.kt)("strong",{parentName:"p"},"Dataproc job"),":"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"name"),(0,r.kt)("th",{parentName:"tr",align:null},"type"),(0,r.kt)("th",{parentName:"tr",align:null},"description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"spark_jar_list")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the required list of spark jars to be used (using ",(0,r.kt)("inlineCode",{parentName:"td"},",")," as separator)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"spark_bucket")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the required bucket to use for spark and biqquery temporary storage")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"spark_job_main_class")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional main class of the spark job (",(0,r.kt)("inlineCode",{parentName:"td"},"ai.starlake.job.Main")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"spark_executor_memory")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional amount of memory to use per executor process (",(0,r.kt)("inlineCode",{parentName:"td"},"11g")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"spark_executor_cores")),(0,r.kt)("td",{parentName:"tr",align:null},"int"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional number of cores to use on each executor (",(0,r.kt)("inlineCode",{parentName:"td"},"4")," by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"spark_executor_instances")),(0,r.kt)("td",{parentName:"tr",align:null},"int"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional number of executor instances (",(0,r.kt)("inlineCode",{parentName:"td"},"1")," by default)")))),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"spark_executor_memory"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"spark_executor_cores")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"spark_executor_instances")," options will be used by default if no ",(0,r.kt)("strong",{parentName:"p"},"StarlakeSparkConfig")," was passed to the ",(0,r.kt)("inlineCode",{parentName:"p"},"sl_load")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"sl_transform")," methods."),(0,r.kt)("h6",{id:"starlakedagstercloudrunjob"},"StarlakeDagsterCloudRunJob"),(0,r.kt)("admonition",{title:"Cloud run",type:"note"},(0,r.kt)("p",{parentName:"admonition"},(0,r.kt)("inlineCode",{parentName:"p"},"ai.starlake.dagster.gcp.StarlakeDagsterCloudRunJob")," class is a concrete implementation of ",(0,r.kt)("inlineCode",{parentName:"p"},"StarlakeDagsterJob")," that overrides the ",(0,r.kt)("inlineCode",{parentName:"p"},"sl_job")," method that will run the starlake command by executing ",(0,r.kt)("strong",{parentName:"p"},"Cloud Run job"),".")),(0,r.kt)("p",null,"Bellow is the list of ",(0,r.kt)("strong",{parentName:"p"},"additional options")," used to configure the ",(0,r.kt)("strong",{parentName:"p"},"Cloud run job"),":"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"name"),(0,r.kt)("th",{parentName:"tr",align:null},"type"),(0,r.kt)("th",{parentName:"tr",align:null},"description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"cloud_run_project_id")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional cloud run project id (the project id on which the composer has been instantiated by default)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"cloud_run_job_name")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the required name of the cloud run job")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("strong",{parentName:"td"},"cloud_run_region")),(0,r.kt)("td",{parentName:"tr",align:null},"str"),(0,r.kt)("td",{parentName:"tr",align:null},"the optional region (",(0,r.kt)("inlineCode",{parentName:"td"},"europe-west1")," by default)")))))),(0,r.kt)("h2",{id:"templates"},"Templates"),(0,r.kt)("h3",{id:"starlake-templates"},"Starlake templates"),(0,r.kt)("p",null,"Starlake templates are listed under the ",(0,r.kt)("strong",{parentName:"p"},"src/main/resources/template/dags")," resource directory. There are ",(0,r.kt)("strong",{parentName:"p"},"two types")," of templates, those for ",(0,r.kt)("strong",{parentName:"p"},"loading")," data and others for ",(0,r.kt)("strong",{parentName:"p"},"transforming")," data."),(0,r.kt)("h4",{id:"data-loading"},"Data loading"),(0,r.kt)("admonition",{title:"Data loading",type:"note"},(0,r.kt)("p",{parentName:"admonition"},"Starlake templates for ",(0,r.kt)("strong",{parentName:"p"},"data loading")," are listed under the ",(0,r.kt)("strong",{parentName:"p"},"load")," subdirectory.")),(0,r.kt)(o.Z,{groupId:"schedulers",mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"airflow",label:"Airflow",mdxType:"TabItem"},(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"Apache Airflow Templates for data loading")),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"__airflow_scheduled_table_tpl.py.j2")," is the ",(0,r.kt)("strong",{parentName:"p"},"abstract template")," to generate Airflow DAGs for ",(0,r.kt)("strong",{parentName:"p"},"data loading")," which ",(0,r.kt)("strong",{parentName:"p"},"requires")," the instantiation of a ",(0,r.kt)("strong",{parentName:"p"},"concrete factory class")," that implements ",(0,r.kt)("em",{parentName:"p"},"ai.starlake.airflow.StarlakeAirflowJob")),(0,r.kt)("p",null,"Currently, there are ",(0,r.kt)("strong",{parentName:"p"},"three Airflow concrete templates")," for data loading."),(0,r.kt)("p",null,"All extend this abstract template by instantiating the corresponding concrete factory class using ",(0,r.kt)("strong",{parentName:"p"},"include statements"),"."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"airflow_scheduled_table_bash.py.j2")," instantiates a ",(0,r.kt)("inlineCode",{parentName:"li"},"StarlakeAirflowBashJob")," class.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="src/main/resources/templates/dags/load/airflow_scheduled_table_bash.py.j2"',title:'"src/main/resources/templates/dags/load/airflow_scheduled_table_bash.py.j2"'},"# This template executes individual bash jobs and requires the following dag generation options set:\n# - SL_STARLAKE_PATH: the path to the starlake executable [OPTIONAL]\n# ...\n{% include 'templates/dags/__starlake_airflow_bash_job.py.j2' %}\n{% include 'templates/dags/load/__airflow_scheduled_table_tpl.py.j2' %}\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"airflow_scheduled_table_cloud_run.py.j2")," instantiates a ",(0,r.kt)("inlineCode",{parentName:"li"},"StarlakeAirflowCloudRunJob")," class.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="src/main/resources/templates/dags/load/airflow_scheduled_table_cloud_run.py.j2"',title:'"src/main/resources/templates/dags/load/airflow_scheduled_table_cloud_run.py.j2"'},"# This template executes individual cloud run jobs and requires the following dag generation options set:\n# - cloud_run_project_id: the project id where the job is located (if not set, the project id of the composer environment will be used) [OPTIONAL]\n# - cloud_run_job_region: the region where the job is located (if not set, europe-west1 will be used) [OPTIONAL]\n# - cloud_run_job_name: the name of the job to execute [REQUIRED]\n# ...\n{% include 'templates/dags/__starlake_airflow_cloud_run_job.py.j2' %}\n{% include 'templates/dags/load/__airflow_scheduled_table_tpl.py.j2' %}\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"airflow_scheduled_table_dataproc.py.j2")," instantiates a ",(0,r.kt)("inlineCode",{parentName:"li"},"StarlakeAirflowDataprocJob")," class.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="src/main/resources/templates/dags/load/airflow_scheduled_table_dataproc.py.j2"',title:'"src/main/resources/templates/dags/load/airflow_scheduled_table_dataproc.py.j2"'},"# This template executes individual dataproc jobs and requires the following dag generation options set:\n# - dataproc_project_id: the project id of the dataproc cluster (if not set, the project id of the composer environment will be used) [OPTIONAL]\n# - dataproc_region: the region of the dataproc cluster (if not set, europe-west1 will be used) [OPTIONAL]\n# - dataproc_subnet: the subnetwork of the dataproc cluster (if not set, the default subnetwork will be used) [OPTIONAL]\n# - dataproc_service_account: the service account of the dataproc cluster (if not set, the default service account will be used) [OPTIONAL]\n# - dataproc_image_version: the image version of the dataproc cluster (if not set, 2.2-debian12 will be used) [OPTIONAL]\n# ...\n{% include 'templates/dags/__starlake_airflow_dataproc_job.py.j2' %}\n{% include 'templates/dags/load/__airflow_scheduled_table_tpl.py.j2' %}\n"))),(0,r.kt)(l.Z,{value:"dagster",label:"Dagster",mdxType:"TabItem"},(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"Dagster Templates for data loading")),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"__dagster_scheduled_table_tpl.py.j2")," is the ",(0,r.kt)("strong",{parentName:"p"},"abstract template")," to generate Dagster DAGs for ",(0,r.kt)("strong",{parentName:"p"},"data loading")," which ",(0,r.kt)("strong",{parentName:"p"},"requires")," the instantiation of a ",(0,r.kt)("strong",{parentName:"p"},"concrete factory class")," that implements ",(0,r.kt)("em",{parentName:"p"},"ai.starlake.dagster.StarlakeDagsterJob")),(0,r.kt)("p",null,"Currently, there are ",(0,r.kt)("strong",{parentName:"p"},"three Dagster concrete templates")," for data loading."),(0,r.kt)("p",null,"All extend this abstract template by instantiating the corresponding concrete factory class using ",(0,r.kt)("strong",{parentName:"p"},"include statements"),"."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"dagster_scheduled_table_shell.py.j2")," instantiates a ",(0,r.kt)("inlineCode",{parentName:"li"},"StarlakeDagsterShellJob")," class.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="src/main/resources/templates/dags/load/dagster_scheduled_table_shell.py.j2"',title:'"src/main/resources/templates/dags/load/dagster_scheduled_table_shell.py.j2"'},"# This template executes individual shell jobs and requires the following dag generation options set:\n# - SL_STARLAKE_PATH: the path to the starlake executable [OPTIONAL]\n# ...\n{% include 'templates/dags/__starlake_dahster_shell_job.py.j2' %}\n{% include 'templates/dags/load/__dagster_scheduled_table_tpl.py.j2' %}\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"dagster_scheduled_table_cloud_run.py.j2")," instantiates a ",(0,r.kt)("inlineCode",{parentName:"li"},"StarlakeDagsterCloudRunJob")," class.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="src/main/resources/templates/dags/load/dagster_scheduled_table_cloud_run.py.j2"',title:'"src/main/resources/templates/dags/load/dagster_scheduled_table_cloud_run.py.j2"'},"# This template executes individual cloud run jobs and requires the following dag generation options set:\n# - cloud_run_project_id: the project id where the job is located (if not set, the project id of the composer environment will be used) [OPTIONAL]\n# - cloud_run_job_region: the region where the job is located (if not set, europe-west1 will be used) [OPTIONAL]\n# - cloud_run_job_name: the name of the job to execute [REQUIRED]\n# ...\n{% include 'templates/dags/__starlake_dagster_cloud_run_job.py.j2' %}\n{% include 'templates/dags/load/__airflow_scheduled_table_tpl.py.j2' %}\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"dagster_scheduled_table_dataproc.py.j2")," instantiates a ",(0,r.kt)("inlineCode",{parentName:"li"},"StarlakeDagsterDataprocJob")," class.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="src/main/resources/templates/dags/load/dagster_scheduled_table_dataproc.py.j2"',title:'"src/main/resources/templates/dags/load/dagster_scheduled_table_dataproc.py.j2"'},"# This template executes individual dataproc jobs and requires the following dag generation options set:\n# - dataproc_project_id: the project id of the dataproc cluster (if not set, the project id of the composer environment will be used) [OPTIONAL]\n# - dataproc_region: the region of the dataproc cluster (if not set, europe-west1 will be used) [OPTIONAL]\n# - dataproc_subnet: the subnetwork of the dataproc cluster (if not set, the default subnetwork will be used) [OPTIONAL]\n# - dataproc_service_account: the service account of the dataproc cluster (if not set, the default service account will be used) [OPTIONAL]\n# - dataproc_image_version: the image version of the dataproc cluster (if not set, 2.2-debian12 will be used) [OPTIONAL]\n# ...\n{% include 'templates/dags/__starlake_dagster_dataproc_job.py.j2' %}\n{% include 'templates/dags/load/__dagster_scheduled_table_tpl.py.j2' %}\n")))),(0,r.kt)("h4",{id:"data-transformation"},"Data transformation"),(0,r.kt)("admonition",{title:"Data transformation",type:"note"},(0,r.kt)("p",{parentName:"admonition"},"Starlake templates for data transformation are listed under the ",(0,r.kt)("strong",{parentName:"p"},"transform")," subdirectory.")),(0,r.kt)(o.Z,{groupId:"schedulers",mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"airflow",label:"Airflow",mdxType:"TabItem"},(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"Apache Airflow Templates for data transformation")),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"__airflow_scheduled_task_tpl.py.j2")," is the ",(0,r.kt)("strong",{parentName:"p"},"abstract template")," to generate Airflow DAGs for ",(0,r.kt)("strong",{parentName:"p"},"data transformation")," which ",(0,r.kt)("strong",{parentName:"p"},"requires"),", in the same way, the instantiation of a ",(0,r.kt)("strong",{parentName:"p"},"concrete factory class")," that implements ",(0,r.kt)("em",{parentName:"p"},"ai.starlake.airflow.StarlakeAirflowJob")),(0,r.kt)("p",null,"Currently, there are ",(0,r.kt)("strong",{parentName:"p"},"three Airflow concrete templates")," for data transformation."),(0,r.kt)("p",null,"All extend this abstract template by instantiating the corresponding concrete factory class using ",(0,r.kt)("strong",{parentName:"p"},"include statements"),"."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"airflow_scheduled_task_bash.py.j2")," instantiates a ",(0,r.kt)("inlineCode",{parentName:"li"},"StarlakeAirflowBashJob")," class.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="src/main/resources/templates/dags/transform/airflow_scheduled_task_bash.py.j2"',title:'"src/main/resources/templates/dags/transform/airflow_scheduled_task_bash.py.j2"'},"# ...\n{% include 'templates/dags/__starlake_airflow_bash_job.py.j2' %}\n{% include 'templates/dags/load/__airflow_scheduled_task_tpl.py.j2' %}\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"airflow_scheduled_task_cloud_run.py.j2")," instantiates a ",(0,r.kt)("inlineCode",{parentName:"li"},"StarlakeAirflowCloudRunJob")," class.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="src/main/resources/templates/dags/transform/airflow_scheduled_task_cloud_run.py.j2"',title:'"src/main/resources/templates/dags/transform/airflow_scheduled_task_cloud_run.py.j2"'},"# ...\n{% include 'templates/dags/__starlake_airflow_cloud_run_job.py.j2' %}\n{% include 'templates/dags/load/__airflow_scheduled_table_tpl.py.j2' %}\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"airflow_scheduled_task_dataproc.py.j2")," instantiates a ",(0,r.kt)("inlineCode",{parentName:"li"},"StarlakeAirflowDataprocJob")," class.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="src/main/resources/templates/dags/transform/airflow_scheduled_task_dataproc.py.j2"',title:'"src/main/resources/templates/dags/transform/airflow_scheduled_task_dataproc.py.j2"'},"# ...\n{% include 'templates/dags/__starlake_airflow_dataproc_job.py.j2' %}\n{% include 'templates/dags/load/__airflow_scheduled_table_tpl.py.j2' %}\n"))),(0,r.kt)(l.Z,{value:"dagster",label:"Dagster",mdxType:"TabItem"},(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"Dagster Templates for data transformation")),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"__dagster_scheduled_task_tpl.py.j2")," is the ",(0,r.kt)("strong",{parentName:"p"},"abstract template")," to generate Dagster DAGs for ",(0,r.kt)("strong",{parentName:"p"},"data transformation")," which ",(0,r.kt)("strong",{parentName:"p"},"requires"),", in the same way, the instantiation of a ",(0,r.kt)("strong",{parentName:"p"},"concrete factory class")," that implements ",(0,r.kt)("em",{parentName:"p"},"ai.starlake.dagster.StarlakeDagsterJob")),(0,r.kt)("p",null,"Currently, there are ",(0,r.kt)("strong",{parentName:"p"},"three Dagster concrete templates")," for data transformation."),(0,r.kt)("p",null,"All extend this abstract template by instantiating the corresponding concrete factory class using ",(0,r.kt)("strong",{parentName:"p"},"include statements"),"."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"dagster_scheduled_task_shell.py.j2")," instantiates a ",(0,r.kt)("inlineCode",{parentName:"li"},"StarlakeDagsterShellJob")," class.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="src/main/resources/templates/dags/transform/dagster_scheduled_task_shell.py.j2"',title:'"src/main/resources/templates/dags/transform/dagster_scheduled_task_shell.py.j2"'},"# ...\n{% include 'templates/dags/__starlake_dagster_shell_job.py.j2' %}\n{% include 'templates/dags/load/__dagster_scheduled_task_tpl.py.j2' %}\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"dagster_scheduled_task_cloud_run.py.j2")," instantiates a ",(0,r.kt)("inlineCode",{parentName:"li"},"StarlakeDagsterCloudRunJob")," class.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="src/main/resources/templates/dags/transform/dagster_scheduled_task_cloud_run.py.j2"',title:'"src/main/resources/templates/dags/transform/dagster_scheduled_task_cloud_run.py.j2"'},"# ...\n{% include 'templates/dags/__starlake_dagster_cloud_run_job.py.j2' %}\n{% include 'templates/dags/load/__dagster_scheduled_table_tpl.py.j2' %}\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"dagster_scheduled_task_dataproc.py.j2")," instantiates a ",(0,r.kt)("inlineCode",{parentName:"li"},"StarlakeDagsterDataprocJob")," class.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="src/main/resources/templates/dags/transform/dagster_scheduled_task_dataproc.py.j2"',title:'"src/main/resources/templates/dags/transform/dagster_scheduled_task_dataproc.py.j2"'},"# ...\n{% include 'templates/dags/__starlake_dagster_dataproc_job.py.j2' %}\n{% include 'templates/dags/load/__dagster_scheduled_table_tpl.py.j2' %}\n")))),(0,r.kt)("h3",{id:"customize-existing-templates"},"Customize existing templates"),(0,r.kt)("p",null,"Although the options are useful for customizing the generated DAGs, there are situations where we need to be able to ",(0,r.kt)("strong",{parentName:"p"},"dynamically")," apply some of them ",(0,r.kt)("strong",{parentName:"p"},"at runtime"),"."),(0,r.kt)("h4",{id:"transform-parameters"},"Transform parameters"),(0,r.kt)("admonition",{title:"Transform parameters",type:"note"},(0,r.kt)("p",{parentName:"admonition"},"Often data transformation requires ",(0,r.kt)("strong",{parentName:"p"},"parameterized SQL queries")," whose ",(0,r.kt)("strong",{parentName:"p"},"parameters")," should be ",(0,r.kt)("strong",{parentName:"p"},"evaluated at runtime"),".")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"-- ...\nstep1 as(\n  SELECT * FROM step0\n  # highlight-next-line\n  WHERE DAT_EXTRACTION >= '{{date_param_min}}' and DAT_EXTRACTION <= '{{date_param_max}}'\n)\n-- ...\n")),(0,r.kt)("h5",{id:"jobs-variable"},"jobs variable"),(0,r.kt)("p",null,"All Starlake DAG templates for data transformation offer the ability of ",(0,r.kt)("strong",{parentName:"p"},"injecting parameter values")," via the optional definition of a ",(0,r.kt)("strong",{parentName:"p"},"dictionary"),"-like ",(0,r.kt)("strong",{parentName:"p"},"Python variable")," named ",(0,r.kt)("strong",{parentName:"p"},"jobs")," where each ",(0,r.kt)("strong",{parentName:"p"},"key")," represents the ",(0,r.kt)("strong",{parentName:"p"},"name of a transformation")," and its ",(0,r.kt)("strong",{parentName:"p"},"value")," the ",(0,r.kt)("strong",{parentName:"p"},"parameters")," to be passed to the transformation.\nEach entry of this dictionary will be added to the ",(0,r.kt)("strong",{parentName:"p"},"options")," of the corresponding DAG."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="src/main/resources/template/dags/__starlake_airflow_cloud_run_job.py.j2"',title:'"src/main/resources/template/dags/__starlake_airflow_cloud_run_job.py.j2"'},'#...\n#optional variable jobs as a dict of all parameters to apply by job\n#eg jobs = {"task1 domain.task1 name": {"options": "task1 transform options"}, "task2 domain.task2 name": {"options": "task2 transform options"}}\n# highlight-next-line\nsl_job = StarlakeAirflowCloudRunJob(options=dict(options, **sys.modules[__name__].__dict__.get(\'jobs\', {})))\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="ai.starlake.job.IStarlakeJob"',title:'"ai.starlake.job.IStarlakeJob"'},'#...\n    def sl_transform(self, task_id: str, transform_name: str, transform_options: str=None, spark_config: StarlakeSparkConfig=None, **kwargs) -> T:\n        """Transform job.\n        Generate the scheduler task that will run the starlake `transform` command.\n\n    Args:\n            task_id (str): The optional task id.\n            transform_name (str): The transform to run.\n            transform_options (str): The optional transform options to use.\n            spark_config (StarlakeSparkConfig): The optional spark configuration to use.\n\n    Returns:\n            T: The scheduler task.\n        """\n        task_id = f"{transform_name}" if not task_id else task_id\n        arguments = ["transform", "--name", transform_name]\n        # highlight-next-line\n        transform_options = transform_options if transform_options else self.__class__.get_context_var(transform_name, {}, self.options).get("options", "")\n        if transform_options:\n            arguments.extend(["--options", transform_options])\n        return self.sl_job(task_id=task_id, arguments=arguments, spark_config=spark_config, **kwargs)\n#...\n')),(0,r.kt)("p",null,"Because this variable has to be defined in the ",(0,r.kt)("strong",{parentName:"p"},"same module")," as that of the ",(0,r.kt)("strong",{parentName:"p"},"generated DAG")," (",(0,r.kt)("inlineCode",{parentName:"p"},"options=dict(options, **sys.modules[__name__].__dict__.get('jobs', {}))"),"), we need to create a ",(0,r.kt)("strong",{parentName:"p"},"customized DAG template")," that should ",(0,r.kt)("strong",{parentName:"p"},"extend")," the existing one(s), including our specific code."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="metadata/dags/templates/__custom_jobs.py.j2"',title:'"metadata/dags/templates/__custom_jobs.py.j2"'},"#...\n\n# highlight-next-line\njobs = #...\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="metadata/dags/templates/custom_scheduled_task_cloud_run.py.j2"',title:'"metadata/dags/templates/custom_scheduled_task_cloud_run.py.j2"'},"#...\n{% include 'dags/templates/__custom_jobs.py.j2' %} # our custom code\n{% include 'templates/dags/transform/airflow_scheduled_task_cloud_run.py.j2' %} # the template to extend\n")),(0,r.kt)("h4",{id:"airflow-user-defined-macros"},"Airflow user defined macros"),(0,r.kt)("admonition",{title:"Airflow user defined macros",type:"note"},(0,r.kt)("p",{parentName:"admonition"},"Because the ",(0,r.kt)("strong",{parentName:"p"},"SQL parameters")," may be closely related to ",(0,r.kt)("strong",{parentName:"p"},"Airflow context variable"),"(s), their ",(0,r.kt)("strong",{parentName:"p"},"evaluation")," may rely on some ",(0,r.kt)("strong",{parentName:"p"},"Airflow user defined macros"),".")),(0,r.kt)("p",null,"All ",(0,r.kt)("strong",{parentName:"p"},"starlake DAG templates")," for data transformation offer the ability to specify ",(0,r.kt)("strong",{parentName:"p"},"User defined macros")," through the optional definition of a ",(0,r.kt)("strong",{parentName:"p"},"dictionary"),"-like ",(0,r.kt)("strong",{parentName:"p"},"Python variable")," named ",(0,r.kt)("strong",{parentName:"p"},"user_defined_macros"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'#...\n# [START instantiate_dag]\nwith DAG(dag_id=os.path.basename(__file__).replace(".py", "").replace(".pyc", "").lower(),\n         schedule_interval=None if cron == "None" else cron,\n         schedule=schedule,\n         default_args=sys.modules[__name__].__dict__.get(\'default_dag_args\', DEFAULT_DAG_ARGS),\n         catchup=False,\n         # highlight-next-line\n         user_defined_macros=sys.modules[__name__].__dict__.get(\'user_defined_macros\', None),\n         user_defined_filters=sys.modules[__name__].__dict__.get(\'user_defined_filters\', None),\n         tags=set([tag.upper() for tag in tags]),\n         description=description) as dag:\n#...\n')),(0,r.kt)("p",null,"Again, because this variable has to be defined in the ",(0,r.kt)("strong",{parentName:"p"},"same module")," as that of the ",(0,r.kt)("strong",{parentName:"p"},"generated DAG")," (",(0,r.kt)("inlineCode",{parentName:"p"},"user_defined_macros=sys.modules[__name__].__dict__.get('user_defined_macros', None)"),"), we need to create a ",(0,r.kt)("strong",{parentName:"p"},"customized DAG template")," that should ",(0,r.kt)("strong",{parentName:"p"},"extend")," the existing one(s), including our specific code."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="metadata/dags/templates/__custom_jobs.py.j2"',title:'"metadata/dags/templates/__custom_jobs.py.j2"'},'from custom import get_days_interval,get_month_periode_depending_on_start_day_params\n\n# highlight-start\nuser_defined_macros = {\n    "days_interval": get_days_interval,\n    "month_periode_depending_on_start_day": get_month_periode_depending_on_start_day_params\n}\n# highlight-end\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="metadata/dags/templates/custom_scheduled_task_cloud_run.py.j2"',title:'"metadata/dags/templates/custom_scheduled_task_cloud_run.py.j2"'},"#...\n{% include 'dags/templates/__custom_jobs.py.j2' %} # relative to the project metadata folder\n{% include 'templates/dags/transform/airflow_scheduled_task_cloud_run.py.j2' %} # relative to src/main/resources starlake resource directory\n")),(0,r.kt)("p",null,"In addition, a ",(0,r.kt)("strong",{parentName:"p"},"good practice")," is to inject those variables using ",(0,r.kt)("strong",{parentName:"p"},"terraform variables")," ..."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="metadata/dags/templates/__custom_jobs.py.j2"',title:'"metadata/dags/templates/__custom_jobs.py.j2"'},'#...\n\nimport json\n\n# highlight-next-line\njobs = json.loads("""${jobs}""")\n')),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"variables.tf"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-terraform"},'variable "jobs" {\n  type = list(object({\n    name    = string\n    options = string\n  }))\n  default = []\n}\n')),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"main.tf"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-terraform"},'locals {\n  jobs = tomap({\n    for job in var.jobs :\n      "${job.name}" => {options=job.options}\n  })\n\n#...\n}\n\nresource "google_storage_bucket_object" "composer_storage_objects" {\n  for_each = local.composer_storage_objects\n  name     = each.value\n  content  = templatefile(\n    "${path.module}/${each.value}",\n    merge(local.composer_storage_variables, {jobs=jsonencode(local.jobs)}, {clusters=jsonencode(var.clusters)})\n  )\n  bucket   = var.composer_bucket\n}\n')),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"vars_dev.tfvars"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-terraform"},"jobs = [\n    {\n        name    = \"Products.TopSellingProducts\"\n        options = \"{{ days_interval(data_interval_end | ds, var.value.get('TOP_SELLING_PRODUCTS_DELTA', '30')) }}\"\n    },\n\n...\n    {\n        name   = \"Products.MonthlySalesPerProduct\"\n        options = \"{{ month_periode_depending_on_start_day(data_interval_end | ds, var.value.get('SALES_PER_PRODUCT_START_DAY', '1')) }}\"\n    }\n]\n")),(0,r.kt)("p",null,"Finally, we will have to define a specific ",(0,r.kt)("strong",{parentName:"p"},"DAG configuration")," that will make use of our ",(0,r.kt)("strong",{parentName:"p"},"customized DAG template"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="metadata/dags/custom_transform_cloud_run.yml"',title:'"metadata/dags/custom_transform_cloud_run.yml"'},'---\ndag:\n  comment: "agregation dag for domain {{domain}} with cloud run" # will appear as a description of the dag\n  # highlight-next-line\n  template: "custom_scheduled_task_cloud_run.py.j2" # the dag template to use\n  filename: "{{domain}}_agr_cloud_run.py" # the relative path to the outputDir specified as a parameter of the `dag-generate` command where the generated dag file will be copied\n  options:\n    sl_env_var: "{\\"SL_ROOT\\": \\"${root_path}\\", \\"SL_DATASETS\\": \\"${root_path}/datasets\\", \\"SL_TIMEZONE\\": \\"Europe/Paris\\"}"\n    cloud_run_project_id: "${project_id}"\n    cloud_run_job_name: "${job_name}-transform" # cloud run job name for auto jobs\n    cloud_run_job_region: "${region}"\n    cloud_run_async: False # whether or not to use asynchronous cloud run job execution\n# retry_on_failure: True # when asynchronous job execution has been selected, it specifies whether or not we want to use a bash sensor with automatic retry for a specific exit code (implies airflow v2.6+)\n    tags: "{{domain}} {{domain}}_CLOUD_RUN" # tags that will be added to the dag\n    load_dependencies: False # whether or not to add all dependencies as airflow tasks within the resulting dag\n    default_pool: "custom_default_pool" # pool to use for all tasks defined within the dag\n')),(0,r.kt)("h4",{id:"dataproc-cluster-configuration"},"Dataproc cluster configuration"),(0,r.kt)("admonition",{title:"Dataproc cluster configuration",type:"note"},(0,r.kt)("p",{parentName:"admonition"},"All starlake ",(0,r.kt)("strong",{parentName:"p"},"DAG templates for dataproc")," offer the ability to ",(0,r.kt)("strong",{parentName:"p"},"customize")," the ",(0,r.kt)("strong",{parentName:"p"},"configuration")," of the ",(0,r.kt)("strong",{parentName:"p"},"dataproc cluster")," through the implementation of optional ",(0,r.kt)("strong",{parentName:"p"},"Python functions")," that will return instances of ",(0,r.kt)("em",{parentName:"p"},"ai.starlake.gcp.StarlakeDataprocMachineConfig")," given the name of the config to apply, which, by default, will be evaluated to the name of the dag (if the option ",(0,r.kt)("strong",{parentName:"p"},"cluster_config_name")," has not been specified).")),(0,r.kt)(o.Z,{groupId:"schedulers",mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"airflow",label:"Airflow",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="src/main/resources/template/dags/__starlake_airflow_dataproc_job.py.j2"',title:'"src/main/resources/template/dags/__starlake_airflow_dataproc_job.py.j2"'},'#...\n#optional get_dataproc_master_config function that returns an instance of StarlakeAirflowDataprocMasterConfig per dag name\ndataproc_master_config = getattr(sys.modules[__name__], "get_dataproc_master_config", default_dataproc_master_config)\n\n#optional get_dataproc_worker_config function that returns an instance of StarlakeAirflowDataprocWorkerConfig per dag name\ndataproc_worker_config = getattr(sys.modules[__name__], "get_dataproc_worker_config", default_dataproc_worker_config)\n\n#optional get_dataproc_secondary_worker_config function that returns an instance of StarlakeAirflowDataprocWorkerConfig per dag name\ndataproc_secondary_worker_config = getattr(sys.modules[__name__], "get_dataproc_secondary_worker_config", lambda dag_name: None)\n\ncluster_config_name = StarlakeAirflowOptions.get_context_var("cluster_config_name", os.path.basename(__file__).replace(".py", "").replace(".pyc", "").lower(), options)\n\n#optional variable jobs as a dict of all options to apply by job\n#eg jobs = {"task1 domain.task1 name": {"options": "task1 transform options"}, "task2 domain.task2 name": {"options": "task2 transform options"}}\nsl_job = StarlakeAirflowDataprocJob(\n    cluster = StarlakeAirflowDataprocCluster(\n        cluster_config=StarlakeAirflowDataprocClusterConfig(\n            cluster_id=sys.modules[__name__].__dict__.get(\'cluster_id\', cluster_config_name),\n            dataproc_name=sys.modules[__name__].__dict__.get(\'dataproc_name\', None),\n            # highlight-start\n            master_config = dataproc_master_config(cluster_config_name, **sys.modules[__name__].__dict__.get(\'dataproc_master_properties\', {})),\n            worker_config = dataproc_worker_config(cluster_config_name, **sys.modules[__name__].__dict__.get(\'dataproc_worker_properties\', {})),\n            secondary_worker_config = dataproc_secondary_worker_config(cluster_config_name),\n            # highlight-end\n            idle_delete_ttl=sys.modules[__name__].__dict__.get(\'dataproc_idle_delete_ttl\', None),\n            single_node=sys.modules[__name__].__dict__.get(\'dataproc_single_node\', None),\n            options=options,\n            **sys.modules[__name__].__dict__.get(\'dataproc_cluster_properties\', {})\n        ),\n        pool=sys.modules[__name__].__dict__.get(\'pool\', None),\n        options=options\n    ),\n    options=dict(options, **sys.modules[__name__].__dict__.get(\'jobs\', {}))\n)\n'))),(0,r.kt)(l.Z,{value:"dagster",label:"Dagster",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="src/main/resources/template/dags/__starlake_dagster_dataproc_job.py.j2"',title:'"src/main/resources/template/dags/__starlake_dagster_dataproc_job.py.j2"'},'#...\n#optional get_dataproc_master_config function that returns an instance of StarlakeDataprocWorkerConfig per dag name\ndataproc_master_config = getattr(sys.modules[__name__], "get_dataproc_master_config", default_dataproc_master_config)\n\n#optional get_dataproc_worker_config function that returns an instance of StarlakeDataprocWorkerConfig per dag name\ndataproc_worker_config = getattr(sys.modules[__name__], "get_dataproc_worker_config", default_dataproc_worker_config)\n\n#optional get_dataproc_secondary_worker_config function that returns an instance of StarlakeDataprocWorkerConfig per dag name\ndataproc_secondary_worker_config = getattr(sys.modules[__name__], "get_dataproc_secondary_worker_config", lambda dag_name: None)\n\ncluster_config_name = StarlakeOptions.get_context_var("cluster_config_name", os.path.basename(__file__).replace(".py", "").replace(".pyc", "").lower(), options)\n\n#optional variable jobs as a dict of all options to apply by job\n#eg jobs = {"task1 domain.task1 name": {"options": "task1 transform options"}, "task2 domain.task2 name": {"options": "task2 transform options"}}\nsl_job = StarlakeDagsterDataprocJob(\n    cluster_config=StarlakeDataprocClusterConfig(\n        cluster_id=sys.modules[__name__].__dict__.get(\'cluster_id\', cluster_config_name),\n        dataproc_name=sys.modules[__name__].__dict__.get(\'dataproc_name\', None),\n        # highlight-start\n        master_config = dataproc_master_config(cluster_config_name, **sys.modules[__name__].__dict__.get(\'dataproc_master_properties\', {})),\n        worker_config = dataproc_worker_config(cluster_config_name, **sys.modules[__name__].__dict__.get(\'dataproc_worker_properties\', {})),\n        secondary_worker_config = dataproc_secondary_worker_config(cluster_config_name),\n        idle_delete_ttl=sys.modules[__name__].__dict__.get(\'dataproc_idle_delete_ttl\', None),\n        # highlight-end\n        single_node=sys.modules[__name__].__dict__.get(\'dataproc_single_node\', None),\n        options=options,\n        **sys.modules[__name__].__dict__.get(\'dataproc_cluster_properties\', {})\n    ), \n    pool=sys.modules[__name__].__dict__.get(\'pool\', None),\n    options=dict(options, **sys.modules[__name__].__dict__.get(\'jobs\', {}))\n)\n')))),(0,r.kt)("p",null,"Again, because those functions should be implemented in the ",(0,r.kt)("strong",{parentName:"p"},"same module")," as that of the ",(0,r.kt)("strong",{parentName:"p"},"generated DAG")," (",(0,r.kt)("inlineCode",{parentName:"p"},'dataproc_master_config = getattr(sys.modules[__name__], "get_dataproc_master_config", default_dataproc_master_config)'),", ...), we need to create a ",(0,r.kt)("strong",{parentName:"p"},"customized DAG template")," that will allow us to implement those methods."),(0,r.kt)("p",null,"A ",(0,r.kt)("strong",{parentName:"p"},"good practice")," is to inject those configurations via the use of ",(0,r.kt)("strong",{parentName:"p"},"Terraform variables"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="metadata/dags/templates/__custom_dataproc.py.j2"',title:'"metadata/dags/templates/__custom_dataproc.py.j2"'},'import json\n\nfrom ai.starlake.job.airflow import AirflowStarlakeOptions\nfrom ai.starlake.job.airflow.gcp import StarlakeDataprocWorkerConfig\n\nclusters:dict = json.loads("""${clusters}""") # Terraform variable\n\n# ...\n\ndef get_dataproc_worker_config(cluster_config_name: str, **kwargs):\n    # lookup a specific configuration given the name of the cluster configuration\n    worker_config = AirflowStarlakeOptions.get_context_var(cluster_config_name.upper().replace(\'-\', \'_\'), clusters.get(cluster_config_name, None), options, deserialize_json=True)\n    if worker_config:\n        return StarlakeDataprocWorkerConfig(\n            num_instances=int(worker_config.get(\'numWorkers\', 0)),\n            machine_type=worker_config.get(\'workerType\', None),\n            disk_type=None,\n            disk_size=None,\n            options=options,\n            **kwargs\n        )\n    else:\n        return None\n\n# additional dataproc cluster properties\ndataproc_cluster_properties = {\n    "spark:spark.driver.maxResultSize": "15360m",\n    "spark:spark.driver.memory": "30720m",\n}\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="metadata/dags/templates/custom_scheduled_task_dataproc.py.j2"',title:'"metadata/dags/templates/custom_scheduled_task_dataproc.py.j2"'},"# our customized DAG template for data transformation using dataproc\n\n{% include 'dags/templates/__custom_jobs.py.j2' %} # specific code to inject jobs parameters\n{% include 'dags/templates/__custom_dataproc.py.j2' %} # specific code to customize the configuration of our dataproc cluster\n{% include 'templates/dags/transform/scheduled_task_dataproc.py.j2' %} # the base Starlake DAG template that needs to be extended\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="metadata/dags/custom_transform_dataproc.yml"',title:'"metadata/dags/custom_transform_dataproc.yml"'},'---\n# norm_dataproc_domain.sl.yml our DAG configuration using our customized DAG template\n\ndag:\n  comment: "dag for transforming tables for domain {{domain}} with dataproc" # will appear as a description of the dag\n  # highlight-next-line\n  template: "custom_scheduled_task_dataproc.py.j2" # the dag template to use\n  filename: "{{domain}}_norm_dataproc.py" # the relative path to the outputDir specified as a parameter of the `dag-generate` command where the generated dag file will be copied\n  options:\n    sl_env_var: "{\\"SL_ROOT\\": \\"${root_path}\\", \\"SL_DATASETS\\": \\"${root_path}/datasets\\", \\"SL_TIMEZONE\\": \\"Europe/Paris\\"}"\n\n    dataproc_name: "${dataproc_name}"\n    dataproc_project_id: "${project_id}"\n    dataproc_region: "${region}"\n    dataproc_subnet: "${subnet}"\n    dataproc_service_account: "${dataproc_service_account}"\n    dataproc_image_version: "${dataproc_image_version}"\n    dataproc_master_machine_type: "${dataproc_master_machine_type}"\n    dataproc_worker_machine_type: "${dataproc_worker_machine_type}"\n    dataproc_num_workers: "${dataproc_num_workers}"\n    cluster_config_name: "{{domain|lower|replace(\'_\', \'-\')}}-norms"# the name of the cluster configuration that will be looked up\n    spark_config_name: "{{domain|lower|replace(\'_\', \'-\')}}-norms"\n    spark_jar_list: "gs://${artefacts_bucket}/${main_jar}" #gs://${artefacts_bucket}/org.yaml/snakeyaml/2.2/jars/snakeyaml-2.2.jar gs://spark-lib/bigquery/spark-3.5-bigquery-0.35.1.jar gs://${artefacts_bucket}/com.google.cloud.spark/spark-bigquery-with-dependencies_2.12/${spark_bq_version}/spark-bigquery-with-dependencies_2.12-${spark_bq_version}.jar\n    spark_bucket: "${datastore_bucket}"\n\n    tags: "{{domain}} {{domain}}_DATAPROC" # tags that will be added to the dag\n    load_dependencies: False # whether or not to add all dependencies as airflow tasks within the resulting dag\n    default_pool: "custom_default_pool" # pool to use for all tasks defined within the dag\n')),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"variables.tf"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-terraform"},'variable "clusters" {\n  type = map(object({\n    workerType             = string\n    numWorkers             = string\n    sparkExecutorInstances = string\n    numVcpu                = string\n    memAlloc               = string\n  }))\n  default = {}\n}\n')),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"main.tf"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-terraform"},'resource "google_storage_bucket_object" "composer_storage_objects" {\n  for_each = local.composer_storage_objects\n  name     = each.value\n  content  = templatefile(\n    "${path.module}/${each.value}",\n    merge(local.composer_storage_variables, {jobs=jsonencode(local.jobs)}, {clusters=jsonencode(var.clusters)})\n  )\n  bucket   = var.composer_bucket\n}\n')),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("em",{parentName:"li"},"vars_dev.tfvars"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-terraform"},'clusters = {\n    products-norms = {\n        workerType             = ""\n        numWorkers             = "0"\n        sparkExecutorInstances = "0"\n        numVcpu                = "0"\n        memAlloc               = ""\n    },\n    customers-norms = {\n        workerType             = "n1-standard-4"\n        numWorkers             = "4"\n        sparkExecutorInstances = "3"\n        numVcpu                = "16"\n        memAlloc               = "30g"\n    },\n}\n')),(0,r.kt)("h4",{id:"spark-configuration"},"Spark configuration"),(0,r.kt)("admonition",{title:"Spark configuration",type:"note"},(0,r.kt)("p",{parentName:"admonition"},"As for the configuration of the dataproc cluster, it is possible to ",(0,r.kt)("strong",{parentName:"p"},"customize")," the ",(0,r.kt)("strong",{parentName:"p"},"spark configuration")," thanks to the optional implementation of a ",(0,r.kt)("strong",{parentName:"p"},"Python function")," named ",(0,r.kt)("strong",{parentName:"p"},"get_spark_config")," that will return an instance of ",(0,r.kt)("em",{parentName:"p"},"StarlakeSparkConfig")," given the name of a spark configuration to apply, which by default is the name of the transformation (if the option ",(0,r.kt)("strong",{parentName:"p"},"spark_config_name")," has not been defined).")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="src/main/resources/template/dags/__common_airflow.py.j2"',title:'"src/main/resources/template/dags/__common_airflow.py.j2"'},'#...\nspark_config = getattr(sys.modules[__name__], "get_spark_config", default_spark_config)\n')),(0,r.kt)(o.Z,{groupId:"schedulers",mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"airflow",label:"Airflow",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="src/main/resources/template/dags/transform/__airflow_scheduled_task_tpl.py.j2"',title:'"src/main/resources/template/dags/transform/__airflow_scheduled_task_tpl.py.j2"'},"#...\n    def create_task(airflow_task_id: str, task_name: str, task_type: str):\n        spark_config_name=StarlakeAirflowOptions.get_context_var('spark_config_name', task_name.lower(), options)\n        if (task_type == 'task'):\n            return sl_job.sl_transform(\n                task_id=airflow_task_id, \n                transform_name=task_name,\n                # highlight-next-line\n                spark_config=spark_config(spark_config_name, **sys.modules[__name__].__dict__.get('spark_properties', {}))\n            )\n        else:\n            load_domain_and_table = task_name.split(\".\",1)\n            domain = load_domain_and_table[0]\n            table = load_domain_and_table[1]\n            return sl_job.sl_load(\n                task_id=airflow_task_id, \n                domain=domain, \n                table=table,\n                # highlight-next-line\n                spark_config=spark_config(spark_config_name, **sys.modules[__name__].__dict__.get('spark_properties', {}))\n            )\n"))),(0,r.kt)(l.Z,{value:"dagster",label:"Dagster",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="src/main/resources/template/dags/transform/__dagster_scheduled_task_tpl.py.j2"',title:'"src/main/resources/template/dags/transform/__dagster_scheduled_task_tpl.py.j2"'},"#...\ndef create_task(task_id: str, task_name: str, task_type: str, ins: dict={\"start\": In(Nothing)}):\n    spark_config_name=sl_job.get_context_var('spark_config_name', task_name.lower(), options)\n    if (task_type == 'task'):\n        return sl_job.sl_transform(\n            task_id=task_id, \n            transform_name=task_name,\n            # highlight-next-line\n            spark_config=spark_config(spark_config_name, **sys.modules[__name__].__dict__.get('spark_properties', {})),\n            ins=ins,\n        )\n    else:\n        load_domain_and_table = task_name.split(\".\",1)\n        domain = load_domain_and_table[0]\n        table = load_domain_and_table[1]\n        return sl_job.sl_load(\n            task_id=task_id, \n            domain=domain, \n            table=table,\n            # highlight-next-line\n            spark_config=spark_config(spark_config_name, **sys.modules[__name__].__dict__.get('spark_properties', {})),\n            ins=ins,\n        )\n")))),(0,r.kt)("p",null,"Again, because this function should be implemented  in the ",(0,r.kt)("strong",{parentName:"p"},"same module")," as that of the ",(0,r.kt)("strong",{parentName:"p"},"generated DAG"),", we need to create a ",(0,r.kt)("strong",{parentName:"p"},"customized DAG template")," that will allow us to implement this method, and a ",(0,r.kt)("strong",{parentName:"p"},"good practice")," will be to inject those configurations via the use of ",(0,r.kt)("strong",{parentName:"p"},"Terraform variables"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="metadata/dags/templates/__custom_dataproc.py.j2"',title:'"metadata/dags/templates/__custom_dataproc.py.j2"'},"import json\n\nfrom ai.starlake.job import StarlakeSparkConfig\nfrom ai.starlake.job.airflow import AirflowStarlakeOptions\n\nclusters:dict = json.loads(\"\"\"${clusters}\"\"\") # Terraform variable\n\ndef get_spark_config(spark_config_name: str, **kwargs):\n    # use of the Terraform variable to lookup the spark configuration\n    spark_config = AirflowStarlakeOptions.get_context_var(spark_config_name.upper().replace('-', '_'), clusters.get(spark_config_name, None), options, deserialize_json=True)\n    if spark_config:\n        return StarlakeSparkConfig(\n            memory=spark_config.get('memAlloc', None),\n            cores=int(spark_config.get('numVcpu', 0)),\n            instances=int(spark_config.get('sparkExecutorInstances', 0)),\n            cls_options=AirflowStarlakeOptions(),\n            options=options,\n            **kwargs\n        )\n    else:\n        return None\n\n#...\n")),(0,r.kt)("h2",{id:"dependencies"},"Dependencies"),(0,r.kt)("p",null,"For any transformation, Starlake is able to calculate all its dependencies towards other tasks or loads thanks to the ",(0,r.kt)("strong",{parentName:"p"},"analysis")," of ",(0,r.kt)("strong",{parentName:"p"},"SQL queries"),"."),(0,r.kt)("p",null,"As seen previously, the ",(0,r.kt)("strong",{parentName:"p"},"load_dependencies")," option defines whether or not we wish to recursively ",(0,r.kt)("strong",{parentName:"p"},"generate all the dependencies")," associated with each task for which the transformation DAG must be generated (False by default). If we choose to not generate those dependencies, the corresponding DAG will be scheduled using the ",(0,r.kt)("strong",{parentName:"p"},"Airflow's data-aware scheduling mechanism"),"."),(0,r.kt)("p",null,"All ",(0,r.kt)("strong",{parentName:"p"},"dependencies")," for data transformation are ",(0,r.kt)("strong",{parentName:"p"},"available")," in the generated DAG via the ",(0,r.kt)("strong",{parentName:"p"},"Python")," dictionary ",(0,r.kt)("strong",{parentName:"p"},"variable")," ",(0,r.kt)("strong",{parentName:"p"},"task_deps"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'task_deps=json.loads("""[ {\n  "data" : {\n    "name" : "Customers.HighValueCustomers",\n    "typ" : "task",\n    "parent" : "Customers.CustomerLifeTimeValue",\n    "parentTyp" : "task",\n    "parentRef" : "CustomerLifetimeValue",\n    "sink" : "Customers.HighValueCustomers"\n  },\n  "children" : [ {\n    "data" : {\n      "name" : "Customers.CustomerLifeTimeValue",\n      "typ" : "task",\n      "parent" : "starbake.Customers",\n      "parentTyp" : "table",\n      "parentRef" : "starbake.Customers",\n      "sink" : "Customers.CustomerLifeTimeValue"\n    },\n    "children" : [ {\n      "data" : {\n        "name" : "starbake.Customers",\n        "typ" : "table",\n        "parentTyp" : "unknown"\n      },\n      "task" : false\n    }, {\n      "data" : {\n        "name" : "starbake.Orders",\n        "typ" : "table",\n        "parentTyp" : "unknown"\n      },\n      "task" : false\n    } ],\n    "task" : true\n  } ],\n  "task" : true\n} ]""")\n')),(0,r.kt)("h3",{id:"inline"},"Inline"),(0,r.kt)("p",null,"In this strategy (",(0,r.kt)("strong",{parentName:"p"},"load_dependencies")," = ",(0,r.kt)("em",{parentName:"p"},"True"),"), all the dependencies related to the transformation will be generated."),(0,r.kt)(o.Z,{groupId:"schedulers",mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"airflow",label:"Airflow",mdxType:"TabItem"},(0,r.kt)("p",null,(0,r.kt)("img",{src:a(50500).Z,width:"1818",height:"476"}))),(0,r.kt)(l.Z,{value:"dagster",label:"Dagster",mdxType:"TabItem"},(0,r.kt)("p",null,(0,r.kt)("img",{src:a(23800).Z,width:"1248",height:"1172"})))),(0,r.kt)("h3",{id:"external-state-change"},"External state change"),(0,r.kt)("p",null,"In this strategy (",(0,r.kt)("strong",{parentName:"p"},"load_dependencies")," = ",(0,r.kt)("em",{parentName:"p"},"False"),"), the default strategy, the scheduler will ",(0,r.kt)("strong",{parentName:"p"},"launch")," a ",(0,r.kt)("strong",{parentName:"p"},"run")," for the corresponding ",(0,r.kt)("strong",{parentName:"p"},"transform DAG")," if its ",(0,r.kt)("strong",{parentName:"p"},"dependencies")," are ",(0,r.kt)("strong",{parentName:"p"},"met"),"."),(0,r.kt)("h4",{id:"airflow-data-aware-scheduling"},"Airflow Data-aware scheduling"),(0,r.kt)(o.Z,{groupId:"schedulers",mdxType:"Tabs"},(0,r.kt)(l.Z,{value:"airflow",label:"Airflow",mdxType:"TabItem"},(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"Airflow Data-aware scheduling")),(0,r.kt)("p",null,"In this strategy, a ",(0,r.kt)("strong",{parentName:"p"},"schedule")," will be created to ",(0,r.kt)("strong",{parentName:"p"},"check")," if the ",(0,r.kt)("strong",{parentName:"p"},"dependencies")," are ",(0,r.kt)("strong",{parentName:"p"},"met")," via the use of ",(0,r.kt)("strong",{parentName:"p"},"Airflow Datasets"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="src/main/resources/template/dags/transform/__airflow_scheduled_task_tpl.py.j2"',title:'"src/main/resources/template/dags/transform/__airflow_scheduled_task_tpl.py.j2"'},"#...\nschedule = None\n\ndatasets: Set[str] = []\n\n_extra_dataset: Union[dict, None] = sys.modules[__name__].__dict__.get('extra_dataset', None)\n\n_extra_dataset_parameters = '?' + '&'.join(list(f'{k}={v}' for (k,v) in _extra_dataset.items())) if _extra_dataset else ''\n\n# if you choose to not load the dependencies, a schedule will be created to check if the dependencies are met\ndef _load_datasets(task: dict):\n    if 'children' in task:\n        for child in task['children']:\n            datasets.append(keep_ascii_only(child['data']['name']).lower())\n            _load_datasets(child)\n\nif load_dependencies.lower() != 'true':\n    for task in task_deps:\n        _load_datasets(task)\n    schedule = list(map(lambda dataset: Dataset(dataset + _extra_dataset_parameters), datasets))\n\n#...\n\nwith DAG(dag_id=os.path.basename(__file__).replace(\".py\", \"\").replace(\".pyc\", \"\").lower(),\n         schedule_interval=None if cron == \"None\" else cron,\n         # highlight-next-line\n         schedule=schedule,\n         default_args=sys.modules[__name__].__dict__.get('default_dag_args', DEFAULT_DAG_ARGS),\n         catchup=False,\n         user_defined_macros=sys.modules[__name__].__dict__.get('user_defined_macros', None),\n         user_defined_filters=sys.modules[__name__].__dict__.get('user_defined_filters', None),\n         tags=set([tag.upper() for tag in tags]),\n         description=description) as dag:\n#...\n")),(0,r.kt)("p",null,"Those ",(0,r.kt)("strong",{parentName:"p"},"required Datasets")," are ",(0,r.kt)("strong",{parentName:"p"},"updated")," for ",(0,r.kt)("strong",{parentName:"p"},"each")," ",(0,r.kt)("strong",{parentName:"p"},"load")," and ",(0,r.kt)("strong",{parentName:"p"},"task")," that have been ",(0,r.kt)("strong",{parentName:"p"},"executed"),"."),(0,r.kt)("p",null,"The ",(0,r.kt)("em",{parentName:"p"},"ai.starlake.airflow.StarlakeAirflowJob")," class is responsible for ",(0,r.kt)("strong",{parentName:"p"},"recording")," the ",(0,r.kt)("strong",{parentName:"p"},"outlets")," related to the execution of each starlake command."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="ai.starlake.airflow.StarlakeAirflowJob"',title:'"ai.starlake.airflow.StarlakeAirflowJob"'},"def __init__(\n    self,\n    pre_load_strategy: Union[StarlakePreLoadStrategy, str, None],\n    options: dict=None,\n    **kwargs) -> None:\n    #...\n    self.outlets: List[Dataset] = kwargs.get('outlets', [])\n\ndef sl_import(self, task_id: str, domain: str, **kwargs) -> BaseOperator:\n    #...\n    dataset = Dataset(keep_ascii_only(domain).lower())\n    self.outlets += kwargs.get('outlets', []) + [dataset]\n    #...\n\ndef sl_load(\n    self,\n    task_id: str,\n    domain: str,\n    table: str,\n    spark_config: StarlakeSparkConfig=None,\n    **kwargs) -> BaseOperator:\n    #...\n    dataset = Dataset(keep_ascii_only(f'{domain}.{table}').lower())\n    self.outlets += kwargs.get('outlets', []) + [dataset]\n    #...\n\ndef sl_transform(\n    self,\n    task_id: str,\n    transform_name: str,\n    transform_options: str=None,\n    spark_config: StarlakeSparkConfig=None,\n    **kwargs) -> BaseOperator:\n    #...\n    dataset = Dataset(keep_ascii_only(transform_name).lower())\n    self.outlets += kwargs.get('outlets', []) + [dataset]\n    #...\n")),(0,r.kt)("p",null,"All the ",(0,r.kt)("strong",{parentName:"p"},"outlets")," that have been ",(0,r.kt)("strong",{parentName:"p"},"recorded")," are ",(0,r.kt)("strong",{parentName:"p"},"available")," in the ",(0,r.kt)("em",{parentName:"p"},"outlets")," ",(0,r.kt)("em",{parentName:"p"},"property")," of the ",(0,r.kt)("strong",{parentName:"p"},"starlake concrete factory class instance")," and are used at the very ",(0,r.kt)("strong",{parentName:"p"},"last step")," of the corresponding DAG to update the Datasets."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="src/main/resources/template/dags/transform/__airflow_scheduled_task_tpl.py.j2"',title:'"src/main/resources/template/dags/transform/__airflow_scheduled_task_tpl.py.j2"'},'    end = sl_job.dummy_op(task_id="end", outlets=[Dataset(keep_ascii_only(dag.dag_id))]+list(map(lambda x: Dataset(x.uri + _extra_dataset_parameters), sl_job.outlets)))\n')),(0,r.kt)("p",null,(0,r.kt)("img",{src:a(86624).Z,width:"1694",height:"1360"})),(0,r.kt)("p",null,"In conjonction with the Starlake dag generation, the outlets property can be used to ",(0,r.kt)("strong",{parentName:"p"},"schedule")," ",(0,r.kt)("strong",{parentName:"p"},"effortless")," ",(0,r.kt)("strong",{parentName:"p"},"DAGs")," that will run the transform commands.")),(0,r.kt)(l.Z,{value:"dagster",label:"Dagster",mdxType:"TabItem"},(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"Dagster Multi Asset Sensor")),(0,r.kt)("p",null,"In this strategy, a ",(0,r.kt)("strong",{parentName:"p"},"sensor")," will be created to ",(0,r.kt)("strong",{parentName:"p"},"check")," if the ",(0,r.kt)("strong",{parentName:"p"},"dependencies")," are ",(0,r.kt)("strong",{parentName:"p"},"met")," via the use of ",(0,r.kt)("strong",{parentName:"p"},"Dagster Assets"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="src/main/resources/template/dags/transform/__dagster_scheduled_task_tpl.py.j2"',title:'"src/main/resources/template/dags/transform/__dagster_scheduled_task_tpl.py.j2"'},"#...\nfrom dagster import AssetKey, MultiAssetSensorDefinition, MultiAssetSensorEvaluationContext, SkipReason, Definitions\n\n# if you want to load dependencies, set load_dependencies to True in the options\nload_dependencies: bool = StarlakeDagsterJob.get_context_var(var_name='load_dependencies', default_value='False', options=options).lower() == 'true'\n\n# highlight-next-line\nsensor = None\n\n# if you choose to not load the dependencies, a sensor will be created to check if the dependencies are met\n# highlight-next-line\nif not load_dependencies:\n    assets: Set[str] = []\n\n    def load_assets(task: dict):\n        if 'children' in task:\n            for child in task['children']:\n                assets.append(sanitize_id(child['data']['name']))\n                load_assets(child)\n\n    for task in task_deps:\n        load_assets(task)\n\n    def multi_asset_sensor_with_skip_reason(context: MultiAssetSensorEvaluationContext):\n        asset_events = context.latest_materialization_records_by_key()\n        if all(asset_events.values()):\n            context.advance_all_cursors()\n            return RunRequest()\n        elif any(asset_events.values()):\n            materialized_asset_key_strs = [\n                key.to_user_string() for key, value in asset_events.items() if value\n            ]\n            not_materialized_asset_key_strs = [\n                key.to_user_string() for key, value in asset_events.items() if not value\n            ]\n            return SkipReason(\n                f\"Observed materializations for {materialized_asset_key_strs}, \"\n                f\"but not for {not_materialized_asset_key_strs}\"\n            )\n        else:\n            return SkipReason(\"No materializations observed\")\n\n    sensor = MultiAssetSensorDefinition(\n        name = f'{job_name}_sensor',\n        monitored_assets = list(map(lambda asset: AssetKey(asset), assets)),\n        asset_materialization_fn = multi_asset_sensor_with_skip_reason,\n        minimum_interval_seconds = 60,\n        description = f\"Sensor for {job_name}\",\n        job_name = job_name,\n    )\n#...\ndefs = Definitions(\n   jobs=[generate_job()],\n   schedules=crons,\n   # highlight-next-line\n   sensors=[sensor] if sensor else [],\n)\n")),(0,r.kt)("p",null,"Those ",(0,r.kt)("strong",{parentName:"p"},"required Assets")," are ",(0,r.kt)("strong",{parentName:"p"},"materialized")," for ",(0,r.kt)("strong",{parentName:"p"},"each")," ",(0,r.kt)("strong",{parentName:"p"},"load")," and ",(0,r.kt)("strong",{parentName:"p"},"task")," that have been ",(0,r.kt)("strong",{parentName:"p"},"executed"),"."),(0,r.kt)("p",null,"The ",(0,r.kt)("em",{parentName:"p"},"ai.starlake.dagster.StarlakeDagsterJob")," class is responsible for ",(0,r.kt)("strong",{parentName:"p"},"recording")," the ",(0,r.kt)("strong",{parentName:"p"},"assets")," related to the execution of each starlake command."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="ai.starlake.dagster.StarlakeDagsterJob"',title:'"ai.starlake.dagster.StarlakeDagsterJob"'},"def sl_import(self, task_id: str, domain: str, **kwargs) -> NodeDefinition:\n    # highlight-next-line\n    kwargs.update({'asset': AssetKey(sanitize_id(domain))})\n    #...\n\ndef sl_load(self, task_id: str, domain: str, table: str, spark_config: StarlakeSparkConfig=None, **kwargs) -> NodeDefinition:\n    # highlight-next-line\n    kwargs.update({'asset': AssetKey(sanitize_id(f\"{domain}.{table}\"))})\n    #...\n\ndef sl_transform(self, task_id: str, transform_name: str, transform_options: str = None, spark_config: StarlakeSparkConfig = None, **kwargs) -> NodeDefinition:\n    # highlight-next-line\n    kwargs.update({'asset': AssetKey(sanitize_id(transform_name))})\n    #...\n\n")),(0,r.kt)("p",null,"Each corresponding ",(0,r.kt)("strong",{parentName:"p"},"asset")," will be then ",(0,r.kt)("strong",{parentName:"p"},"materialized")," at run time through the execution of the ",(0,r.kt)("strong",{parentName:"p"},"Dagset op")," defined within the ",(0,r.kt)("strong",{parentName:"p"},"sl_job")," function of the concrete factory class that has been instantiated by the template."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="ai.starlake.dagster.shell.StarlakeShellJob"',title:'"ai.starlake.dagster.shell.StarlakeShellJob"'},'def sl_job(self, task_id: str, arguments: list, spark_config: StarlakeSparkConfig=None, **kwargs) -> NodeDefinition:\n    """Overrides IStarlakeJob.sl_job()\n    Generate the Dagster node that will run the starlake command.\n\n    Args:\n        task_id (str): The required task id.\n        arguments (list): The required arguments of the starlake command to run.\n\n    Returns:\n        OpDefinition: The Dastger node.\n    """\n    command = self.__class__.get_context_var("SL_STARLAKE_PATH", "starlake", self.options) + f" {\' \'.join(arguments)}"\n\n    # highlight-next-line\n    asset_key: AssetKey = kwargs.get("asset", None)\n\n    @op(\n        name=task_id,\n        ins=kwargs.get("ins", {}),\n        out={kwargs.get("out", "result"): Out(str)},\n    )\n    def job(context, **kwargs):\n        output, return_code = execute_shell_command(\n            shell_command=command,\n            output_logging="STREAM",\n            log=context.log,\n            cwd=self.sl_root,\n            env=self.sl_env_vars,\n            log_shell_command=True,\n        )\n\n        if return_code:\n            raise Failure(description=f"Starlake command {command} execution failed with output: {output}")\n\n        if asset_key:\n            # highlight-next-line\n            yield AssetMaterialization(asset_key=asset_key.path, description=kwargs.get("description", f"Starlake command {command} execution succeeded"))\n\n        yield Output(value=output, output_name="result")\n\n    return job\n\n')),(0,r.kt)("p",null,(0,r.kt)("img",{src:a(92930).Z,width:"1456",height:"1270"})))))}k.isMDXComponent=!0},14614:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/ack-a35e707d19fbbf15c9c9a02256ae8e37.png"},14847:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/bash-3c5283201692a4bb3517799af2f70792.png"},26781:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/cloudRunAsynchronous-9af5b54671b432e559c6ebcfc1fd6c33.png"},20393:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/cloudRunSynchronous-a9a0857335e66644e102bcbb4a5c91c5.png"},43477:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/dataproc-2c84f297fde1311d82b631c8c0e12ec3.png"},29058:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/imported-f4e59acc4626a3d1be6243445221df52.png"},6430:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/none-6bc3ff2c4625a2803da2f93ba1cbdfc7.png"},21204:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/pending-503f251179d0e924bddf6fd3661e9d9b.png"},50500:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/transformWithDependencies-6ce6dbb28ddf0b9018924c5ce00a2a44.png"},86624:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/transformWithoutDependencies-ada333ac7d75af94bfffa13f7002e601.png"},7751:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/ack-cb876d6a841b0a6cdc7ba3fcaef8d58f.png"},94056:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/imported-51a3bc590106303bcb5928f0081b7230.png"},72646:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/none-0d95c524db0294eb1ce72aeec1ffaeb0.png"},46253:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/pending-61ae544a143576d9b20ee57cf4790d25.png"},29115:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/shell-f558eed9b581045c04ffa229bd353a24.png"},23800:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/transformWithDependencies-2a4e9bfe4d23a8268dbdf11969610f48.png"},92930:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/transformWithoutDependencies-ae80fd075e409206d2ba9fe652fb65bc.png"}}]);