"use strict";(self.webpackChunkstarlake=self.webpackChunkstarlake||[]).push([[3083],{15680:(e,r,t)=>{t.d(r,{xA:()=>c,yg:()=>y});var n=t(96540);function a(e,r,t){return r in e?Object.defineProperty(e,r,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[r]=t,e}function o(e,r){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);r&&(n=n.filter((function(r){return Object.getOwnPropertyDescriptor(e,r).enumerable}))),t.push.apply(t,n)}return t}function l(e){for(var r=1;r<arguments.length;r++){var t=null!=arguments[r]?arguments[r]:{};r%2?o(Object(t),!0).forEach((function(r){a(e,r,t[r])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(r){Object.defineProperty(e,r,Object.getOwnPropertyDescriptor(t,r))}))}return e}function i(e,r){if(null==e)return{};var t,n,a=function(e,r){if(null==e)return{};var t,n,a={},o=Object.keys(e);for(n=0;n<o.length;n++)t=o[n],r.indexOf(t)>=0||(a[t]=e[t]);return a}(e,r);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)t=o[n],r.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=n.createContext({}),u=function(e){var r=n.useContext(s),t=r;return e&&(t="function"==typeof e?e(r):l(l({},r),e)),t},c=function(e){var r=u(e.components);return n.createElement(s.Provider,{value:r},e.children)},p="mdxType",f={inlineCode:"code",wrapper:function(e){var r=e.children;return n.createElement(n.Fragment,{},r)}},d=n.forwardRef((function(e,r){var t=e.components,a=e.mdxType,o=e.originalType,s=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),p=u(t),d=a,y=p["".concat(s,".").concat(d)]||p[d]||f[d]||o;return t?n.createElement(y,l(l({ref:r},c),{},{components:t})):n.createElement(y,l({ref:r},c))}));function y(e,r){var t=arguments,a=r&&r.mdxType;if("string"==typeof e||a){var o=t.length,l=new Array(o);l[0]=d;var i={};for(var s in r)hasOwnProperty.call(r,s)&&(i[s]=r[s]);i.originalType=e,i[p]="string"==typeof e?e:a,l[1]=i;for(var u=2;u<o;u++)l[u]=t[u];return n.createElement.apply(null,l)}return n.createElement.apply(null,t)}d.displayName="MDXCreateElement"},40277:(e,r,t)=>{t.r(r),t.d(r,{assets:()=>s,contentTitle:()=>l,default:()=>f,frontMatter:()=>o,metadata:()=>i,toc:()=>u});var n=t(9668),a=(t(96540),t(15680));const o={},l="Azure Synapse Spark Pools",i={unversionedId:"platforms/azure",id:"version-1.0.0/platforms/azure",title:"Azure Synapse Spark Pools",description:"Running Locally",source:"@site/versioned_docs/version-1.0.0/0700-platforms/020.azure.md",sourceDirName:"0700-platforms",slug:"/platforms/azure",permalink:"/starlake/docs/1.0.0/platforms/azure",draft:!1,editUrl:"https://github.com/starlake-ai/starlake/edit/master/docs/versioned_docs/version-1.0.0/0700-platforms/020.azure.md",tags:[],version:"1.0.0",sidebarPosition:20,frontMatter:{},sidebar:"starlakeSidebar",previous:{title:"aws",permalink:"/starlake/docs/1.0.0/platforms/aws"},next:{title:"Databricks on any cloud",permalink:"/starlake/docs/1.0.0/platforms/databricks"}},s={},u=[{value:"Running Locally",id:"running-locally",level:2},{value:"Running on Azure",id:"running-on-azure",level:2}],c={toc:u},p="wrapper";function f(e){let{components:r,...t}=e;return(0,a.yg)(p,(0,n.A)({},c,t,{components:r,mdxType:"MDXLayout"}),(0,a.yg)("h1",{id:"azure-synapse-spark-pools"},"Azure Synapse Spark Pools"),(0,a.yg)("h2",{id:"running-locally"},"Running Locally"),(0,a.yg)("p",null,"Starlake need to access ADFS. You need to provide the credentials in one of the three ways below:"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Through a core-site.xml file present in the classpath (you'll probably use this method when running the ingestion process from your laptop):")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-xml"},' <?xml version="1.0" encoding="UTF-8"?>\n <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n <configuration>\n     <property>\n         <name>fs.azure.account.key.ebizcomet.dfs.core.windows.net</name>\n         <value>*******==</value>\n     </property>\n     <property>\n         <name>fs.default.name</name>\n         <value>abfs://cometfs@ebizcomet.dfs.core.windows.net/</value>\n     </property>\n </configuration>\n')),(0,a.yg)("h2",{id:"running-on-azure"},"Running on Azure"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"At cluster creation as specified ",(0,a.yg)("inlineCode",{parentName:"li"},"here <https://docs.microsoft.com/fr-fr/azure/databricks/data/data-sources/azure/azure-datalake-gen2#rdd-api>"),"_.\n(you'll probably use this method on a production cluster)")),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Through a specific application.conf file in the starlake-assembly.jar classpath.\nYou must add the spark.hadoop. prefix to the corresponding Hadoop configuration keys to propagate them to the Hadoop configurations that are used used in the Starlake Spark Job.")))}f.isMDXComponent=!0}}]);