"use strict";(self.webpackChunkstarlake=self.webpackChunkstarlake||[]).push([[1186],{76935:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>s,metadata:()=>i,toc:()=>c});var r=a(74848),n=a(28453);const s={},o="Google Cloud",i={id:"configuration/platforms/gcp",title:"Google Cloud",description:"Two options are available when running on Google Cloud:",source:"@site/docs/0500-configuration/0700-platforms/040.gcp.md",sourceDirName:"0500-configuration/0700-platforms",slug:"/configuration/platforms/gcp",permalink:"/starlake/docs/next/configuration/platforms/gcp",draft:!1,unlisted:!1,editUrl:"https://github.com/starlake-ai/starlake/edit/master/docs/docs/0500-configuration/0700-platforms/040.gcp.md",tags:[],version:"current",sidebarPosition:40,frontMatter:{},sidebar:"starlakeSidebar",previous:{title:"Databricks on any cloud",permalink:"/starlake/docs/next/configuration/platforms/databricks"},next:{title:"Local filesystem",permalink:"/starlake/docs/next/configuration/platforms/file"}},l={},c=[{value:"Production Mode with Google Dataproc",id:"production-mode-with-google-dataproc",level:2},{value:"Create a service account",id:"create-a-service-account",level:3},{value:"Create a Dataproc cluster",id:"create-a-dataproc-cluster",level:3},{value:"Create a Starlake job",id:"create-a-starlake-job",level:3},{value:"Ingest your data",id:"ingest-your-data",level:3},{value:"Running Locally with Spark (Dev. Mode)",id:"running-locally-with-spark-dev-mode",level:2}];function d(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,n.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.h1,{id:"google-cloud",children:"Google Cloud"}),"\n",(0,r.jsx)(t.p,{children:"Two options are available when running on Google Cloud:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsx)(t.li,{children:"Production Mode: Running the jobs against BigQuery with Cloud Run Jobs or the Dataproc cluster executing on Google Cloud"}),"\n",(0,r.jsx)(t.li,{children:"Development Mode: Running the jobs against BigQuery with a Spark Job running on your computer."}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"In both cases, the metadata are stored in Google CLoud Storage, only the cloud run / spark job is either running\non Dataproc or locally on your computer"}),"\n",(0,r.jsxs)(t.p,{children:["Running on Cloud run is straightforward. Please take a look at the DAG template for ",(0,r.jsx)(t.img,{src:"https://github.com/starlake-ai/starlake/blob/master/src/main/resources/templates/dags/load/scheduled_table_cloudrun.py.j2",alt:"tables"})," and ",(0,r.jsx)(t.img,{src:"https://github.com/starlake-ai/starlake/blob/master/src/main/resources/templates/dags/transform/scheduled_task_bash.py.j2",alt:"transforms"})]}),"\n",(0,r.jsx)(t.h2,{id:"production-mode-with-google-dataproc",children:"Production Mode with Google Dataproc"}),"\n",(0,r.jsx)(t.p,{children:"Follow the steps below to run starlake on Google Cloud:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsx)(t.li,{children:"Create a service account"}),"\n",(0,r.jsx)(t.li,{children:"Create a Dataproc cluster"}),"\n",(0,r.jsx)(t.li,{children:"Create a Starlake job"}),"\n",(0,r.jsx)(t.li,{children:"Ingest your data"}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"create-a-service-account",children:"Create a service account"}),"\n",(0,r.jsxs)(t.p,{children:["Create a bucket and name it for example ",(0,r.jsx)(t.code,{children:"starlake-app"}),". This bucket will have the following purposes:"]}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsx)(t.li,{children:"Store Starlake jars"}),"\n",(0,r.jsx)(t.li,{children:"Store Starlake metadata"}),"\n",(0,r.jsx)(t.li,{children:"Crteate datasets and tables in BigQuery. since we are ingesting into BigQuery, no parquet file is store on Cloud Storage."}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"Create a service account and assign it the Storage Admin and BigQuery Admin roles. Depending on your security configuration,\nyou may be required to use lower access rights."}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"Create Service Account",src:a(56973).A+"",title:"create service account",width:"850",height:"627"})}),"\n",(0,r.jsxs)(t.p,{children:["Copy your starlake project to the ",(0,r.jsx)(t.code,{children:"gd://starlake-app/mnt/userguide"})," directory using the following script run from\nthe ",(0,r.jsx)(t.a,{href:"https://github.com/starlake-ai/starlake/tree/master/samples/cloud",children:"samples/cloud"})," folder:"]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-bash",children:"\ngsutil cp -r userguide/ gs://staralke-app/mnt/starlake-app/\n\n"})}),"\n",(0,r.jsx)(t.h3,{id:"create-a-dataproc-cluster",children:"Create a Dataproc cluster"}),"\n",(0,r.jsx)(t.p,{children:"Dataproc is the Google service for running Spark jobs. After enabling the Dataproc API, create a cluster and define\nthe environnment variables below:"}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"Create Dataproc cluster",src:a(14036).A+"",title:"Create Dataproc cluster",width:"850",height:"579"})}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Environnement variable"}),(0,r.jsx)(t.th,{children:"Value"}),(0,r.jsx)(t.th,{children:"Description"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"SL_ROOT"}),(0,r.jsx)(t.td,{children:"/mnt/starlake-app"}),(0,r.jsx)(t.td,{children:"It should reference the base directory where your starlake metadata is located"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"SL_AUDIT_SINK_TYPE"}),(0,r.jsx)(t.td,{children:"BigQuerySink"}),(0,r.jsx)(t.td,{children:"Where to save audit logs. Here we decide to save it in BigQuery. Tos ave it as a hive table or file on the cloud storage, set it to FsSink"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"SL_FS"}),(0,r.jsx)(t.td,{children:"gs://starlake-app"}),(0,r.jsx)(t.td,{children:"Filesystem. Reference the cloud storage bucket where all the files will be located."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"TEMPORARY_GCS_BUCKET"}),(0,r.jsx)(t.td,{children:"starlake-app"}),(0,r.jsx)(t.td,{children:"Bucket name where Google Cloud API store temporary files when saving data to BigQuery"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:"SL_ENV"}),(0,r.jsx)(t.td,{children:"BQ"}),(0,r.jsxs)(t.td,{children:["Starlake Env variables. This will instruct Starlake to use the env.",(0,r.jsx)(t.code,{children:"BQ"}),".sl.yml file located at the root of your project when running comet. The ",(0,r.jsx)(t.code,{children:"sink_type"})," in this file instruct Starlake to save datasets in BigQuery instead of parquet files in Cloud Storage."]})]})]})]}),"\n",(0,r.jsx)(t.p,{children:"To create the dataproc cluster using the CLI instead, just run the command below:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-bash",children:"\ngcloud dataproc clusters create cluster-88ea \\\n      --region europe-west1 \\\n      --zone europe-west1-b \\\n      --master-machine-type n1-standard-4 \\\n      --master-boot-disk-size 500 \\\n      --num-workers 2 \\\n      --worker-machine-type n1-standard-4 \\\n      --worker-boot-disk-size 500 \\\n      --image-version 2.0-debian10 \\\n      --project my-starlake-project-id \\\n      --properties \\\n        spark-env:SL_AUDIT_SINK_TYPE=BigQuerySink, \\\n        spark-env:SL_ENV=BQ, \\\n        spark-env:SL_FS=gs://starlake-app, \\\n        spark-env:SL_ROOT=/mnt/userguide, \\\n        spark-env:TEMPORARY_GCS_BUCKET=starlake-app\n\n"})}),"\n",(0,r.jsx)(t.h3,{id:"create-a-starlake-job",children:"Create a Starlake job"}),"\n",(0,r.jsx)(t.p,{children:"Assuming that you copied the starlake assembly to the root for the gs://starlake-app bucket,\njust create using the POST request below or through the user interface."}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"Create Dataproc job",src:a(44881).A+"",title:"Create Dataproc job",width:"850",height:"1205"})}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-bash",children:'\nPOST /v1/projects/my-starlake-project-id/regions/europe-west1/jobs:submit/\n{\n  "projectId": "my-starlake-project",\n  "job": {\n    "placement": {},\n    "statusHistory": [],\n    "reference": {\n      "jobId": "job-aacf2cd5",\n      "projectId": "my-starlake-project-id"\n    },\n    "sparkJob": {\n      "mainClass": "ai.starlake.job.Main",\n      "properties": {},\n      "jarFileUris": [\n        "gs://starlake-app/starlake-VERSION-assembly.jar",\n        "gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar"\n      ],\n      "args": [\n        "import"\n      ]\n    }\n  }\n}\n\n'})}),"\n",(0,r.jsxs)(t.p,{children:["We create above the import job. To create the watch job, just create a new job and replace ",(0,r.jsx)(t.code,{children:"import"}),"by ",(0,r.jsx)(t.code,{children:"watch"})," in\nthe ",(0,r.jsx)(t.code,{children:"Arguments"})," field as shown below:"]}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"Watch job",src:a(83973).A+"",title:"Watch job",width:"850",height:"122"})}),"\n",(0,r.jsx)(t.h3,{id:"ingest-your-data",children:"Ingest your data"}),"\n",(0,r.jsxs)(t.p,{children:["Start the ",(0,r.jsx)(t.code,{children:"import"})," job first and then the ",(0,r.jsx)(t.code,{children:"watch"})," job. The execution logs are available in the Dataproc UI:"]}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"tasks runs",src:a(17507).A+"",title:"tasks runs",width:"850",height:"98"})}),"\n",(0,r.jsxs)(t.p,{children:["Since we ingested data into BigQuery, We find it available in BigQuery datasets and tables\n",(0,r.jsx)(t.img,{alt:"starlake watch",src:a(85915).A+"",title:"starlake watch",width:"1560",height:"1064"})]}),"\n",(0,r.jsxs)(t.p,{children:["The audit log for the above jobs are available in a BigQuery table since we set the ",(0,r.jsx)(t.code,{children:"SL_AUDIT_SINK_TYPE=BigQuerySink"})," environnment variable."]}),"\n",(0,r.jsx)(t.h2,{id:"running-locally-with-spark-dev-mode",children:"Running Locally with Spark (Dev. Mode)"}),"\n",(0,r.jsx)(t.p,{children:"When describing your data format, you may need to run, for testing purposes, your job locally against the remote GCP Project hosting your BigQuery datasets.\nIn that case, you need to set the GCP_PROJECT env var and create a custom core-site.xml in your classpath as described below:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-xml",children:" <configuration>\n     <property>\n         <name>fs.gs.impl</name>\n         <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</value>\n     </property>\n     <property>\n         <name>fs.AbstractFileSystem.gs.impl</name>\n         <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS</value>\n     </property>\n     <property>\n         <name>fs.gs.project.id</name>\n         <value>myproject-1234</value>\n     </property>\n     <property>\n         <name>google.cloud.auth.service.account.enable</name>\n         <value>true</value>\n     </property>\n     <property>\n         <name>google.cloud.auth.service.account.json.keyfile</name>\n         <value>/Users/me/.gcloud/keys/myproject-1234.json</value>\n     </property>\n     <property>\n         <name>fs.default.name</name>\n         <value>gs://startlake-app</value>\n     </property>\n     <property>\n         <name>fs.defaultFS</name>\n         <value>gs://startlake-app</value>\n     </property>\n     <property>\n         <name>fs.gs.system.bucket</name>\n         <value>startlake-app</value>\n     </property>\n </configuration>\n"})})]})}function u(e={}){const{wrapper:t}={...(0,n.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},85915:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/bigquery-82292e4b7de01626a23fdf348c3d2c85.png"},14036:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/create-dataproc-015c4fffce23828183a0e5554ea9a983.png"},44881:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/create-import-job-970b379ee437c890078d3b20a31e6eb1.png"},56973:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/create-service-account-5645ba303a6b049b7de97d9c236eb3ce.png"},83973:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/create-watch-job-0dccfab45b80fa6f1afaef00957e178c.png"},17507:(e,t,a)=>{a.d(t,{A:()=>r});const r=a.p+"assets/images/runs-f87c0dd6d17d6a0fde9aaefe91cc98a6.png"},28453:(e,t,a)=>{a.d(t,{R:()=>o,x:()=>i});var r=a(96540);const n={},s=r.createContext(n);function o(e){const t=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:o(e.components),r.createElement(s.Provider,{value:t},e.children)}}}]);