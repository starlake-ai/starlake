"use strict";(self.webpackChunkstarlake=self.webpackChunkstarlake||[]).push([[7504],{9673:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>X,frontMatter:()=>i,metadata:()=>c,toc:()=>u});var r=a(74848),t=a(28453),o=a(11470),s=a(19365);const i={},l="Connections",c={id:"configuration/connections",title:"Connections",description:"Connections are defined in the connections section under the root attribute application.",source:"@site/versioned_docs/version-1.2.0/0500-configuration/0110-connections.mdx",sourceDirName:"0500-configuration",slug:"/configuration/connections",permalink:"/starlake/docs/configuration/connections",draft:!1,unlisted:!1,editUrl:"https://github.com/starlake-ai/starlake/edit/master/docs/versioned_docs/version-1.2.0/0500-configuration/0110-connections.mdx",tags:[],version:"1.2.0",sidebarPosition:110,frontMatter:{},sidebar:"starlakeSidebar",previous:{title:"Environment",permalink:"/starlake/docs/configuration/environment"},next:{title:"Platforms",permalink:"/starlake/docs/category/platforms"}},d={},u=[{value:"Local File System",id:"local-file-system",level:2},{value:"Google BigQuery",id:"google-bigquery",level:2},{value:"Apache Spark / Databricks",id:"apache-spark--databricks",level:2},{value:"Snowflake",id:"snowflake",level:2},{value:"Amazon Redshift",id:"amazon-redshift",level:2},{value:"Postgres",id:"postgres",level:2}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"connections",children:"Connections"}),"\n","\n",(0,r.jsxs)(n.p,{children:["Connections are defined in the ",(0,r.jsx)(n.code,{children:"connections"})," section under the root attribute ",(0,r.jsx)(n.code,{children:"application"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"The following types of connections are supported:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#local-file-system",children:"Local File System"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#google-bigquery",children:"BigQuery"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#apache-spark--databricks",children:"Spark / Databricks"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#snowflake",children:"Snowflake"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#amazon-redshift",children:"Redshift"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#postgres",children:"Postgres"})}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"local-file-system",children:"Local File System"}),"\n",(0,r.jsx)(n.p,{children:"The local file system connection is used to read and write files to the local file system."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"application:\n    connections:\n    local:\n        type: local\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Files will be stored in the ",(0,r.jsx)(n.code,{children:"area"})," directory under the ",(0,r.jsx)(n.code,{children:"datasets"})," directory.\nD\xe9fault values for ",(0,r.jsx)(n.code,{children:"area"})," and ",(0,r.jsx)(n.code,{children:"datasets"})," can be set in the ",(0,r.jsx)(n.code,{children:"application"})," section."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'application:\n    datasets = "{{root}}/datasets" # or set it through the SL_DATASETS environnement variable.\n    area:\n        pending: "pending" # Location where files of pending load are stored. May be overloaded by the ${SL_AREA_PENDING} environment variable.\n        unresolved: "unresolved" # Location where files that do not match any pattern are moved. May be overloaded by the ${SL_AREA_UNRESOLVED} environment variable.\n        archive: "archive" # Location where files are moved after they have been processed. May be overloaded by the ${SL_AREA_ARCHIVE} environment variable.\n        ingesting: "ingesting" # Location where files are moved while they are being processed. May be overloaded by the ${SL_AREA_INGESTING} environment variable.\n        accepted: "accepted" # Location where files are moved after they have been processed and accepted. May be overloaded by the ${SL_AREA_ACCEPTED} environment variable.\n        rejected: "rejected" # Location where files are moved after they have been processed and rejected. May be overloaded by the ${SL_AREA_REJECTED} environment variable.\n        business: "business" # Location where transform tasks store their result. May be overloaded by the ${SL_AREA_BUSINESS} environment variable.\n        replay: "replay" # Location rejected records are stored in their orginial format. May be overloaded by the ${SL_AREA_REPLAY} environment variable.\n        hiveDatabase: "${domain}_${area}" # Hive database name. May be overloaded by the ${SL_AREA_HIVE_DATABASE} environment variable.\n\n'})}),"\n",(0,r.jsx)(n.h2,{id:"google-bigquery",children:"Google BigQuery"}),"\n",(0,r.jsx)(n.p,{children:"Starlake support native and spark / dataproc bigquery connections."}),"\n",(0,r.jsxs)(o.A,{groupId:"bq_connections",children:[(0,r.jsx)(s.A,{label:"BigQuery",value:"bigquery",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'application:\n  connections:\n    bigquery:\n      type: "bigquery"\n      options:\n        location: "us-central1" # EU or US or ...\n        authType: "APPLICATION_DEFAULT"\n        authScopes: "https://www.googleapis.com/auth/cloud-platform" # comma separated list of scopes\n        #authType: SERVICE_ACCOUNT_JSON_KEYFILE\n        #jsonKeyfile: "/Users/me/.gcloud/keys/starlake-me.json"\n        #authType: "ACCESS_TOKEN"\n        #gcpAccessToken: "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"\n  accessPolicies: # Required when applying access policies to table columns (Column Level Security)\n    apply: true\n    location: EU\n    taxonomy: RGPD\n'})})}),(0,r.jsx)(s.A,{label:"Spark BigQuery Direct",value:"spark-bigquery-direct",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'application:\n  connections:\n    bigquery:\n      type: "bigquery"\n      sparkFormat: "bigquery"\n      options:\n        writeMethod: "direct" # direct or indirect (indirect is required for certain features see https://github.com/GoogleCloudDataproc/spark-bigquery-connector)\n        location: "us-central1" # EU or US or ...\n        authType: "APPLICATION_DEFAULT"\n        authScopes: "https://www.googleapis.com/auth/cloud-platform" # comma separated list of scopes\n        # authType: SERVICE_ACCOUNT_JSON_KEYFILE\n        # jsonKeyfile: "/Users/me/.gcloud/keys/starlake-me.json"\n        # authType: "ACCESS_TOKEN"\n        # gcpAccessToken: "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"\n  spark:\n    datasource:\n      bigquery: # Setting properties here will apply them to all bigquery data sources (connection.type == bigquery)\n        allowFieldAddition: "true" # Allow schema updates. To disable, set it to false\n        allowFieldRelaxation: "true" # Allow schema updates. To disable, set it to false\n\n'})})}),(0,r.jsx)(s.A,{label:"Spark BigQuery Indirect",value:"spark-bigquery-indirect",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'application:\n  connections:\n    bigquery:\n      type: "bigquery"\n      sparkFormat: "bigquery"\n      options:\n        writeMethod: "indirect" # direct or indirect (indirect is required for certain features see https://github.com/GoogleCloudDataproc/spark-bigquery-connector)\n        gcsBucket: "starlake-app" # Temporary GCS Bucket where intermediary files will be stored. Required in indirect mode only\n        location: "us-central1" # EU or US or ...\n        authType: "APPLICATION_DEFAULT"\n        authScopes: "https://www.googleapis.com/auth/cloud-platform" # comma separated list of scopes\n        materializationDataset: "my-bucket-name" # when sparkFormat is defined, required by the spark-bigquery-connector (https://github.com/GoogleCloudDataproc/spark-bigquery-connector?tab=readme-ov-file#properties)\n        #authType: SERVICE_ACCOUNT_JSON_KEYFILE\n        #jsonKeyfile: "/Users/me/.gcloud/keys/starlake-me.json"\n        #authType: "ACCESS_TOKEN"\n        #gcpAccessToken: "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"\n  spark:\n    datasource:\n      bigquery:\n        allowFieldAddition: "true" # Allow schema updates. To disable, set it to false\n        allowFieldRelaxation: "true" # Allow schema updates. To disable, set it to false\n\n'})})})]}),"\n",(0,r.jsx)(n.h2,{id:"apache-spark--databricks",children:"Apache Spark / Databricks"}),"\n",(0,r.jsx)(n.p,{children:"Spark connections are used to read and write data from Spark."}),"\n",(0,r.jsxs)(o.A,{groupId:"spark_connections",children:[(0,r.jsx)(s.A,{label:"Spark Parquet",value:"spark-parquet",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'application:\n  connections:\n    spark:\n      type: "spark"\n      options:\n        # any spark configuration can be set here\n      \n'})})}),(0,r.jsxs)(s.A,{label:"Spark Delta",value:"spark-delta",children:[(0,r.jsx)(n.p,{children:"In addition to the connection defined below, please download the following jars:"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://repo1.maven.org/maven2/io/delta/delta-spark_2.12",children:"delta-spark_2.12-VERSION.jar"})," and place it in the ",(0,r.jsx)(n.code,{children:"bin/deps"})," directory of the starlake directory."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://repo1.maven.org/maven2/io/delta/delta-storage",children:"delta-storage_2.12-VERSION.jar"})," and place it in the ",(0,r.jsx)(n.code,{children:"bin/deps"})," directory of the starlake directory."]}),"\n"]}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'application:\n  connections:\n    spark:\n      type: "spark"\n      options:\n        # any spark configuration can be set here\n  spark:\n    sql:\n      extensions: "io.delta.sql.DeltaSparkSessionExtension"\n      catalog:\n        spark_catalog: "org.apache.spark.sql.delta.catalog.DeltaCatalog"\n\n'})})]})]}),"\n",(0,r.jsx)(n.h2,{id:"snowflake",children:"Snowflake"}),"\n",(0,r.jsxs)(o.A,{groupId:"snow_connections",children:[(0,r.jsx)(s.A,{label:"Snowflake JDBC",value:"snow-jdbc",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'\napplication:\n  connectionRef: {{connection}}\n  connections:\n    snowflake:\n      type: jdbc\n      options:\n        url: "jdbc:snowflake://{{SNOWFLAKE_ACCOUNT}}.snowflakecomputing.com"\n        driver: "net.snowflake.client.jdbc.SnowflakeDriver"\n        user: {{SNOWFLAKE_USER}}\n        password: {{SNOWFLAKE_PASSWORD}}\n        warehouse: {{SNOWFLAKE_WAREHOUSE}}\n        db: {{SNOWFLAKE_DB}}\n        keep_column_case: "off"\n        preActions: "alter session set TIMESTAMP_TYPE_MAPPING = \'TIMESTAMP_LTZ\';ALTER SESSION SET QUOTED_IDENTIFIERS_IGNORE_CASE = true"\n      \n'})})}),(0,r.jsx)(s.A,{label:"Snowflake Spark",value:"snow-spark",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'\napplication:\n  connectionRef: {{connection}}\n  connections:\n    snowflake:\n    spark-snowflake:\n      type: jdbc\n      sparkFormat: snowflake\n      options:\n        sfUrl: "{{SNOWFLAKE_ACCOUNT}}.snowflakecomputing.com" # make sure you do not prefix by jdbc:snowflake://. This is done by the snowflaek driver\n        #sfDriver: "net.snowflake.client.jdbc.SnowflakeDriver"\n        sfUser: {{SNOWFLAKE_USER}}\n        sfPassword: {{SNOWFLAKE_PASSWORD}}\n        sfWarehouse: {{SNOWFLAKE_WAREHOUSE}}\n        sfDatabase: {{SNOWFLAKE_DB}}\n        keep_column_case: "off"\n        autopushdown: on\n        preActions: "alter session set TIMESTAMP_TYPE_MAPPING = \'TIMESTAMP_LTZ\';ALTER SESSION SET QUOTED_IDENTIFIERS_IGNORE_CASE = true"\n\n'})})})]}),"\n",(0,r.jsx)(n.h2,{id:"amazon-redshift",children:"Amazon Redshift"}),"\n",(0,r.jsxs)(o.A,{groupId:"redshift_connections",children:[(0,r.jsx)(s.A,{label:"Redshift JDBC",value:"redshift-jdbc",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'\napplication:\n  connections:\n    redshift:\n      options:\n        url: "jdbc:redshift://account.region.redshift.amazonaws.com:5439/database",\n        driver: com.amazon.redshift.Driver\n        password: "{{REDSHIFT_PASSWORD}}"\n        tempdir: "s3a://bucketName/data",\n        tempdir_region: "eu-central-1" # required only if running from outside AWS (your laptop ...)\n        aws_iam_role: "arn:aws:iam::aws_count_id:role/role_name"\n  \n'})})}),(0,r.jsx)(s.A,{label:"Redshift Spark",value:"redshift-spark",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'\napplication:\n  connections:\n    redshift:\n      sparkFormat: "io.github.spark_redshift_community.spark.redshift" # if running on top of Spark or else  "redshift" if running on top of Databricks\n      options:\n        url: "jdbc:redshift://account.region.redshift.amazonaws.com:5439/database",\n        driver: com.amazon.redshift.Driver\n        password: "{{REDSHIFT_PASSWORD}}"\n        tempdir: "s3a://bucketName/data",\n        tempdir_region: "eu-central-1" # required only if running from outside AWS (your laptop ...)\n        aws_iam_role: "arn:aws:iam::aws_count_id:role/role_name"\n\n'})})})]}),"\n",(0,r.jsx)(n.h2,{id:"postgres",children:"Postgres"}),"\n",(0,r.jsxs)(o.A,{groupId:"pg_connections",children:[(0,r.jsx)(s.A,{label:"Postgres JDBC",value:"postgres-jdbc",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'application:\n  connectionRef: "postgresql"\n  connections:\n    postgresql:\n      type: jdbc\n      options:\n        url: "jdbc:postgresql://{{POSTGRES_HOST}}:{{POSTGRES_PORT}}/{{POSTGRES_DATABASE}}"\n        driver: "org.postgresql.Driver"\n        user: "{{DATABASE_USER}}"\n        password: "{{DATABASE_PASSWORD}}"\n        quoteIdentifiers: false\n'})})}),(0,r.jsx)(s.A,{label:"Postgres Spark",value:"pg-spark",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'\napplication:\n  connectionRef: "postgresql"\n  connections:\n    postgresql:\n      type: jdbc\n      sparkFormat: jdbc\n      options:\n        url: "jdbc:postgresql://{{POSTGRES_HOST}}:{{POSTGRES_PORT}}/{{POSTGRES_DATABASE}}"\n        driver: "org.postgresql.Driver"\n        user: "{{DATABASE_USER}}"\n        password: "{{DATABASE_PASSWORD}}"\n        quoteIdentifiers: false\n'})})})]})]})}function X(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}},19365:(e,n,a)=>{a.d(n,{A:()=>s});a(96540);var r=a(18215);const t={tabItem:"tabItem_Ymn6"};var o=a(74848);function s(e){let{children:n,hidden:a,className:s}=e;return(0,o.jsx)("div",{role:"tabpanel",className:(0,r.A)(t.tabItem,s),hidden:a,children:n})}},11470:(e,n,a)=>{a.d(n,{A:()=>A});var r=a(96540),t=a(18215),o=a(23104),s=a(56347),i=a(205),l=a(57485),c=a(31682),d=a(70679);function u(e){return r.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function p(e){const{values:n,children:a}=e;return(0,r.useMemo)((()=>{const e=n??function(e){return u(e).map((e=>{let{props:{value:n,label:a,attributes:r,default:t}}=e;return{value:n,label:a,attributes:r,default:t}}))}(a);return function(e){const n=(0,c.X)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,a])}function X(e){let{value:n,tabValues:a}=e;return a.some((e=>e.value===n))}function h(e){let{queryString:n=!1,groupId:a}=e;const t=(0,s.W6)(),o=function(e){let{queryString:n=!1,groupId:a}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:n,groupId:a});return[(0,l.aZ)(o),(0,r.useCallback)((e=>{if(!o)return;const n=new URLSearchParams(t.location.search);n.set(o,e),t.replace({...t.location,search:n.toString()})}),[o,t])]}function f(e){const{defaultValue:n,queryString:a=!1,groupId:t}=e,o=p(e),[s,l]=(0,r.useState)((()=>function(e){let{defaultValue:n,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!X({value:n,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const r=a.find((e=>e.default))??a[0];if(!r)throw new Error("Unexpected error: 0 tabValues");return r.value}({defaultValue:n,tabValues:o}))),[c,u]=h({queryString:a,groupId:t}),[f,m]=function(e){let{groupId:n}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(n),[t,o]=(0,d.Dv)(a);return[t,(0,r.useCallback)((e=>{a&&o.set(e)}),[a,o])]}({groupId:t}),b=(()=>{const e=c??f;return X({value:e,tabValues:o})?e:null})();(0,i.A)((()=>{b&&l(b)}),[b]);return{selectedValue:s,selectValue:(0,r.useCallback)((e=>{if(!X({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);l(e),u(e),m(e)}),[u,m,o]),tabValues:o}}var m=a(92303);const b={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var g=a(74848);function y(e){let{className:n,block:a,selectedValue:r,selectValue:s,tabValues:i}=e;const l=[],{blockElementScrollPositionUntilNextRender:c}=(0,o.a_)(),d=e=>{const n=e.currentTarget,a=l.indexOf(n),t=i[a].value;t!==r&&(c(n),s(t))},u=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const a=l.indexOf(e.currentTarget)+1;n=l[a]??l[0];break}case"ArrowLeft":{const a=l.indexOf(e.currentTarget)-1;n=l[a]??l[l.length-1];break}}n?.focus()};return(0,g.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,t.A)("tabs",{"tabs--block":a},n),children:i.map((e=>{let{value:n,label:a,attributes:o}=e;return(0,g.jsx)("li",{role:"tab",tabIndex:r===n?0:-1,"aria-selected":r===n,ref:e=>l.push(e),onKeyDown:u,onClick:d,...o,className:(0,t.A)("tabs__item",b.tabItem,o?.className,{"tabs__item--active":r===n}),children:a??n},n)}))})}function S(e){let{lazy:n,children:a,selectedValue:t}=e;const o=(Array.isArray(a)?a:[a]).filter(Boolean);if(n){const e=o.find((e=>e.props.value===t));return e?(0,r.cloneElement)(e,{className:"margin-top--md"}):null}return(0,g.jsx)("div",{className:"margin-top--md",children:o.map(((e,n)=>(0,r.cloneElement)(e,{key:n,hidden:e.props.value!==t})))})}function v(e){const n=f(e);return(0,g.jsxs)("div",{className:(0,t.A)("tabs-container",b.tabList),children:[(0,g.jsx)(y,{...n,...e}),(0,g.jsx)(S,{...n,...e})]})}function A(e){const n=(0,m.A)();return(0,g.jsx)(v,{...e,children:u(e.children)},String(n))}},28453:(e,n,a)=>{a.d(n,{R:()=>s,x:()=>i});var r=a(96540);const t={},o=r.createContext(t);function s(e){const n=r.useContext(o);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);