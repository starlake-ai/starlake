"use strict";(self.webpackChunkstarlake=self.webpackChunkstarlake||[]).push([[9696],{15680:(e,n,a)=>{a.d(n,{xA:()=>u,yg:()=>m});var t=a(96540);function r(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function o(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}function l(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?o(Object(a),!0).forEach((function(n){r(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function i(e,n){if(null==e)return{};var a,t,r=function(e,n){if(null==e)return{};var a,t,r={},o=Object.keys(e);for(t=0;t<o.length;t++)a=o[t],n.indexOf(a)>=0||(r[a]=e[a]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(t=0;t<o.length;t++)a=o[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=t.createContext({}),c=function(e){var n=t.useContext(s),a=n;return e&&(a="function"==typeof e?e(n):l(l({},n),e)),a},u=function(e){var n=c(e.components);return t.createElement(s.Provider,{value:n},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},X=t.forwardRef((function(e,n){var a=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),p=c(a),X=r,m=p["".concat(s,".").concat(X)]||p[X]||d[X]||o;return a?t.createElement(m,l(l({ref:n},u),{},{components:a})):t.createElement(m,l({ref:n},u))}));function m(e,n){var a=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=a.length,l=new Array(o);l[0]=X;var i={};for(var s in n)hasOwnProperty.call(n,s)&&(i[s]=n[s]);i.originalType=e,i[p]="string"==typeof e?e:r,l[1]=i;for(var c=2;c<o;c++)l[c]=a[c];return t.createElement.apply(null,l)}return t.createElement.apply(null,a)}X.displayName="MDXCreateElement"},19365:(e,n,a)=>{a.d(n,{A:()=>l});var t=a(96540),r=a(20053);const o={tabItem:"tabItem_Ymn6"};function l(e){let{children:n,hidden:a,className:l}=e;return t.createElement("div",{role:"tabpanel",className:(0,r.A)(o.tabItem,l),hidden:a},n)}},11470:(e,n,a)=>{a.d(n,{A:()=>S});var t=a(9668),r=a(96540),o=a(20053),l=a(23104),i=a(56347),s=a(57485),c=a(31682),u=a(89466);function p(e){return function(e){return r.Children.map(e,(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:n,label:a,attributes:t,default:r}}=e;return{value:n,label:a,attributes:t,default:r}}))}function d(e){const{values:n,children:a}=e;return(0,r.useMemo)((()=>{const e=n??p(a);return function(e){const n=(0,c.X)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,a])}function X(e){let{value:n,tabValues:a}=e;return a.some((e=>e.value===n))}function m(e){let{queryString:n=!1,groupId:a}=e;const t=(0,i.W6)(),o=function(e){let{queryString:n=!1,groupId:a}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:n,groupId:a});return[(0,s.aZ)(o),(0,r.useCallback)((e=>{if(!o)return;const n=new URLSearchParams(t.location.search);n.set(o,e),t.replace({...t.location,search:n.toString()})}),[o,t])]}function y(e){const{defaultValue:n,queryString:a=!1,groupId:t}=e,o=d(e),[l,i]=(0,r.useState)((()=>function(e){let{defaultValue:n,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!X({value:n,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const t=a.find((e=>e.default))??a[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:o}))),[s,c]=m({queryString:a,groupId:t}),[p,y]=function(e){let{groupId:n}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(n),[t,o]=(0,u.Dv)(a);return[t,(0,r.useCallback)((e=>{a&&o.set(e)}),[a,o])]}({groupId:t}),g=(()=>{const e=s??p;return X({value:e,tabValues:o})?e:null})();(0,r.useLayoutEffect)((()=>{g&&i(g)}),[g]);return{selectedValue:l,selectValue:(0,r.useCallback)((e=>{if(!X({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);i(e),c(e),y(e)}),[c,y,o]),tabValues:o}}var g=a(92303);const f={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function b(e){let{className:n,block:a,selectedValue:i,selectValue:s,tabValues:c}=e;const u=[],{blockElementScrollPositionUntilNextRender:p}=(0,l.a_)(),d=e=>{const n=e.currentTarget,a=u.indexOf(n),t=c[a].value;t!==i&&(p(n),s(t))},X=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const a=u.indexOf(e.currentTarget)+1;n=u[a]??u[0];break}case"ArrowLeft":{const a=u.indexOf(e.currentTarget)-1;n=u[a]??u[u.length-1];break}}n?.focus()};return r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.A)("tabs",{"tabs--block":a},n)},c.map((e=>{let{value:n,label:a,attributes:l}=e;return r.createElement("li",(0,t.A)({role:"tab",tabIndex:i===n?0:-1,"aria-selected":i===n,key:n,ref:e=>u.push(e),onKeyDown:X,onClick:d},l,{className:(0,o.A)("tabs__item",f.tabItem,l?.className,{"tabs__item--active":i===n})}),a??n)})))}function h(e){let{lazy:n,children:a,selectedValue:t}=e;const o=(Array.isArray(a)?a:[a]).filter(Boolean);if(n){const e=o.find((e=>e.props.value===t));return e?(0,r.cloneElement)(e,{className:"margin-top--md"}):null}return r.createElement("div",{className:"margin-top--md"},o.map(((e,n)=>(0,r.cloneElement)(e,{key:n,hidden:e.props.value!==t}))))}function v(e){const n=y(e);return r.createElement("div",{className:(0,o.A)("tabs-container",f.tabList)},r.createElement(b,(0,t.A)({},e,n)),r.createElement(h,(0,t.A)({},e,n)))}function S(e){const n=(0,g.A)();return r.createElement(v,(0,t.A)({key:String(n)},e))}},41094:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>u,contentTitle:()=>s,default:()=>m,frontMatter:()=>i,metadata:()=>c,toc:()=>p});var t=a(9668),r=(a(96540),a(15680)),o=a(11470),l=a(19365);const i={},s="Connections",c={unversionedId:"configuration/connections",id:"version-1.0.0/configuration/connections",title:"Connections",description:"Connections are defined in the connections section under the root attribute application.",source:"@site/versioned_docs/version-1.0.0/0500-configuration/0110-connections.mdx",sourceDirName:"0500-configuration",slug:"/configuration/connections",permalink:"/docs/1.0.0/configuration/connections",draft:!1,editUrl:"https://github.com/starlake-ai/starlake/edit/master/docs/versioned_docs/version-1.0.0/0500-configuration/0110-connections.mdx",tags:[],version:"1.0.0",sidebarPosition:110,frontMatter:{},sidebar:"starlakeSidebar",previous:{title:"Scheduling",permalink:"/docs/1.0.0/configuration/scheduling"},next:{title:"Filesystem",permalink:"/docs/1.0.0/configuration/filesystem"}},u={},p=[{value:"Local File System",id:"local-file-system",level:2},{value:"Google BigQuery",id:"google-bigquery",level:2},{value:"Apache Spark / Databricks",id:"apache-spark--databricks",level:2},{value:"Snowflake",id:"snowflake",level:2},{value:"Amazon Redshift",id:"amazon-redshift",level:2},{value:"Postgres",id:"postgres",level:2}],d={toc:p},X="wrapper";function m(e){let{components:n,...a}=e;return(0,r.yg)(X,(0,t.A)({},d,a,{components:n,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"connections"},"Connections"),(0,r.yg)("p",null,"Connections are defined in the ",(0,r.yg)("inlineCode",{parentName:"p"},"connections")," section under the root attribute ",(0,r.yg)("inlineCode",{parentName:"p"},"application"),"."),(0,r.yg)("p",null,"The following types of connections are supported:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#local-file-system"},"Local File System")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#google-bigquery"},"BigQuery")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#apache-spark--databricks"},"Spark / Databricks")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#snowflake"},"Snowflake")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#amazon-redshift"},"Redshift")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#postgres"},"Postgres"))),(0,r.yg)("h2",{id:"local-file-system"},"Local File System"),(0,r.yg)("p",null,"The local file system connection is used to read and write files to the local file system."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"application:\n    connections:\n    local:\n        type: local\n")),(0,r.yg)("p",null,"Files will be stored in the ",(0,r.yg)("inlineCode",{parentName:"p"},"area")," directory under the ",(0,r.yg)("inlineCode",{parentName:"p"},"datasets")," directory.\nD\xe9fault values for ",(0,r.yg)("inlineCode",{parentName:"p"},"area")," and ",(0,r.yg)("inlineCode",{parentName:"p"},"datasets")," can be set in the ",(0,r.yg)("inlineCode",{parentName:"p"},"application")," section."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'application:\n    datasets = "{{root}}/datasets" # or set it through the SL_DATASETS environnement variable.\n    area:\n        pending: "pending" # Location where files of pending load are stored. May be overloaded by the ${SL_AREA_PENDING} environment variable.\n        unresolved: "unresolved" # Location where files that do not match any pattern are moved. May be overloaded by the ${SL_AREA_UNRESOLVED} environment variable.\n        archive: "archive" # Location where files are moved after they have been processed. May be overloaded by the ${SL_AREA_ARCHIVE} environment variable.\n        ingesting: "ingesting" # Location where files are moved while they are being processed. May be overloaded by the ${SL_AREA_INGESTING} environment variable.\n        accepted: "accepted" # Location where files are moved after they have been processed and accepted. May be overloaded by the ${SL_AREA_ACCEPTED} environment variable.\n        rejected: "rejected" # Location where files are moved after they have been processed and rejected. May be overloaded by the ${SL_AREA_REJECTED} environment variable.\n        business: "business" # Location where transform tasks store their result. May be overloaded by the ${SL_AREA_BUSINESS} environment variable.\n        replay: "replay" # Location rejected records are stored in their orginial format. May be overloaded by the ${SL_AREA_REPLAY} environment variable.\n        hiveDatabase: "${domain}_${area}" # Hive database name. May be overloaded by the ${SL_AREA_HIVE_DATABASE} environment variable.\n\n')),(0,r.yg)("h2",{id:"google-bigquery"},"Google BigQuery"),(0,r.yg)("p",null,"Starlake support native and spark / dataproc bigquery connections."),(0,r.yg)(o.A,{groupId:"bq_connections",mdxType:"Tabs"},(0,r.yg)(l.A,{label:"BigQuery",value:"bigquery",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'application:\n  connections:\n    bigquery:\n      type: "bigquery"\n      options:\n        location: "us-central1" # EU or US or ...\n        authType: "APPLICATION_DEFAULT"\n        authScopes: "https://www.googleapis.com/auth/cloud-platform" # comma separated list of scopes\n        #authType: SERVICE_ACCOUNT_JSON_KEYFILE\n        #jsonKeyfile: "/Users/me/.gcloud/keys/starlake-me.json"\n        #authType: "ACCESS_TOKEN"\n        #gcpAccessToken: "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"\n  accessPolicies: # Required when applying access policies to table columns (Column Level Security)\n    apply: true\n    location: EU\n    taxonomy: RGPD\n'))),(0,r.yg)(l.A,{label:"Spark BigQuery Direct",value:"spark-bigquery-direct",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'application:\n  connections:\n    bigquery:\n      type: "bigquery"\n      sparkFormat: "bigquery"\n      options:\n        writeMethod: "direct" # direct or indirect (indirect is required for certain features see https://github.com/GoogleCloudDataproc/spark-bigquery-connector)\n        location: "us-central1" # EU or US or ...\n        authType: "APPLICATION_DEFAULT"\n        authScopes: "https://www.googleapis.com/auth/cloud-platform" # comma separated list of scopes\n        #authType: SERVICE_ACCOUNT_JSON_KEYFILE\n        #jsonKeyfile: "/Users/me/.gcloud/keys/starlake-me.json"\n        #authType: "ACCESS_TOKEN"\n        #gcpAccessToken: "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"\n  spark:\n    datasource:\n      bigquery:\n        allowFieldAddition: "true" # Allow schema updates. To disable, set it to false\n        allowFieldRelaxation: "true" # Allow schema updates. To disable, set it to false\n\n'))),(0,r.yg)(l.A,{label:"Spark BigQuery Indirect",value:"spark-bigquery-indirect",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'application:\n  connections:\n    bigquery:\n      type: "bigquery"\n      sparkFormat: "bigquery"\n      options:\n        writeMethod: "indirect" # direct or indirect (indirect is required for certain features see https://github.com/GoogleCloudDataproc/spark-bigquery-connector)\n        gcsBucket: "starlake-app" # Temporary GCS Bucket where intermediary files will be stored. Required in indirect mode only\n        location: "us-central1" # EU or US or ...\n        authType: "APPLICATION_DEFAULT"\n        authScopes: "https://www.googleapis.com/auth/cloud-platform" # comma separated list of scopes\n        #authType: SERVICE_ACCOUNT_JSON_KEYFILE\n        #jsonKeyfile: "/Users/me/.gcloud/keys/starlake-me.json"\n        #authType: "ACCESS_TOKEN"\n        #gcpAccessToken: "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"\n  spark:\n    datasource:\n      bigquery:\n        allowFieldAddition: "true" # Allow schema updates. To disable, set it to false\n        allowFieldRelaxation: "true" # Allow schema updates. To disable, set it to false\n\n')))),(0,r.yg)("h2",{id:"apache-spark--databricks"},"Apache Spark / Databricks"),(0,r.yg)("p",null,"Spark connections are used to read and write data from Spark."),(0,r.yg)(o.A,{groupId:"spark_connections",mdxType:"Tabs"},(0,r.yg)(l.A,{label:"Spark Parquet",value:"spark-parquet",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'application:\n  connections:\n    spark:\n      type: "spark"\n      options:\n        # any spark configuration can be set here\n      \n'))),(0,r.yg)(l.A,{label:"Spark Delta",value:"spark-delta",mdxType:"TabItem"},(0,r.yg)("p",null,"In addition to the connection defined below, please download the following jars: "),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"https://repo1.maven.org/maven2/io/delta/delta-spark_2.12"},"delta-spark_2.12-VERSION.jar")," and place it in the ",(0,r.yg)("inlineCode",{parentName:"li"},"bin/deps")," directory of the starlake directory."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"https://repo1.maven.org/maven2/io/delta/delta-storage"},"delta-storage_2.12-VERSION.jar")," and place it in the ",(0,r.yg)("inlineCode",{parentName:"li"},"bin/deps")," directory of the starlake directory.")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'application:\n  connections:\n    spark:\n      type: "spark"\n      options:\n        # any spark configuration can be set here\n  spark:\n    sql:\n      extensions: "io.delta.sql.DeltaSparkSessionExtension"\n      catalog:\n        spark_catalog: "org.apache.spark.sql.delta.catalog.DeltaCatalog"\n\n')))),(0,r.yg)("h2",{id:"snowflake"},"Snowflake"),(0,r.yg)(o.A,{groupId:"snow_connections",mdxType:"Tabs"},(0,r.yg)(l.A,{label:"Snowflake JDBC",value:"snow-jdbc",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'\napplication:\n  connectionRef: {{connection}}\n  connections:\n    snowflake:\n      type: jdbc\n      options:\n        url: "jdbc:snowflake://{{SNOWFLAKE_ACCOUNT}}.snowflakecomputing.com"\n        driver: "net.snowflake.client.jdbc.SnowflakeDriver"\n        user: {{SNOWFLAKE_USER}}\n        password: {{SNOWFLAKE_PASSWORD}}\n        warehouse: {{SNOWFLAKE_WAREHOUSE}}\n        db: {{SNOWFLAKE_DB}}\n        keep_column_case: "off"\n        preActions: "alter session set TIMESTAMP_TYPE_MAPPING = \'TIMESTAMP_LTZ\';ALTER SESSION SET QUOTED_IDENTIFIERS_IGNORE_CASE = true"\n      \n'))),(0,r.yg)(l.A,{label:"Snowflake Spark",value:"snow-spark",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'\napplication:\n  connectionRef: {{connection}}\n  connections:\n    snowflake:\n    spark-snowflake:\n      type: jdbc\n      sparkFormat: snowflake\n      options:\n        sfUrl: "{{SNOWFLAKE_ACCOUNT}}.snowflakecomputing.com" # make sure you do not prefix by jdbc:snowflake://. This is done by the snowflaek driver\n        #sfDriver: "net.snowflake.client.jdbc.SnowflakeDriver"\n        sfUser: {{SNOWFLAKE_USER}}\n        sfPassword: {{SNOWFLAKE_PASSWORD}}\n        sfWarehouse: {{SNOWFLAKE_WAREHOUSE}}\n        sfDatabase: {{SNOWFLAKE_DB}}\n        keep_column_case: "off"\n        autopushdown: on\n        preActions: "alter session set TIMESTAMP_TYPE_MAPPING = \'TIMESTAMP_LTZ\';ALTER SESSION SET QUOTED_IDENTIFIERS_IGNORE_CASE = true"\n\n')))),(0,r.yg)("h2",{id:"amazon-redshift"},"Amazon Redshift"),(0,r.yg)(o.A,{groupId:"redshift_connections",mdxType:"Tabs"},(0,r.yg)(l.A,{label:"Redshift JDBC",value:"redshift-jdbc",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'\napplication:\n  connections:\n    redshift:\n      options:\n        url: "jdbc:redshift://account.region.redshift.amazonaws.com:5439/database",\n        driver: com.amazon.redshift.Driver\n        password: "{{REDSHIFT_PASSWORD}}"\n        tempdir: "s3a://bucketName/data",\n        tempdir_region: "eu-central-1" # required only if running from outside AWS (your laptop ...)\n        aws_iam_role: "arn:aws:iam::aws_count_id:role/role_name"\n  \n'))),(0,r.yg)(l.A,{label:"Redshift Spark",value:"redshift-spark",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'\napplication:\n  connections:\n    redshift:\n      sparkFormat: "io.github.spark_redshift_community.spark.redshift" # if running on top of Spark or else  "redshift" if running on top of Databricks\n      options:\n        url: "jdbc:redshift://account.region.redshift.amazonaws.com:5439/database",\n        driver: com.amazon.redshift.Driver\n        password: "{{REDSHIFT_PASSWORD}}"\n        tempdir: "s3a://bucketName/data",\n        tempdir_region: "eu-central-1" # required only if running from outside AWS (your laptop ...)\n        aws_iam_role: "arn:aws:iam::aws_count_id:role/role_name"\n\n')))),(0,r.yg)("h2",{id:"postgres"},"Postgres"),(0,r.yg)(o.A,{groupId:"pg_connections",mdxType:"Tabs"},(0,r.yg)(l.A,{label:"Postgres JDBC",value:"postgres-jdbc",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'application:\n  connectionRef: "postgresql"\n  connections:\n    postgresql:\n      type: jdbc\n      options:\n        url: "jdbc:postgresql://{{POSTGRES_HOST}}:{{POSTGRES_PORT}}/{{POSTGRES_DATABASE}}"\n        driver: "org.postgresql.Driver"\n        user: "{{DATABASE_USER}}"\n        password: "{{DATABASE_PASSWORD}}"\n        quoteIdentifiers: false\n'))),(0,r.yg)(l.A,{label:"Postgres Spark",value:"pg-spark",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'\napplication:\n  connectionRef: "postgresql"\n  connections:\n    postgresql:\n      type: jdbc\n      sparkFormat: jdbc\n      options:\n        url: "jdbc:postgresql://{{POSTGRES_HOST}}:{{POSTGRES_PORT}}/{{POSTGRES_DATABASE}}"\n        driver: "org.postgresql.Driver"\n        user: "{{DATABASE_USER}}"\n        password: "{{DATABASE_PASSWORD}}"\n        quoteIdentifiers: false\n')))))}m.isMDXComponent=!0}}]);