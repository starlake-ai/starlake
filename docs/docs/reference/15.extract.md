---
sidebar_position: 10
---

# Extract

This step is optional and useful only if you intend to extract data from a SQL Database into
a set of files before ingesting it into a datalake or warehouse.

![](/img/extract/extract-general.png)


## Configuration

We'll have to go through the following steps to extract data from a JDBC compliant database
into a set of files that will be loaded into our datawarehouse:

1. Extract the database table schemas
2. Extract the data

![](/img/extract/extract-detailed.png)

### Extract table schemas

To extract the tables into a set of DSV files, create the dbextract.comet.yml YAML specification file
that describe the tables and columns you are willing to extract using the following syntax:

````yaml
extract:
    connectionRef: "mypostgresdb" # Connection name as defined in the connections section of the application.conf file
    jdbcSchemas:
        - catalog: "business" # Optional catalog name in the target database
          schema: "public" # Database schema where tables are located
          tables:
            - name: "user"
              write: "Append" # Append or Overwrite to describe how the data should be loaded
              pattern: "user-.*.csv" # File pattern to recognize when loading this type of table
              fechSize: 100 # May be defined and inherited from top or schema level
              partitionColumn: "id" # Only values more recent (higher) than the last exported ones will be exported. 
              numPartitions: 2 # level of parallelism. May be inherited from the schema level
              columns: # optional list of columns, if not present all columns should be exported.
                - id
                - email
            - name: product # All columns should be exported
            - name: "*" # Ignore any other table spec. Just export all tables
          tableTypes: # One or many of the types below
            - "TABLE"
            - "VIEW"
            - "SYSTEM TABLE"
            - "GLOBAL TEMPORARY"
            - "LOCAL TEMPORARY"
            - "ALIAS"
            - "SYNONYM"
          template: "/absolute/path/domain-template.yml" # Metadata to use for the generated YML file.
````

````json
connections {
  mypostgresdb {
    format = "jdbc"
    options {
      url: "jdbc:postgresql://127.0.0.1:5403/comet",
      user: "postgres",
      password: "ficpug-Podbid-7fobnu",
      driver: "org.postgresql.Driver"
    }
  }

  audit { /* Connection to the database that will register the extract history */ 
    format = "jdbc"
    options {
      url: "jdbc:postgresql://127.0.0.1:5403/comet",
      user: "postgres",
      password: "ficpug-Podbid-7fobnu",
      driver: "org.postgresql.Driver"
    }
  }
}
````

:::note

we may replace connectionRef tag by the connection tag and provide the connection options inline right inside the YAML configuration file as follows:

:::note

if you need to set common jdbc schema attributes, you can use globalJdbcSchema on the same level as jdbcSchemas and define the same attributes. Tables can't be set commonly.

````yaml
extract:
    connection:
      url: "jdbc:postgresql://127.0.0.1:5403/comet",
      user: "postgres",
      password: "ficpug-Podbid-7fobnu"
      driver: "org.postgresql.Driver"
    jdbcSchemas:
        - catalog: "business" # Optional catalog name in the target database
          schema: "public" # Database schema where tables are located
          tables:
      ...

````

:::

To extract all the tables, simply set the `name`attribute below the `table` attribute to "*"

To import all the columns of a table, do not define the columns attribute.

This will generate a YAML file with a metadata section in the metadata/domains directory:

```bash
$ starlake.sh extract-schema --config dbextract.comet.yml # extract description
```


## Extract Data

Simply run the 
```bash
$ starlake.sh extract-data \
    --config dbextract.comet.yml \ # extract description
    --output-dir data-out \ # output directory
    --separator ';' \ # use ';' as a separator
    --clean # clean output folder first 
```


