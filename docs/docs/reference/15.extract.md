---
sidebar_position: 10
---

# Extract

## Configuration

This step is optional and useful only if you intend to extract data from a SQL Database into
a set of files before ingesting it into a datalake or warehouse.

To extract the tables into DSV files, create a YAML specification file
that describe the tables and columns you are willing to extract using the following syntax:

````yaml
extract:
    jdbcSchemas:
        - connection: "test-h2" # Connection name as defined in the connections section of the application.conf file
          catalog: "business" # Optional catalog name in the target database
          schema: "public" # Database schema where tables are located
          tables:
            - name: "user"
              columns: # optional list of columns, if not present all columns should be exported.
                - id
                - email
            - name: product # All columns should be exported
            - name: "*" # Ignore any other table spec. Just export all tables
          tableTypes: # One or many of the types below
            - "TABLE"
            - "VIEW"
            - "SYSTEM TABLE"
            - "GLOBAL TEMPORARY"
            - "LOCAL TEMPORARY"
            - "ALIAS"
            - "SYNONYM"
          template: "/my-templates/domain-template.yml" # Metadata to use for the generated YML file.
````

To extract all the tables, simply set the "name" attribute to "*"

To import all the columns of a table, do not define the columns attribute.

This will generate a YML file with a metadata section.

Then we can [extract the data](#extract) with a specific mustache template using the [CLI](../cli/extract.md) 

Once data are extracted you can proceed to the [load step](../userguide/load.md).



## Extract Mustache Template

Write a mustache template that run a SQL export request to the target file.
The following parameters are available :

* full_export : Boolean. If true means that we are requesting a full export
* export_file : filename of the exported data
* table_name : table name in uppercase
* delimiter : delimiter to use in the export file
* columns : Column map with the single name attribute

Below an example for an Oracle database.

```sql
    -- How the data should be exported
    SET ECHO OFF
    SET VERIFY OFF
    SET TRIMSPOOL ON
    SET TRIMOUT ON
    SET LINESIZE 9999
    SET PAGESIZE 0
    -- We decide to export without any header.
    -- Do not forget to value the withHeader property to false in the YAML file
    SET HEADING OFF
    SET FEEDBACK OFF
    SET TIMING OFF
    SET TIME OFF
    SET LONG 10000
    -- The separator here should be the one used in the YAML file
    SET COLSEP ';'
    SET HEADSEP off

    -- Stop in case of failure
    WHENEVER SQLERROR exit 1;

    -- Data time pattern we want to use
    ALTER SESSION SET NLS_DATE_FORMAT = 'yyyymmddhh24miss';

    -- The output file directory
    DEFINE OUTPUT_DIR = &1;

    -- Get current date/time.
    COLUMN DT_VAL NEW_VALUE CURRENT_EXPORT_DATE_CHAR;
    SELECT TO_CHAR(SYSDATE) DT_VAL FROM DUAL;

    -- We store in a dedicated table, the last export date/time.
    -- Useful for incremental exports
    COLUMN LED NEW_VALUE LAST_EXPORT_DATE;
    SELECT
        COALESCE(
            (
                SELECT
                    MAX(DA_LAST_EXPORT_DATE)
                FROM
                    MY_SCHEMA.COMET_EXPORT_STATUS
                WHERE
                    LI_SCHEMA_NAME = 'MY_SCHEMA' AND
                    LI_TABLE_NAME = '{{table_name}}'
            ),
            TO_DATE('19700101','yyyymmdd') -- If table has never been exported
        ) LED
    FROM
        DUAL;

    -- Start export
    PROMPT EXPORTING {{table_name}} TO &OUTPUT_DIR/{{export_file}}_{{#full_export}}FULL{{/full_export}}{{^full_export}}DELTA{{/full_export}}_&CURRENT_EXPORT_DATE_CHAR\.csv;
    SPOOL &OUTPUT_DIR/{{export_file}}_{{#full_export}}FULL{{/full_export}}{{^full_export}}DELTA{{/full_export}}_&CURRENT_EXPORT_DATE_CHAR\.csv REPLACE

    ALTER SESSION SET NLS_DATE_FORMAT = 'yyyy-mm-dd hh24:mi:ss';

    -- SQL to execute if an incremental export is requested
    {{^full_export}}
    SELECT
        {{#columns}}
        TO_CHAR({{name}}) || ';' ||
        {{/columns}}
        ''
    FROM
        MY_SCHEMA.{{table_name}}
    WHERE
        {{delta_column}} >= '&LAST_EXPORT_DATE' AND {{delta_column}} IS NOT NULL;
    {{/full_export}}

    -- SQL to execute if a full export is requested
    {{#full_export}}
    SELECT
        {{#columns}}
        TO_CHAR({{name}}) || ';' ||
        {{/columns}}
        ''
    FROM
        MY_SCHEMA.{{table_name}};
    {{/full_export}}

    -- Export finished successfully
    SPOOL OFF

    -- Update reporot table containing the last expoort date
    -- This is useful for audit purpose and for incremental export since we store the last export date here.
    BEGIN
        INSERT INTO
            MY_SCHEMA.COMET_EXPORT_STATUS (LI_SCHEMA_NAME, LI_TABLE_NAME, DA_LAST_EXPORT_DATE, TYPE_LAST_EXPORT, NB_ROWS_LAST_EXPORT)
        VALUES
            (
                'MY_SCHEMA',
                '{{table_name}}',
                TO_DATE(&CURRENT_EXPORT_DATE_CHAR),
                {{#full_export}}
                'FULL',
                (
                    SELECT
                        COUNT(*)
                    FROM
                        MY_SCHEMA.{{table_name}}
                )
                {{/full_export}}
                {{^full_export}}
                'DELTA',
                (
                    SELECT
                        COUNT(*)
                    FROM
                        MY_SCHEMA.{{table_name}}
                    WHERE
                        {{delta_column}} >= '&LAST_EXPORT_DATE' AND {{delta_column}} IS NOT NULL
                )
                {{/full_export}}
            );
    END;
    /

    EXIT SUCCESS

    sqlplus sys/Ora_db1 as SYSDBA @ EXTRACT_{{table_name}}.sql /opt/oracle/user-scripts/scripts/
```
