---
sidebar_position: 10
---

# Extract

This step is optional and useful only if you intend to extract data from a SQL Database into
a set of files before ingesting it into a datalake or warehouse.

## Configuration

We'll have to go through the following steps to extract data from a JDBC compliant database
into a set of files that will be loaded into our datawarehouse:

1. Extract database table schemas
2. Create extraction scripts specific to your database engine

### Extract table schemas

To extract the tables into a set of DSV files, create a YAML specification file
that describe the tables and columns you are willing to extract using the following syntax:

````yaml
extract:
    connectionRef: "mypostgresdb" # Connection name as defined in the connections section of the application.conf file
    jdbcSchemas:
        - catalog: "business" # Optional catalog name in the target database
          schema: "public" # Database schema where tables are located
          tables:
            - name: "user"
              write: "Append" # Append or Overwrite to describe how the data should be loaded
              pattern: "user-.*.csv" # File pattern to recognize when loading this type of table
              columns: # optional list of columns, if not present all columns should be exported.
                - id
                - email
            - name: product # All columns should be exported
            - name: "*" # Ignore any other table spec. Just export all tables
          tableTypes: # One or many of the types below
            - "TABLE"
            - "VIEW"
            - "SYSTEM TABLE"
            - "GLOBAL TEMPORARY"
            - "LOCAL TEMPORARY"
            - "ALIAS"
            - "SYNONYM"
          template: "/absolute/path/domain-template.yml" # Metadata to use for the generated YML file.
````

````json
connections {
  mypostgresdb {
    format = "jdbc"
    options {
      url: "jdbc:postgresql://127.0.0.1:5403/comet",
      user: "postgres",
      password: "ficpug-Podbid-7fobnu",
      driver: "org.postgresql.Driver"
    }
  }
}
````

:::note

we may replace connectionRef tag by the connection tag and provide the connection options inline right inside the YAML configuration file as follows:

:::note

if you need to set common jdbc schema attributes, you can use globalJdbcSchema on the same level as jdbcSchemas and define the same attributes. Tables can't be set commonly.

````yaml
extract:
    connection:
      url: "jdbc:postgresql://127.0.0.1:5403/comet",
      user: "postgres",
      password: "ficpug-Podbid-7fobnu"
      driver: "org.postgresql.Driver"
    jdbcSchemas:
        - catalog: "business" # Optional catalog name in the target database
          schema: "public" # Database schema where tables are located
          tables:
      ...

````

:::

To extract all the tables, simply set the `name`attribute below the `table` attribute to "*"

To import all the columns of a table, do not define the columns attribute.

This will generate a YAML file with a metadata section based on the metadata .

Then we can create the extract scripts with a specific mustache / jinja / ssp template

## Extract Scripts

When you bootstrap a project, in the extract folder, you ll find in the `metadata/extract` sub-folder a hierarchy with one folder per database type

```
extract/
├── domain-template.comet.yml
├── mysql
│        └── extract-domain-table.sh.mustache
├── oracle
│        ├── extract-domain-table.sh.mustache
│        ├── extract-domain-table.sql.mustache
│        └── extract_init.sql.mustache
└── postgres
    └── extract-domain-table.sh.mustache
```

Each folder contains default script templates you can use or modify to fit your database configuration.
To build the script for a specific database, you'll have to run the following command:

```bash
$ starlake.sh extract-script \
    --template oracle # May be mysql, oracle, postgres or any folder name located in the metadata/extract folder and containing .mustache/ .j2 / .ssp template files.
    --domain domain1,domain2 # List of daomains to build the scripts for. If this parameter is not specified, all domains are processed.
    --audit-schema # Schema where the data extraction report table is created and reports are stored. Whennot specified, the table is created in the same schema as the table being extracted
    --delta-column # Only in Append mode (extraction in incremental mode), this is the column to use detect new data since last extraction. Usually a timestamp or a auto incremented numeric id. 
```

This will generate one script per table and when run, each script will extract the data to one file per table. Scripts are generated in the `generated` subdirectory.

If you need to create your own extraction script template, the following variables may be referenced in your custom template:

- domain: Name of the database schema of the table being exported. 
- table: Table name in lowercase
- delimiter: character/string to use as a column delimiter in the target CSV file
- columns: List of columns, more details below. 
- full_export: Is it a table that should eb extracted in incremental mode or full mode ? 
- audit_schema:  database schema of the extraction report table


The list of columns above is a list of object with the following attributes:
- name: Column name in lowercase
- type: Database column type
- ignore: Should we ignore the extraction of this column ?
- privacyLevel: UDF to apply to this column
- trailing_col_char: Trailing char if this column is not the last one. Useful in mustache template templates (see the oracle extraction example in the extract/oracle directory)

