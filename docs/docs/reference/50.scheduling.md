---
sidebar_position: 50
title: Scheduling
---

# Scheduling
Scheduling Spark jobs using the native job schedulers such as Airflow fits the following cases:

- You are running Starlake wth an explicit reference to the domain and schema you want to ingest. 
In this case, you need to create as many jobs as the number of schemas

But what if you want to run one job per domain or one job for all the domains and let Starlake detect the 
file type. In that case, Starlake will run sequentially the ingestion on each file for which a schema is defined.

using the property `grouped` or env var `SLK_GROUP`, you will only be able to ingest files of the exact same schema simultaneously.

To ingest concurrently files with a different schema, Starlake takes advantage of the [Spark intra-application Scheduler](https://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application).
Just create in the `metadata` directory, a file named `fairscheduler.xml` and define the pools as described in the Spark documentation and set the following variables:

- mode: with the value `FIFO` to run the jobs sequentially or `FAIR` to run the jobs concurrently
- pool-name: set it to the name of the pool assigned to Starlake in the file `fairscheduler.xml`.
- file (optional): set this value to the absolute path of the file `fairscheduler.xml` if it is not located in the `metadata` folder. 
When set to the  empty string, the default job scheduler is used. 
- max-jobs: the number of ingestion to run concurrently with the same Spark Session. The value 1 meaning no concurrency.

```
scheduling {
  max-jobs = 10
  mode = "FAIR"
  file = "" // empty string means the file fairscheduler.xml is located in the metadata directory 
  pool-name = "starlake-pool"
}
```

