# Filesystem

A filesystem is the location where datasets and Starlake Data Pipeline metadata used for ingestion are stored.
* On premise this reference the folder where datasets and metadata are stored, eq.
  * On a local filesystem: file://
  * On a HDFS: hdfs://localhost:9000
* In the cloud:
  * On Google Cloud Platform: gs://my-bucket
  * On Microsoft Azure: abfs://my-bucket@starlake.dfs.core.windows.net
  * On Amazon Web Service: s3a://my_bucket
  
By default, Starlake expect metadata in the /tmp/metadata folder and will store ingested datasets in the /tmp/datasets folder.
Below is how the folders look like by default for the provided `userguide` sample.

````
    /userguide
    |-- datasets (Root folder of ingested datasets)
    |   |-- accepted (Root folder of all valid records)
    |   |   |-- hr (domain name as specified in the name attribute of the /tmp/metadata/hr.yml)
    |   |   |   `-- sellers (Schema name as specified in the /tmp/metadata/hr.yml)
    |   |   |       |-- _SUCCESS
    |   |   |       `-- part-00000-292c081b-7291-4797-b935-17bc9409b03b.snappy.parquet
    |   |   `-- sales
    |   |       |-- customers (valid records for this schema as specified in the /tmp/metadata/sales.yml)
    |   |       |   |-- _SUCCESS
    |   |       |   `-- part-00000-562501a1-34ef-4b94-b527-8e93bcbb5f89.snappy.parquet
    |   |       `-- orders (valid records for this schema as specified in the /tmp/metadata/sales.yml)
    |   |           |-- _SUCCESS
    |   |           `-- part-00000-92544093-4ae2-4a98-8df8-a5aba19a1b27.snappy.parquet
    |   |-- archive (Source files as found in the incoming folder are saved here after processing)
    |   |   |-- hr (Domain name)
    |   |   |   `-- sellers-2018-01-01.json
    |   |   `-- sales
    |   |       |-- customers-2018-01-01.psv
    |   |       `-- orders-2018-01-01.csv
    |   |-- business
    |   |   |-- hr
    |   |   `-- sales
    |   |-- metrics
    |   |   |-- discrete
    |   |   |-- continuous
    |   |   `-- frequencies
    |   |-- ingesting (Temporary folder used during ingestion by Starlake)
    |   |   |-- hr (One temporary subfolder / domain)
    |   |   `-- sales
    |   |-- pending (Source files are copied here from the incoming folder before processing)
    |   |   |-- hr (one folder / domain)
    |   |   `-- sales
    |   |-- rejected (invalid records in processed datasets are stored here)
    |   |   |-- hr (Domain name)
    |   |   |   `-- sellers (Schema name)
    |   |   |       |-- _SUCCESS
    |   |   |       `-- part-00000-aef2dde6-af24-4e20-ad88-3e5238916e57.snappy.parquet
    |   |   `-- sales
    |   |       |-- customers
    |   |       |   |-- _SUCCESS
    |   |       |   `-- part-00000-e6fa5ff9-ad29-4e5f-a5ff-549dd331fafd.snappy.parquet
    |   |       `-- orders
    |   |           |-- _SUCCESS
    |   |           `-- part-00000-6f7ba5d4-960b-4ac6-a123-87a7ab2d212f.snappy.parquet
    |   `-- unresolved (Files found in the incoming folder but do not match any schema)
    |       `-- hr
    |           `-- dummy.json
    `-- metadata (Root of metadata files)
        |-- load (all domain definition files are located in this folder)
        |   |-- hr/_config.sl.yml (One definition file / domain)
        |   `-- hr/sales.yml
        `-- assertions (All assertion definitions go here)
        |   |-- default.sl.yml (Predefined assertion definitions)
        |   `-- assertions.sl.yml (assertion definitions defined here are accessible throughout the project)
        `-- types (All semantic types are defined here)
        |   |-- default.sl.yml (Default semantic types)
        |   `-- types.sl.yml (User defined semantic types, overwrite default ones)
        `-- transform (All transform jobs go here)
            `-- hr/sales_by_name.sql (Compute sales by )
````

Starlake Data Pipeline allows you to store datasets and metadata in two different filesystems. Thi is useful if you want to define a specific lifecycle for your datasets.
Almost all options are customizable through environnement variables.
The main env vars are described below, you may change default settings. The exhaustive list of predefined env vars can be found in the src/main/resource/reference.conf file.

|HOCON Variable|Env variable|Default Value|Description
|:--------------|:------------|:-------|:-----------
|root|SL_ROOT|/tmp|Root directory of the datasets and metadata files in the defined filesystem above
|datasets|SL_DATASETS|${root}"/datasets"|Folder where datasets are located in the datasets `file-system`
|metadata|SL_METADATA|${root}"/metadata" otherwise|Folder where metadata are located in the metadata `metadata-file-system`
|area.pending|SL_AREA_PENDING|pending|Source files are copied here from the incoming folder before processing
|area.unresolved|SL_AREA_UNRESOLVED|unresolved|Files found in the incoming folder but do not match any schema
|area.archive|SL_AREA_ARCHIVE|archive|Source files as found in the incoming folder are saved here after processing
|area.ingesting|SL_AREA_INGESTING|ingesting|Temporary folder used during ingestion by Starlake
|area.accepted|SL_AREA_ACCEPTED|accepted|root folder of all valid records
|area.rejected|SL_AREA_REJECTED|rejected|invalid records in processed datasets are stored here
|area.business|SL_AREA_BUSINESS|business|root folder for all datasets produced by autojobs
|archive|SL_ARCHIVE|true|Should we archive the incoming files once they are ingested
|default-write-format|SL_DEFAULT_WRITE_FORMAT|parquet|How accepted records are stored (parquet / orc / json / csv / avro)
|default-rejected-write-format|SL_DEFAULT_REJECTED_WRITE_FORMAT|parquet|How rejected records are stored (parquet / orc / json / csv / avro)
|default-audit-write-format|SL_DEFAULT_AUDIT_WRITE_FORMAT|parquet|How audit is stored (parquet / orc / json / csv / avro)
|hive|SL_HIVE|true|Should we create external Hive tables for ingested files ?
|analyze|SL_ANALYZE|true|Should we computed basic statistics ? (requires SL_HIVE to be set to true)
|launcher|SL_LAUNCHER|simple|Which orchestrator to use ? Valid values are airflow or simple (direct call)


To make sure, the same schema is not ingested by two concurrent Starlake processes, Starlake Data Pipeline uses a file lock when necessary.

|HOCON Variable|Env variable|Default Value|Description
|:--------------|:------------|:-------|:-----------
|lock.path|SL_LOCK_PATH|${root}"/locks"|Root folder where lock file is created
|lock.timeout|SL_LOCK_TIMEOUT|-1|How long to wait for the file lock to be available (in seconds)

