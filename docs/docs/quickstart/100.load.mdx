---
sidebar_position: 100
title: Load
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

In this section you will learn how to load and transform data using the samples files created when [bootstrapping a new project](bootstrap).
You will learn how to:
- import: recognize new arriving files that need to be loaded into your warehouse
- load: validate file records and load data into you warehouse
- transform: apply transformation on your previously loaded data


## Sample scenario

Say we have to load the `customers` file into the warehouse.
The customers are provided by the "sales" department as delimited separated values files
and are required to be loaded incrementally.


#### customers dataset

``File customers-2018-01-01.psv from "sales" department``

|id|signup|contact|birthdate|firstname|lastname|country|
|---|---|---|---|---|---|---|
|A009701|2010-01-31 23:04:15|k@m.com|1980-04-15|Kylian|Mbapp√©|France|
|B000001|2016-12-01 09:56:02|n@b.com|1980-04-15|Napoleon|Bonaparte|France|
|B000001|2016-12-02 09:56:02|m@c.com|1980-04-15|Marie|Curie|France|
|B000002|2016-12-02 09:56:02|z@z.com|1980-04-15|Zinedine|Zidane|France|
|B000003|2016-12-03 09:56:02|e@g.com|1980-04-15|Eva|Green|France|
|B000012|2016-12-03 09:56:02|k@b.com|1980-04-15|Karim|Benzema|France|
|B000004|2016-12-04 09:56:02|m@c.com|1980-04-15|Marion|Cotillard|France|
|B000005|2016-12-05 09:56:02|a@g.com|1980-04-15|Ariana|Grande|USA|
|B000006|2016-12-06 09:56:02|m@j.com|1980-04-15|Michael|Jordan|USA|
|B000007|2016-12-07 09:56:02|m@a.com|1980-04-15|Muhammad|Ali|USA|
|B000008|2016-12-08 09:56:02|t@s.com|1980-04-15|Taylor|Swift|USA|
|B000009|2016-12-09 09:56:02|e@p.com|1980-04-15|Elvis|Presley|USA|
|B000010|2016-12-10 09:56:02|s@j.com|1980-04-15|Steve|Jobs|USA|
|B000011|2016-12-11 09:56:02|a@l.com|1980-04-15|Abraham|Lincoln|USA|


## Infer the schema

In Starlake terms, after loading, we will end up with:
- one domain: `sales` . A domain is equivalent to a database schema or a BigQuery Dataset.
- one table: the `customers` table in the `sales` domain 

We first need to write a YML configuration file that describe the structure of the file to load into the warehouse.
Instead of writing this file by hand, we may infer this YML configuration file using the `infer-schema` command.



<Tabs groupId="platforms">
<TabItem value="linux_macos" label="Linux/MacOS">

```sh
$ cd $HOME/myproject
$ $HOME/starlake/starlake.sh infer-schema               \
    --domain sales                                      \
    --table customers                                   \
    --input incoming/sales/customers-2018-01-01.psv     \
    --output-dir metadata/domains --with-header
```

</TabItem>
<TabItem value="windows" label="Windows">

```powershell
c:\users\me\starlake> %userprofile%\starlake\starlake.cmd infer-schema  ^
    --domain sales                                                      ^
    --table customers                                                   ^
    --input incoming/sales/customers-2018-01-01.psv                     ^
    --output-dir metadata/domains --with-header
```

</TabItem>
<TabItem value="docker" label="Docker">

```shell 
$ docker run                                                        \
    -e COMET_ROOT=/app/myproject                                    \
    -v $HOME/myproject:/app/myproject -it starlake infer-schema     \
    --domain sales                                                  \
    --table customers                                               \
    --input $HOME/myproject/incoming/sales/customers-2018-01-01.psv \
    --output-dir $HOME/myproject/metadata/domains                   \
    --with-header```
```

</TabItem>
</Tabs>

This command will infer the YML configuration file used by the loader to watch the folder for new files to load and to validate each record in those files.
This file may be found in the `sales.comet.yml` file under the `metadata/domains/` folder.

The contents of the file looks like this:

```yaml
---
load:
  name: "sales"         # The domain name
  metadata:             # All load specific rules regarding the file go here
    mode: "FILE"        # we are loading a file here
    format: "DSV"       # The format is delimiter separated values
    encoding: "UTF-8"   # The input is encoded in UTF-8
    multiline: false    # One line per record
    array: false        # Is it an array of JSON. Ignored in DSV mode
    withHeader: true    # Does the file contain a header ?
    separator: "|"      # What is the delimiter value (multi-char delimited are allowed) ?
    quote: "\""         # How fields are quoted if any
    escape: "\\"        # what is the escape char ?
    write: "APPEND"     # Should we overwrite the destination or append data in the destination table
    directory: "{{root_path}}/incoming/sales" # Where will the files land before loading ? Please set the root_path variable in metadata/env.comet.yml 
  tables:
  - name: "customers"   # destination table name
    pattern: "customers-2018-01-01.psv" #file pattern. Please replace it by a file pattern eq. customers-.*.psv
    attributes:         # Description of the fields to recognize
    - name: "id"        # attribute name and column name in the destination table if no rename attribute is defined
      type: "string"    # expected type
      array: false      # is it an array (false by default, ignored in DSV files) ? 
      required: false   # Is this field required in the source (false by default, change it accordingly) ?
      privacy: "NONE"   # Should we crypt this field before loading to the warehouse (No crypting by default )?
      ignore: false     # Should this field be excluded (false by default) ?
    - name: "signup"    # second attribute
      type: "timestamp" # recognized type by analyzing input.
    - name: "contact"
      type: "string"
      ...
    - name: "birthdate"
      type: "date"      # recognized as semantic type date.
      ...
    - name: "firstname"
      type: "string"
      ...
    - name: "lastname"
      type: "string"
      ...
    - name: "country"
      type: "string"
      ...               # and so on ...

```

We have seen in the [bootstrap section](bootstrap) that three environment files were created such as `metadata/env.comet.yml`.
These environment files define variables that will be used for runtime substitution in any YAML file. Variables in YAML files are enclosed with {{ and }}.

Furthermore, it is important to know that `metadata/env.comet.yml` define the default variables.
Any other environment files will be mixed with this default one and specific environment file have higher precedence.
In other word, we have the following environments

| Environment | File used                       |
|-------------|---------------------------------|
| default     | env.comet.yml                   |
| FS          | env.FS.comet.yml, env.comet.yml |
| BQ          | env.BQ.comet.yml, env.comet.yml |

Environment is set thanks to `COMET_ENV` environment variable. For more details, have a look at the [environment reference](../reference/environment)

In our example, the `root_path` must be defined to the location where the incoming directory containing the datasets to load will be located.
Set it to your project directory as follows in the default environment file:

```
env:
  root_path: ${HOME}/myproject
```

## Load files
Loading is a two step process: The optional `import` step and the `load` step.

### Import step
In this first step, Starlake will look at the directory defined in the YAML file and look at the file that matches the expected pattern.
In our example, the directory is `{{root_path}}/incoming/sales` and the expected file pattern has been changed to `customers-.*.psv`
`import` move files from domain directory defined in any domain files such as `metadata/domains/sales.comet.yml` to the `datasets/pending` directory.
Moving files is only done when it matches at least one table pattern otherwise it is ignored.

Before loading our data, we are going to add another entry in our yaml file by setting `load.metadata.ack=ack`.
This entry define the behavior for all tables defined in our yaml file to wait for an ack with the same name but ending with `.ack` file before importing it.
By default, the generated yaml doesn't check for any `.ack` file, hence all files matching the pattern are imported.

<Tabs groupId="platforms">
<TabItem value="linux_macos" label="Linux/MacOS">

```sh
$ cd $HOME/myproject
$ $HOME/starlake/starlake.sh import
```

</TabItem>
<TabItem value="windows" label="Windows">

```powershell
c:\users\me\starlake> %userprofile%\starlake\starlake.cmd import
```

</TabItem>
<TabItem value="docker" label="Docker">

```shell 
$ docker run                                                        \
    -e COMET_ROOT=/app/myproject                                    \
    -v $HOME/myproject:/app/myproject -it starlake import
```

</TabItem>
</Tabs>

The file has now been moved to the `myproject/datasets/pending/sales` directory.

:::note

This step is optional and does not need to be run if your files directly arrive in the `datasets/pending/sales` folder.
Also note that all these source directories may be [redefined](../reference/configuration).

:::

### Load step

In this second step, each line in the file present in the `datasets/pending` folder is checked against the schema described in the YAML file
and its result is stored in the warehouse.

Before loading data to the warehouse, create a copy of the psv file and name it `unknown-table.psv`. Then run the `watch` command to load data as below:

<Tabs groupId="platforms">
<TabItem value="linux_macos" label="Linux/MacOS">

```sh
$ cd $HOME/myproject
$ $HOME/starlake/starlake.sh watch
```

</TabItem>
<TabItem value="windows" label="Windows">

```powershell
c:\users\me\starlake> %userprofile%\starlake\starlake.cmd watch
```

</TabItem>
<TabItem value="docker" label="Docker">

```shell 
$ docker run                                                        \
    -e COMET_ROOT=/app/myproject                                    \
    -v $HOME/myproject:/app/myproject -it starlake watch
```

</TabItem>
</Tabs>

This will load the dataset and store it as a parquet file into the folders:
- `datasets/accepted` for valid records
- `datasets/rejected` for invalid records
- `datasets/unresolved` for unrecognized files

The latter one is where you should find `unknown-table.psv`.

:::note

Through minimum extra configuration, we are able to run Starlake on top of any warehouse and have these datasets available as tables.

:::

:::caution

Starlake validate the data against the table's schema of the first pattern that match with the file name.
Hence, you must be careful regarding the pattern you set.
Make sure that there is no overlap.

:::
