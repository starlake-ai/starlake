# Load

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

In this section you will learn how to load and transform data using the samples files created when [bootstrapping a new project](bootstrap).
You will learn how to:
- load: validate file records and load data into you warehouse
- transform: apply transformation on your previously loaded data


## Load files
Loading is a two step process: The optional `import` step and the `load` step.

### Import step
In this first step, Starlake will look at the _directory_ attribute value in the YAML files and look at the file that matches the expected patterns defined in the table definition files.
In our example, the directories are `{{incoming_path}}/sales` and `{{incoming_path}}/hr`.

the `import` command moves the files that satisfy one table pattern from the _incoming_ folder to the `datasets/pending` folder.
Files that do not satisfy any pattern won't be loaded and are moved to the `datasets/unresolved` directory.


<Tabs groupId="platforms">
<TabItem value="linux_macos" label="Linux/MacOS">

```sh
$ cd $HOME/quickstart
$ starlake import
```

</TabItem>
<TabItem value="windows" label="Windows">

```powershell
c:\users\me\quickstart> starlake import
```

</TabItem>
<TabItem value="docker" label="Docker">

```shell
$ docker run                                                  \
    -e SL_ROOT=/app/quickstart                                \
    -v $HOME/quickstart:/app/quickstart -it starlake import
```

</TabItem>
</Tabs>

The sample data files has now been moved to the `quickstart/datasets/pending/sales` directory.

:::note

This step is optional and does not need to be run if your files directly arrive in the `datasets/pending/sales` folder.
Also note that all these source directories may be [redefined](../reference/configuration).

:::

### Load step

In this second step, each line in the file present in the `datasets/pending` folder is checked against the schema described in the YAML file
and its result is stored in the warehouse.


<Tabs groupId="platforms">
<TabItem value="linux_macos" label="Linux/MacOS">

```sh
$ cd $HOME/quickstart
$ starlake load
```

</TabItem>
<TabItem value="windows" label="Windows">

```powershell
c:\users\me\starlake> starlake load
```

</TabItem>
<TabItem value="docker" label="Docker">

```shell
$ docker run                                                      \
    -e SL_ROOT=/app/quickstart                                    \
    -v $HOME/myproject:/app/quickstart -it starlake load
```

</TabItem>
</Tabs>

This will load the data files and since we chose the localFilesystem connection, they will be stored them as parquet files into the following folders:
- `datasets/accepted` for valid records
- `datasets/rejected` for invalid records
- `datasets/unresolved` for unrecognized files


:::caution

Starlake validate the data against the table's schema of the first pattern that match with the file name.
Hence, you must be careful regarding the pattern you set.
Make sure that there is no overlap.

:::


### Check the result

You can check the result by running the following python script to read the parquet file from the project directory:

```python
import pandas as pd
filepath = 'datasets/accepted/sales/customers/'
df = pd.read_parquet(filepath)
df.head
```

### Loading the data into a different store
We just loaded our text file as a parquet file. This is a very common format for data scientists and analysts.
Through minimum extra configuration, we are able to run Starlake on top of any warehouse and have these datasets available as tables.

The examples below describe the extra configuration required in the metadata/application.sl.yml configuration file
to load the data into  bigquery, databricks, redshift, snowflake or postgresql.



<Tabs groupId="datawarehouse">
<TabItem value="bigquery" label="bigquery">

```yaml
application:
  connectionRef: "bigquery"
  loader: native
  connections:
    bigquery:
      type: "bigquery"
      options:
        location: "EU" # EU or US or ..
        authType: "APPLICATION_DEFAULT"
        authScopes: "https://www.googleapis.com/auth/cloud-platform" # comma separated list of scopes
        #authType: SERVICE_ACCOUNT_JSON_KEYFILE
        #jsonKeyfile: "/Users/me/.gcloud/keys/starlake-me.json"
        #authType: "ACCESS_TOKEN"
        #gcpAccessToken: "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"
```  

</TabItem>
<TabItem value="databricks" label="databricks">

```yaml
application:
  connectionRef: "databricks"
  loader: spark # note the spark loader here
  connections:
    localFilesystem:
      type: "databricks"
```

</TabItem>
<TabItem value="postgresql" label="postgresql">

```yaml
application:
  connectionRef: "postgresql"
  loader: spark # note the spark loader here
  connections:
    postgresql:
      type: jdbc
      options:
        url: "jdbc:postgresql://{{POSTGRES_HOST}}:{{POSTGRES_PORT}}/{{POSTGRES_DATABASE}}"
        driver: "org.postgresql.Driver"
        supportTruncateOnInsert: false
        user: "{{DATABASE_USER}}"
        password: "{{DATABASE_PASSWORD}}"
        quoteIdentifiers: false
```  

</TabItem>
<TabItem value="redshift" label="redshift">

```yaml
application:
  connectionRef: "reshift"
  loader: spark # note the spark loader here
  connections:
    redshift:
      sparkFormat: "com.databricks.spark.redshift"
      options:
        url: "jdbc:redshift://redshifthost:5439/database",
        user: "username",
        password: "pass",
        tempdir: "s3n://path/for/temp/data",
        aws_iam_role: "arn:aws:iam::123456789000:role/redshift_iam_role"
```

</TabItem>
<TabItem value="snowflake" label="snowflake">

```yaml
application:
  connectionRef: "snowflake"
  loader: native
  connections:
    snowflake:
      type: jdbc
      options:
        url: "jdbc:snowflake://{{SNOWFLAKE_ACCOUNT}}.snowflakecomputing.com/"
        driver: "net.snowflake.client.jdbc.SnowflakeDriver"
        user: "{{SNOWFLAKE_USER}}"
        password: "{{SNOWFLAKE_PASSWORD}}"
        warehouse: "{{SNOWFLAKE_WAREHOUSE}}"
        db: "{{SNOWFLAKE_DB}}"
        keep_column_case: "off"
        preactions: "alter session set TIMESTAMP_TYPE_MAPPING = 'TIMESTAMP_LTZ';ALTER SESSION SET QUOTED_IDENTIFIERS_IGNORE_CASE = true"
```

</TabItem>
<TabItem value="spark-bigquery" label="spark bigquery">

```yaml
application:
  connectionRef: "spark-bigquery"
  loader: spark # note the spark loader here
  connections:
    spark-bigquery:
      type: "bigquery"
      options:
        location: "EU" # EU or US or ..
        authType: "APPLICATION_DEFAULT"
        authScopes: "https://www.googleapis.com/auth/cloud-platform" # comma separated list of scopes
        writeMethod: "direct" # direct or indirect (indirect is required for certain features see https://github.com/GoogleCloudDataproc/spark-bigquery-connector)
        #gcsBucket: "starlake-app" # Temporary GCS Bucket where intermediary files will be stored. Required in indirect mode only
        #authType: SERVICE_ACCOUNT_JSON_KEYFILE
        #jsonKeyfile: "/Users/me/.gcloud/keys/starlake-me.json"
        #authType: "ACCESS_TOKEN"
        #gcpAccessToken: "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"
```

</TabItem>
<TabItem value="spark-snowflake" label="spark snowflake">

```yaml
application:
  connectionRef: "spark-snowflake"
  loader: spark # note the spark loader here
  connections:
      spark-snowflake:
        type: jdbc
        sparkFormat: snowflake
        options:
          sfUrl: "{{SNOWFLAKE_ACCOUNT}}.snowflakecomputing.com" # make sure you do not prefix by jdbc:snowflake://. This is done by the snowflaek driver
          #sfDriver: "net.snowflake.client.jdbc.SnowflakeDriver"
          sfUser: "{{SNOWFLAKE_USER}}"
          sfPassword: "{{SNOWFLAKE_PASSWORD}}"
          sfWarehouse: "{{SNOWFLAKE_WAREHOUSE}}"
          sfDatabase: "{{SNOWFLAKE_DB}}"
          keep_column_case: "off"
          autopushdown: on
          preactions: "alter session set TIMESTAMP_TYPE_MAPPING = 'TIMESTAMP_LTZ';ALTER SESSION SET QUOTED_IDENTIFIERS_IGNORE_CASE = true"

```

</TabItem>
</Tabs>

Using Spark instead of the BigQuery Load API may slow down the ingestion process but it has among others the following advantages:
- It allows to load data from any source supported by Spark including Fixed Width Files, XML files, JSON Arrays files ...
- It allows to load data into any destination supported by Spark including Snowflake, Amazon Redshift ...
- It allows to apply any transformation supported by Spark
- It allows to report any number of errors instead of 5 errors max with the BigQuery Load API (This is a BigQuery API Limitation)


To load the data into BigQuery, simply put back the samples data files now archived in the datasets folder back
to the sample-data/hr and sample-data/sales folders and run the _import_ and _load_ commands again.

:::note

Spark is not required to load data into any of the target datawarehouse, nor is it required to run transformations on the data.

Even when using Spark, you do not need to instantiate a cluster.  Spark becomes useful for advanced data validation at load time.

:::
