---
sidebar_position: 100
title: Load
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

In this section you will learn how to load and transform data using the samples files created when [bootstrapping a new project](bootstrap).
You will learn how to:
- import: recognize new arriving files that need to be loaded into your warehouse
- load: validate file records and load data into you warehouse
- transform: apply transformation on your previously loaded data


## Sample scenario

Say we have to load the `customers` file into the warehouse.
The customers are provided by the "sales" department as delimited separated values files
and are required to be loaded incrementally.


#### customers dataset

``File customers-2018-01-01.psv from "sales" department``

|id|signup|contact|birthdate|firstname|lastname|country|
|---|---|---|---|---|---|---|
|A009701|2010-01-31 23:04:15|k@m.com|1980-04-15|Kylian|Mbapp√©|France|
|B000001|2016-12-01 09:56:02|n@b.com|1980-04-15|Napoleon|Bonaparte|France|
|B000001|2016-12-02 09:56:02|m@c.com|1980-04-15|Marie|Curie|France|
|B000002|2016-12-02 09:56:02|z@z.com|1980-04-15|Zinedine|Zidane|France|
|B000003|2016-12-03 09:56:02|e@g.com|1980-04-15|Eva|Green|France|
|B000012|2016-12-03 09:56:02|k@b.com|1980-04-15|Karim|Benzema|France|
|B000004|2016-12-04 09:56:02|m@c.com|1980-04-15|Marion|Cotillard|France|
|B000005|2016-12-05 09:56:02|a@g.com|1980-04-15|Ariana|Grande|USA|
|B000006|2016-12-06 09:56:02|m@j.com|1980-04-15|Michael|Jordan|USA|
|B000007|2016-12-07 09:56:02|m@a.com|1980-04-15|Muhammad|Ali|USA|
|B000008|2016-12-08 09:56:02|t@s.com|1980-04-15|Taylor|Swift|USA|
|B000009|2016-12-09 09:56:02|e@p.com|1980-04-15|Elvis|Presley|USA|
|B000010|2016-12-10 09:56:02|s@j.com|1980-04-15|Steve|Jobs|USA|
|B000011|2016-12-11 09:56:02|a@l.com|1980-04-15|Abraham|Lincoln|USA|


## Infer the schema

In Starlake terms, after loading, we will end up with:
- one domain: `sales` . A domain is equivalent to a database schema or a BigQuery dataset.
- one table: the `customers` table in the `sales` domain

We first need to write a YAML configuration file that describe the structure of the file to load into the warehouse.
Instead of writing this file by hand, we may infer this YAML configuration file using the `infer-schema` command.



<Tabs groupId="platforms">
<TabItem value="linux_macos" label="Linux/MacOS">

```sh
$ cd $HOME/myproject
$ $HOME/starlake/starlake.sh infer-schema               \
    --domain sales                                      \
    --table customers                                   \
    --input incoming/customers-2018-01-01.psv           \
    --with-header
```

</TabItem>
<TabItem value="windows" label="Windows">

```powershell
c:\users\me\starlake> %userprofile%\starlake\starlake.cmd infer-schema  ^
    --domain sales                                                      ^
    --table customers                                                   ^
    --input incoming/customers-2018-01-01.psv                           ^
    --with-header
```

</TabItem>
<TabItem value="docker" label="Docker">

```shell
$ docker run                                                        \
    -e SL_ROOT=/app/myproject                                       \
    -v $HOME/myproject:/app/myproject -it starlake infer-schema     \
    --domain sales                                                  \
    --table customers                                               \
    --input $HOME/myproject/incoming/customers-2018-01-01.psv       \
    --with-header```
```

</TabItem>
</Tabs>

This inferred YAML file may be found in the `customers.comet.yml` file under the `metadata/load/sales` folder.
Notice how the `customers.comet.yml` file is named after the _table_ name and stored in the folder named after the _domain_ name.
The domain configuration file __config.comet.yml_ is also stored in the _domain_ folder.


The contents of the files look like this:

### Domain configuration file: _config.comet.yml
This file describes the properties shared by all tables in this domain.
Here we assume that all tables in the `sales` domain are loaded from the _incoming_ folder.

`{{root_path}}` is a variable path set in the env file (more on this later).

```yaml title="Domain configuration file: _config.comet.yml"
---
load:
  name: "sales"
  metadata:
    directory: "{{root_path}}/incoming"
```
You may change the path referenced by the _directory_ attribute to any other path.


### Table configuration file: customers.comet.yml

The YAML file describes the structure of the file to load into the warehouse.
The `pattern` property is a regular expression that will be used to match the file name.

```yaml title="Table schema: customers.comet.yml"
---
table:
  name: "customers"   # destination table name
  pattern: "customers.*.psv" # This property is a regular expression that will be used to match the file name.
                                      # Please replace it by the adequate file pattern eq. customers-.*.psv if required
  attributes:         # Description of the fields to recognize
    - name: "id"        # attribute name and column name in the destination table if no rename attribute is defined
      type: "string"    # expected type
      array: false      # is it an array (false by default, ignored in DSV files) ?
      required: false   # Is this field required in the source (false by default, change it accordingly) ?
      privacy: "NONE"   # Should we encrypt this field before loading to the warehouse (No encryption by default )?
      ignore: false     # Should this field be excluded (false by default) ?
    - name: "signup"    # second attribute
      type: "timestamp" # recognized type by analyzing input.
    - name: "contact"
      type: "string"
      # ...
    - name: "birthdate"
      type: "date"      # recognized as semantic type date.
      # ...
    - name: "firstname"
      type: "string"
      # ...
    - name: "lastname"
      type: "string"
      # ...
    - name: "country"
      type: "string"
      # ...               # and so on ...
  metadata:
    mode: "FILE"
    format: "DSV"
    encoding: "UTF-8"
    multiline: false
    array: false
    withHeader: true
    separator: "|"
    quote: "\""
    escape: "\\"

```

We have seen in the [bootstrap section](bootstrap) that three environment files.
These environment files define the variables that will be used for runtime substitution in any YAML file. Variables in YAML files are enclosed with {{ and }}.

:::note

You may also use the _${ }_ syntax to define variables.

:::


The environment file `metadata/env.comet.yml` define the default variables.
Any other environment file will be mixed with this default one and specific environment file have higher precedence.
In our example, we have the following environment files:

| Environment | File used                          |
|-------------|------------------------------------|
| default     | env.comet.yml                      |
| LOCAL       | env.LOCAL.comet.yml, env.comet.yml |
| BQ          | env.BQ.comet.yml, env.comet.yml    |

The active environment is set through the `SL_ENV` environment variable. For more details, have a look at the [environment reference](../reference/environment)

In our example, the `root_path` must be defined to the location where the incoming directory containing the datasets to load will be located.
Set it to your project directory as follows in the default environment file:

```
env:
  root_path: ${HOME}/myproject
```

or simply leave it as `{{SL_ROOT}}`.

`SL_ROOT` is a special variable that is set to the directory from where the `starlake` command is executed.


## Load files
Loading is a two step process: The optional `import` step and the `load` step.

### Import step
In this first step, Starlake will look at the _directory_ attribute value in the YAML file and look at the file that matches the expected pattern defined in the table definition.
In our example, the directory is `{{root_path}}/incoming` and the expected file pattern has been changed to `customers.*.psv`

the `import` command moves the files that satisfy one table pattern from the _incoming_ folder to the `datasets/pending` folder.
Files that do not satisfy any pattern won't be loaded and are moved to the `datasets/unresolved` directory.


<Tabs groupId="platforms">
<TabItem value="linux_macos" label="Linux/MacOS">

```sh
$ cd $HOME/myproject
$ $HOME/starlake/starlake.sh import
```

</TabItem>
<TabItem value="windows" label="Windows">

```powershell
c:\users\me\starlake> %userprofile%\starlake\starlake.cmd import
```

</TabItem>
<TabItem value="docker" label="Docker">

```shell
$ docker run                                                        \
    -e SL_ROOT=/app/myproject                                    \
    -v $HOME/myproject:/app/myproject -it starlake import
```

</TabItem>
</Tabs>

The _customers_ file has now been moved to the `myproject/datasets/pending/sales` directory.

:::note

This step is optional and does not need to be run if your files directly arrive in the `datasets/pending/sales` folder.
Also note that all these source directories may be [redefined](../reference/configuration).

:::

### Load step

In this second step, each line in the file present in the `datasets/pending` folder is checked against the schema described in the YAML file
and its result is stored in the warehouse.


<Tabs groupId="platforms">
<TabItem value="linux_macos" label="Linux/MacOS">

```sh
$ cd $HOME/myproject
$ $HOME/starlake/starlake.sh load
```

</TabItem>
<TabItem value="windows" label="Windows">

```powershell
c:\users\me\starlake> %userprofile%\starlake\starlake.cmd load
```

</TabItem>
<TabItem value="docker" label="Docker">

```shell
$ docker run                                                        \
    -e SL_ROOT=/app/myproject                                    \
    -v $HOME/myproject:/app/myproject -it starlake load
```

</TabItem>
</Tabs>

This will load the CSV file and store it as a parquet file into the following folders:
- `datasets/accepted` for valid records
- `datasets/rejected` for invalid records
- `datasets/unresolved` for unrecognized files


:::caution

Starlake validate the data against the table's schema of the first pattern that match with the file name.
Hence, you must be careful regarding the pattern you set.
Make sure that there is no overlap.

:::


### Check the result

You can check the result by running the following python script to read the parquet file from the current directory:

```python
import pandas as pd
filepath = 'datasets/accepted/sales/customers/'
df = pd.read_parquet(filepath)
df.head
```

## Configuration
### How the result went tot he filesystem as parquet files
Starlake tried to find the definition of the active environment. It searches to it in the ```SL_ENV``` variable.
This variable may be defined as:
- a value in the `metadata/env.comet.yml` file
- a value passed as an environment variable
- a java property

In our case this variable has been set as follows to the _LOCAL_ value.

```yaml {3,3} title="Default env.comet.yml configuration file. Always loaded"
env:
  root_path: "{{SL_ROOT}}"
  SL_ENV: LOCAL
  myConnectionRef: "???"
  bigQueryMaterializationDataset: "???" # need to be defined for spark loader on BigQuery only
  loader: "???" # native or spark. Need to be defined in the specific env.{{SL_ENV}}.comet.yml file
```
This means that the specific _env.LOCAL.comet.yml_ environment file will be loaded and mixed with the default one :

```yaml title="env.LOCAL.comet.yml is Loaded since SL_ENV is evaluated to LOCAL"
env:
  myConnectionRef: "localFilesystem" # use the local file system connection as defined in application.comet.yml
  loader: "spark" # spark since we are running in local mode
```

The ```connectionRef``` refers to the datawarehouse connection defined in the `application.comet.yml` file as follows:

```yaml {3,4} title="application.comet.yml"
application:
  connections:
    localFilesystem:
      type: "fs"
    bigquery:
      type: "bigquery"
      options:
        gcsBucket: "starlake-app" # Temporary GCS Bucket where intermediary files will be stored. Used by the spark loader only.
        authType: APPLICATION_DEFAULT
        #authType: SERVICE_ACCOUNT_JSON_KEYFILE
        #jsonKeyfile: "/Users/hayssams/.gcloud/keys/starlake-hayssams.json"

  # default connection
  connectionRef: "{{ myConnectionRef }}" # myConnectionRef is defined in the env.LOCAL.comet.yml & env.BQ.comet.yml files

  loader: "{{ loader }}" # native or spark depending on the env.LOCAL.comet.yml & env.BQ.comet.yml files
  spark:
    datasource:
      bigquery:
        materializationDataset: {{ bigQueryMaterializationDataset }} # need to be defined for spark loader on BigQuery only

```


### Loading the data into BigQuery
We just loaded our text file as a parquet file. This is a very common format for data scientists and analysts.
Through minimum extra configuration, we are able to run Starlake on top of any warehouse and have these datasets available as tables.

In our example, we will load the data into a BigQuery table.
define the default env as being BQ (```SL_ENV=BQ```) in the `metadata/env.comet.yml` file
and set the values specific to BigQuery in the metadata/env.BQ.comet.yml file as follows:

```yaml  title="the env.BQ.comet.yml configuration file":
env:
  myConnectionRef: "bigquery" # Use the bigquery connection defined in the application.comet.yml file
  loader: "native" # native or spark # native means we want to load the data using the BigQuery API
```

To load the file using Spark simply set the loader to spark as follows:
```yaml title="the env.BQ.comet.yml configuration file, used when SL_ENV is set to BQ":
env:
  myConnectionRef: "bigquery" # Use the bigquery connection defined in the application.comet.yml file
  bigQueryMaterializationDataset: "BQ_TEST_DS" # Spark require a dataset to store temporary tables
  loader: "spark" # Use Spark as the ingestion engine
```

Using Spark instead of the BigQuery Load API may slow down the ingestion process but it has among others the following advantages:
- It allows to load data from any source supported by Spark including Fixed Width Files, XML files, JSON Arrays files ...
- It allows to load data into any destination supported by Spark including Snowflake, Amazon Redshift ...
- It allows to apply any transformation supported by Spark
- It allows to report any number of errors instead of 5 errors max with the BigQuery Load API (This is a BigQuery API Limitation)


To load the data into BigQuery, simply put back the customer-*.psv file in the incoming folder
and run the _import_ and _load_ commands again.


