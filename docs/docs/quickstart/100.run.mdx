---
sidebar_position: 100
title: Move Data
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

In this section you will learn how to load and transform data using the samples files created when [bootstrapping a new project](bootstrap).
You will learn how to:
- import: recognize new arriving files that need to be loaded into your warehouse
- load: Validate file records and load data into you warehouse
- transform: Apply transformation oin your previously loaded data


## Sample scenario

Say we have to load the `customers` file into the warehouse.
The customers are provided by the "sales" department as delimited separated values files
and are required to be loaded incrementally.


#### customers dataset

``File customers-2018-05-10.psv from "sales" department``

|id|signup|contact|birthdate|name1|name2|
|---|---|---|---|---|---|
|A009701|2010-01-31 23:04:15|me@home.com|1980-10-14|Donald|Obama|
|B308629|2016-12-01 09:56:02|you@land.com|1980-10-14|Barack|Trump|


## Infer the schema

In Starlake terms, after loading, we will end up with:
- one domain: `sales` . A domain is equivalent to a database schema or a BigQuery Dataset.
- one table: the `customers` table in the `sales` domain 

We first need to write a YML configuration file that describe the structure of the file to load into the warehouse.
Instead of writing this file by hand, we may infer this YML configuration file using the `infer-schema` command.



<Tabs groupId="platforms">
<TabItem value="linux_macos" label="Linux/MacOS">

```sh
$ cd $HOME/myproject
$ $HOME/starlake/starlake.sh infer-schema               \
    --domain sales                                      \
    --table customers                                   \
    --input incoming/sales/customers-2018-01-01.psv     \
    --output-dir metadata/domains --with-header
```

</TabItem>
<TabItem value="windows" label="Windows">

```powershell
c:\users\me\starlake> %userprofile%\starlake\starlake.cmd infer-schema  ^
    --domain sales                                                      ^
    --table customers                                                   ^
    --input incoming/sales/customers-2018-01-01.psv                     ^
    --output-dir metadata/domains --with-header
```

</TabItem>
<TabItem value="docker" label="Docker">

```shell 
$ docker run                                                        \
    -e COMET_ROOT=/app/myproject                                    \
    -v $HOME/myproject:/app/myproject -it starlake infer-schema     \
    --domain sales                                                  \
    --table customers                                               \
    --input $HOME/myproject/incoming/sales/customers-2018-01-01.psv \
    --output-dir $HOME/myproject/metadata/domains                   \
    --with-header```
```

</TabItem>
</Tabs>

This command will infer the YML configuration file used by the loader to watch the folder for new files to load and to validate each record in those files.
This file may be found in the `sales.comet.yml` file under the `metadata/domains/` folder.

The contents of the file looks like this:

```yaml

---
load:
  name: "sales"         # The domain name
  metadata:             # All load specific rules regarding the file go here
    mode: "FILE"        # we are loading a file here
    format: "DSV"       # The format is delimiter separated values
    encoding: "UTF-8"   # The input is encoded in UTF-8
    multiline: false    # One line per record
    array: false        # Is it an array of JSON. Ignored in DSV mode
    withHeader: true    # Does the file contain a header ?
    separator: "|"      # What is the delimiter value (multi-char delimited are allowed) ?
    quote: "\""         # How fields are quoted if any
    escape: "\\"        # what is the escape char ?
    write: "APPEND"     # Should we overwrite the destination or append data in the destination table
    directory: "{{root_path}}/incoming/sales" # Where will the files land before loading ? Please set the root_path variable in metadata/env.comet.yml 
  tables:
  - name: "customers"   # destination table name
    pattern: "customers-2018-01-01.psv" #file pattern. Please replace it by a file pattern eq. customers-.*.psv
    attributes:         # Description of the fields to recognize
    - name: "id"        # attribute name and column name in the destination table if no rename attribute is defined
      type: "string"    # expected type
      array: false      # is it an array (false by default, ignored in DSV files) ? 
      required: false   # Is this field required in the source (false by default, change it accordingly) ?
      privacy: "NONE"   # Should we crypt this field before loading to the warehouse (No crypting by default )?
      ignore: false     # Should this field be excluded (false by default) ?
    - name: "signup"    # second attribute
      type: "timestamp" # recognized type by analyzing input.
      ...               # and so on ...

```

An environment file is also created. This `metadata/env.comet.yml` env file contains variables that will be substituted at runtime when present in any YAML file. Variables in YAML files are enclosed with {{ and }}.
In our example, the `root_path` must be defined to the location where the incoming directory containing the datasets to load will be located.
Set it to your project directory as follows:

```
env:
  root_path: ${HOME}/myproject
```

## Load files
Loading is a two step process: The optional `import` step and the `load` step.

### Import step
In this first step, Starlake will look at the directory defined in the YAML file and look at the file that matches the expected pattern.
In our example, the directory is `{{root_path}}/incoming/sales` and the expected file pattern is `customers-.*.psv`
The file is moved to the `datasets/pending` directory where its content will be validated before the file is loaded in the warehouse.
Any file that do not match the expected pattern is moved to the `datasets/unresolved` folder.

<Tabs groupId="platforms">
<TabItem value="linux_macos" label="Linux/MacOS">

```sh
$ cd $HOME/myproject
$ $HOME/starlake/starlake.sh import
```

</TabItem>
<TabItem value="windows" label="Windows">

```powershell
c:\users\me\starlake> %userprofile%\starlake\starlake.cmd import
```

</TabItem>
<TabItem value="docker" label="Docker">

```shell 
$ docker run                                                        \
    -e COMET_ROOT=/app/myproject                                    \
    -v $HOME/myproject:/app/myproject -it starlake import
```

</TabItem>
</Tabs>

The file has now been moved to the `myproject/datasets/pending/sales` directory.

:::note

This step is optional and does not need to be run if your files directly arrive in the `datasets/pending/sales` folder.

:::

### Load step

In this second step, each line in the file present in the `datasets/pending` folder is checked against the schema described in the YAML file.
and its result is stored in the warehouse. To load the file, simply run the watch command below:

<Tabs groupId="platforms">
<TabItem value="linux_macos" label="Linux/MacOS">

```sh
$ cd $HOME/myproject
$ $HOME/starlake/starlake.sh watch
```

</TabItem>
<TabItem value="windows" label="Windows">

```powershell
c:\users\me\starlake> %userprofile%\starlake\starlake.cmd watch
```

</TabItem>
<TabItem value="docker" label="Docker">

```shell 
$ docker run                                                        \
    -e COMET_ROOT=/app/myproject                                    \
    -v $HOME/myproject:/app/myproject -it starlake watch
```

</TabItem>
</Tabs>

This will load the dataset and store it as a parquet file into the folders:
- `datasets/accepted` for valid records
- `datasets/rejected` for invalid records
- `datasets/unresolved` for unrecognized files

Through minimum configuration, you will be able to run Starlake on top of any warehouse and have these datasets available as tables.


