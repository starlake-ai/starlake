# Load

## Load Strategy
When many files that have the same pattern and thus belong to the same schema, it is possible to ingest them one after the other using an ingestion policy
or ingest all of them at once.

When ingesting the files with the same schema one after the other, it is possible to use a custom ordering policy by settings the `SL_LOAD_STRATEGY` environment variable. Currently, the following ordering policies are defined:

* `ai.starlake.job.load.IngestionTimeStrategy`: Order the files by modification date
* `ai.starlake.job.load.IngestionNameStrategy`: Order  the files by name

If you want to use another custom strategy, you'll have to implement the trait below, make it available in the classpath and set the `SL_LOAD_STRATEGY` environment variable

````scala
package ai.starlake.job.load

import java.time.LocalDateTime

import org.apache.hadoop.fs.{FileSystem, Path}

trait LoadStrategy {

  /** List all files in folder
    *
    * @param fs        FileSystem
    * @param path      Absolute folder path
    * @param extension Files should end with this string. To list all files, simply provide an empty string
    * @param since     Minimum modification time of list files. To list all files, simply provide the beginning of all times
    * @param recursive List files recursively
    * @return List of Path
    */
  def list(
    fs: FileSystem,
    path: Path,
    extension: String = "",
    since: LocalDateTime = LocalDateTime.MIN,
    recursive: Boolean
  ): List[Path]
}
````

To ingest all the files at once, set the `SL_GROUPED` variable to true.

|YAML Variable|Env variable|Default Value|Description
|:--------------|:------------|:-------|:-----------
|grouped|SL_GROUPED|false|Should files with the same schema be ingested all at once ?
|loadStrategyClass|SL_LOAD_STRATEGY|ai.starlake.job.load.IngestionTimeStrategy|When `grouped` is false, which ingestion order strategy to use


Below is an example of YAML file with the default values.

```yaml
application:
  load-strategy-class: "ai.starlake.job.load.IngestionTimeStrategy" # may be overriden by the ${?SL_LOAD_STRATEGY} env variable
  grouped: false # may be overriden by the ${?SL_GROUPED} env variable
```

## Adaptive Load
Have you ever needed to change the way you feed your table from time to time or periodically?
Adaptive Write may be the solution to your need. This feature allows you to adjust the loading mode at runtime,
according to various criteria listed in the table below.

For example, you want to ingest in APPEND mode throughout the week,
except on Sundays when the source sends you all of certain tables,
as discrepancies may occur with the incremental mode.
This can be done automatically by changing the domain or table configuration.

The example below illustrates the change at domain level that will be propagated to all these tables.

```yaml
# _config.sl.yml
load:
   name: "DOMAIN
   metadata:
      ...
      writeStrategy:
         types:
             APPEND: 'dayOfWeek != 7'
             OVERWRITE: 'dayOfWeek == 7'
```

Another example is based on the file name:

```yaml
# _config.sl.yml
load:
   name: "DOMAIN"
   metadata:
      ...
      writeStrategy:
         types:
             OVERWRITE: 'group("mode") == "FULL"'
             APPEND: 'group("mode") == "APPEND"'

# my_table.sl.yml
table:
  ...
  pattern: ".*-(?<mode>FULL|APPEND).csv$"
```

You may want to combine criterias. If so, just use regular boolean operators with `!`, `&&`and `||` and wrap with parenthesis if necessary.

:::note
When using String in expression, makes sure to wrap them with double quotes `"`
:::

### List of valid write strategy

Strategy| Description | Requirements
:---|:---|:---
APPEND|Turn your load into APPEND mode. Ignore `merge` and `dynamicPartitionOverwrite` settings.| None
OVERWRITE|Turn your load into OVERWRITE mode. Ignore `merge` and `dynamicPartitionOverwrite` settings.| None
UPSERT_BY_KEY|Merge incoming data and target data using key. Incoming data for same key have higher precedence. Ignore `timestamp` setting on `merge`.| `merge.key` is defined
UPSERT_BY_KEY_AND_TIMESTAMP|Merge incoming data and target data using key. Select the data with higher timestamp for each key.| `merge.key` and `merge.timestamp` is defined
OVERWRITE_BY_PARTITION|Overwrite target partitions with incoming partitions. Ignore `merge` setting.| `merge.key` and `merge.timestamp` is defined


### List of criterias
Criteria| Description | Example
group(index|name) | File pattern must use (named) capture groups | pattern: `my-file-(F|D).csv$` => m.group(1) == "F"
fileSize | Current file size in bytes | fileSize > 1000
fileSizeB | Current file size in bytes. Alias of fileSize |
fileSizeKo | Current file size in Ko |
fileSizeMo | Current file size in Mo |
fileSizeGo | Current file size in Go |
fileSizeTo | Current file size in To |
isFirstDayOfMonth | Current day is first day of month|
isLastDayOfMonth | Current day is last day of month|
dayOfWeek | Integer representing day of week. Monday = 1, ..., Sunday = 7|
isFileFirstDayOfMonth | File modification date is first day of month|
isFileLastDayOfMonth | File modification date is last day of month|
fileDayOfWeek | Integer representing file modification day of week. Monday = 1, ..., Sunday = 7|

:::note
For criterias relying on datetime, you can change its timezone with `timezone` application setings in `application.sl.yml`
:::

## Validation
During ingestion, the input file is validated up to the attribute level. Three default row validators are defined:

- ai.starlake.job.validator.FlatRowValidator: to validate flat files, eq. DSV, Fixed width and single level Json files.
- ai.starlake.job.validator.TreeRowValidator:  used for tree like documents, eq. XML and JSON files
- ai.starlake.job.validator.AcceptAllValidator: used for any document type (flat and tree like) and accept the input without any validation

The validtor to use is configurable as follows:

YAML Variable|Env. variable|Default value
:---|:---|:---
rowValidatorClass|SL_ROW_VALIDATOR_CLASS|ai.starlake.job.validator.FlatRowValidator
treeValidatorClass|SL_TREE_VALIDATOR_CLASS|ai.starlake.job.validator.TreeRowValidator

## Privacy
Default valid values are NONE, HIDE, MD5, SHA1, SHA256, SHA512, AES(not implemented).
Custom values may also be defined by adding a new privacy option in the application.conf.
The default reference.conf file defines the following valid privacy strategies:
```hocon
privacy {
  options = {
    "none": "ai.starlake.privacy.No",
    "hide": "ai.starlake.privacy.Hide",
    "hide10X": "ai.starlake.privacy.Hide(\"X\",10)",
    "approxLong20": "ai.starlake.privacy.ApproxLong(20)",
    "md5": "ai.starlake.privacy.Md5",
    "sha1": "ai.starlake.privacy.Sha1",
    "sha256": "ai.starlake.privacy.Sha256",
    "sha512": "ai.starlake.privacy.Sha512",
    "initials": "ai.starlake.privacy.Initials"
  }
}
```
In the YAML/HOCON file, reference, you reference the option name. This will apply the function defined in the class referenced by the option value.

Below the predefined strategies:

Privacy Strategy|Privacy class|Description
:---|:---|:---
none|ai.starlake.privacy.No|Return the input string itself
hide|ai.starlake.privacy.Hide(\"X\", 10)|Without a parameter, return the empty string. Otherwise, replace with 10 occurrences of the character 'X'
md5|ai.starlake.privacy.Md5|Return the md5 of the input string
sha1|ai.starlake.privacy.Sha1|Return the sha1 of the input string
sha256|ai.starlake.privacy.Sha256|Return the sha256 of the input string
sha512|ai.starlake.privacy.Sha512|Return the sha256 of the input string
initials|ai.starlake.privacy.Initials|Return the first char of each word (usually applied to user names)

The following startegies are also defined and may be declared in the custom configuration file.

Privacy class|Description
:---|:---
ai.starlake.privacy.IPv4(8)|Return the IPv4 address with the last 8 bytes masked
ai.starlake.privacy.IPv6(8|Return the IPv6 address with the last 8 bytes masked
ai.starlake.privacy.RandomDouble|Return a random double number
ai.starlake.privacy.RandomDouble(10,20)|Return a random double between 10.0 and 20.0
ai.starlake.privacy.RandomLong|Return a random long number
ai.starlake.privacy.RandomLong(10, 20)|Return a random long number between 10 and 20
ai.starlake.privacy.RandomInt|Return a random int number
ai.starlake.privacy.RandomInt(10, 20)|Return a random int number between 10 and 20
ai.starlake.privacy.ApproxDouble(70)|Return a double value with a variation up to 70% applied to the input value
ai.starlake.privacy.ApproxLong(70)|Return a double long with a variation up to 70% applied to the input value
ai.starlake.privacy.Mask(\"*\", 4, 1, 3)| Partially mask the input value with 4 occurrences of the '*' character, 1 on the left side and 3 on the right side.


Any new privacy strategy should implement the following trait:

```scala
/** @param s: String  => Input string to encrypt
  * @param colMap: Map[String, Option[String]] => Map of all the attributes and their corresponding values
  * @param params: List[Any]  => Parameters passed to the algorithm as defined in the conf file.
  *                               Parameter starting with '"' is converted to a string
  *                               Parameter containing a '.' is converted to a double
  *                               Parameter equals to true of false is converted a boolean
  *                               Anything else is converted to an int
  * @return The encrypted string
  */
```
