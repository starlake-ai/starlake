include required("reference.conf")

# NOTE: this file should contain deviations from the standard ("production default") reference.conf for use
# in the Test context
# Typelevel Config loads application-test.conf first and if it doesn't find it, reference.conf
# Our own TypeHelper will look for application-test.conf first.


archive = true

fileSystem = "file://"

sinkToFile=true

hiveInTest = true

grouped = true

analyze = false

expectations.active = true

connections {
  # audit {
  #   sparkFormat = "jdbc"
  #   options {
  #     url: "jdbc:postgresql://127.0.0.1:5403/starlakedb?user=postgres&password=ficpug-Podbid-7fobnu",
  #     user: "postgres",
  #     password: "ficpug-Podbid-7fobnu",
  #     driver: "org.postgresql.Driver"
  #     # driver = "org.h2.Driver"
  #   }
  # }
  bigquery {
    sparkFormat = "bigquery"
    type = "bigquery"
    options {
      authType: APPLICATION_DEFAULT #SERVICE_ACCOUNT_JSON_KEYFILE
      gcsBucket: starlake-app
      #jsonKeyfile: "/Users/me/.gcloud/keys/me.json"
    }
  }
  BQ { # Used for XLSReader
    type = "bigquery"
    options {
      authType: APPLICATION_DEFAULT #SERVICE_ACCOUNT_JSON_KEYFILE
      location: "europe-west1" # EU or US or ..
      gcsBucket: starlake-app
      #jsonKeyfile: "/Users/me/.gcloud/keys/me.json"
    }
  }
  spark {
    sparkFormat = "parquet"
    type = "fs"
    options {
      #gcsBucket: starlake-app
      #authType: SERVICE_ACCOUNT_JSON_KEYFILE
      #jsonKeyfile: "/Users/me/.gcloud/keys/me.json"
    }
  }
}

connectionRef = spark

audit {
  sink {
    connectionRef = "audit"
    # bq-dataset = "audit"
  }
}

metrics {
  active = true
}

spark {
  debug.maxToStringFields=100
  sql.parser.escapedStringLiterals=true
}


