{
  "definitions": {
    "AreaV1": {
      "type": "object",
      "properties": {
        "incoming": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Files are read from  this folder for ingestion by the \"import\" command."
        },
        "stage": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Files recognized by the extensions property are moved to this folder for ingestion by the \"import\" command."
        },
        "unresolved": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Files that cannot be ingested (do not match by any table pattern) are moved to this folder."
        },
        "archive": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Files that have been ingested are moved to this folder if SL_ARCHIVE is set to true."
        },
        "ingesting": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Files that are being ingested are moved to this folder."
        },
        "replay": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Invalid records are stored in this folder in source format when SL_SINK_REPLAY_TO_FILE is set to true."
        },
        "hiveDatabase": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Hive database name to use when running on Spark with Hive support enabled"
        }
      }
    },
    "AuditV1": {
      "type": "object",
      "properties": {
        "path": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Path where audit logs are stored when using filesystem storage"
        },
        "sink": {
          "$ref": "#/definitions/AllSinksV1",
          "description": "Sink configuration for audit logs storage"
        },
        "maxErrors": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Maximum number of errors to tolerate before failing the job. Default is unlimited"
        },
        "database": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Database name where audit tables are stored (project ID in BigQuery)"
        },
        "domain": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Domain/dataset name for audit tables. Default is 'audit'"
        },
        "domainExpectation": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Domain/dataset name for expectation results. Default is 'expectations'"
        },
        "domainRejected": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Domain/dataset name for rejected records. Default is 'rejected'"
        },
        "detailedLoadAudit": {
          "type": "boolean",
          "description": "Create individual entry for each ingested file instead of a global one. Default: false"
        },
        "active": {
          "type": "boolean",
          "description": "Enable or disable audit logging. Default is true"
        },
        "sql": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Custom SQL query to use for audit table creation or data insertion"
        }
      },
      "required": []
    },
    "MetricsV1": {
      "type": "object",
      "properties": {
        "path": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "When using filesystem storage, the path to the metrics file"
        },
        "discreteMaxCardinality": {
          "description": "Max number of unique values accepted for a discrete column. Default is 10",
          "type": "integer"
        },
        "active": {
          "description": "Should metrics be computed ?",
          "type": "boolean"
        }
      }
    },
    "RefV1": {
      "description": "Describe how to resolve a reference in a transform task",
      "type": "object",
      "properties": {
        "input": {
          "$ref": "#/definitions/InputRefV1",
          "description": "The input table to resolve"
        },
        "output": {
          "$ref": "#/definitions/OutputRefV1",
          "description": "The output table resolved with the domain and database"
        }
      },
      "required": [
        "input",
        "output"
      ]
    },
    "HttpV1": {
      "type": "object",
      "properties": {
        "interface": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Network interface to bind the HTTP server to. Default is '0.0.0.0' (all interfaces)"
        },
        "port": {
          "type": "integer",
          "description": "Port number for the HTTP server. Default is 8080"
        }
      }
    },
    "InternalV1": {
      "type": "object",
      "description": "configure Spark internal options",
      "properties": {
        "cacheStorageLevel": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "How the dataframe are cached. Default is MEMORY_AND_DISK_SER.\nAvailable options are (https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/storage/StorageLevel.html):\n- MEMORY_ONLY\n- MEMORY_AND_DISK\n- MEMORY_ONLY_SER\n- MEMORY_AND_DISK_SER\n- DISK_ONLY\n- OFF_HEAP"
        },
        "intermediateBigqueryFormat": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "May be parquet or ORC. Default is parquet. Used for BigQuery intermediate storage. Use ORC for for JSON files to keep the original data structure.\nhttps://stackoverflow.com/questions/53674838/spark-writing-parquet-arraystring-converts-to-a-different-datatype-when-loadin"
        },
        "temporaryGcsBucket": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "The GCS bucket that temporarily holds the data before it is loaded to BigQuery."
        },
        "substituteVars": {
          "description": "Internal use. Do not modify.",
          "type": "boolean"
        },
        "bqAuditSaveInBatchMode": {
          "description": "Should audit logs when using BigQuery be saved in batch or interactive mode ? Interactive by default (false)",
          "type": "boolean"
        }
      }
    },
    "AccessPoliciesV1": {
      "type": "object",
      "properties": {
        "apply": {
          "description": "Should access policies be enforced ?",
          "type": "boolean"
        },
        "location": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "GCP project location. Required if apply is true."
        },
        "database": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "GCP Project id. Required if apply is true."
        },
        "taxonomy": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Taxonomy name. Required if apply is true."
        }
      }
    },
    "MapConnectionV1": {
      "type": "object",
      "description": "Map of jdbc engines",
      "additionalProperties": {
        "$ref": "#/definitions/ConnectionV1"
      }
    },
    "ConvertibleToString": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "boolean"
        },
        {
          "type": "number"
        },
        {
          "type": "integer"
        },
        {
          "type": "null"
        }
      ]
    },
    "PrivacyV1": {
      "type": "object",
      "properties": {
        "options": {
          "$ref": "#/definitions/MapString",
          "description": "Privacy strategies. The following default strategies are defined by default:\n- none: Leave the data as is\n- hide: replace the data with an empty string\n- hideX(\"s\", n): replace the string with n occurrences of the string 's'\n- md5: Redact the data using the MD5 algorithm\n- sha1: Redact the data using the SHA1 algorithm\n- sha256: Redact the data using the SHA256 algorithm\n - sha512: Redact the data using the SHA512 algorithm\n- initials: keep only the first char of each word in the data"
        }
      }
    },
    "SparkSchedulingV1": {
      "type": "object",
      "properties": {
        "maxJobs": {
          "description": "Max number of Spark jobs to run in parallel, default is 1",
          "type": "integer"
        },
        "poolName": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Pool name to use for Spark jobs, default is 'default'"
        },
        "mode": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "This can be FIFO or FAIR, to control whether jobs within the pool queue up behind each other (the default) or share the pool\u2019s resources fairly."
        },
        "file": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Scheduler filename in the metadata folder. If not set, defaults to fairscheduler.xml."
        }
      }
    },
    "ExpectationsConfigV1": {
      "type": "object",
      "properties": {
        "path": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "When using filesystem storage, the path to the expectations file"
        },
        "active": {
          "description": "should expectations be executed ?",
          "type": "boolean"
        },
        "failOnError": {
          "description": "should load / transform fail on expectation error ?",
          "type": "boolean"
        }
      }
    },
    "MapString": {
      "type": "object",
      "description": "Map of string",
      "additionalProperties": {
        "$ref": "#/definitions/ConvertibleToString"
      }
    },
    "KafkaConfigV1": {
      "type": "object",
      "properties": {
        "serverOptions": {
          "$ref": "#/definitions/MapString",
          "description": "Kafka server connection options (e.g., bootstrap.servers, security.protocol)"
        },
        "topics": {
          "type": "object",
          "description": "Map of topic name to topic configuration",
          "additionalProperties": {
            "$ref": "#/definitions/KafkaTopicConfigV1"
          }
        },
        "cometOffsetsMode": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Offset management mode: 'STREAM' for Spark streaming checkpoints or 'FILE' for file-based offset tracking"
        },
        "customDeserializers": {
          "$ref": "#/definitions/MapString",
          "description": "Map of custom deserializer class names for specific data formats (e.g., Avro, Protobuf)"
        }
      }
    },
    "LockV1": {
      "type": "object",
      "properties": {
        "path": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Name of the lock"
        },
        "timeout": {
          "type": "integer",
          "description": "reserved"
        },
        "pollTime": {
          "type": "integer",
          "description": "Default 5 seconds"
        },
        "refreshTime": {
          "type": "integer",
          "description": "Default 5 seconds"
        }
      }
    },
    "DagRefV1": {
      "type": "object",
      "properties": {
        "load": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Dag config to use for load tasks. May be redefined at the table level"
        },
        "transform": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Dag config to use for transform tasks. May be redefined at the task level"
        }
      }
    },
    "MapJdbcEngineV1": {
      "type": "object",
      "description": "Map of jdbc engines",
      "additionalProperties": {
        "$ref": "#/definitions/JdbcEngineV1"
      }
    },
    "AllSinksV1": {
      "type": "object",
      "properties": {
        "connectionRef": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "JDBC: Connection String"
        },
        "clustering": {
          "description": "FS or BQ: List of attributes to use for clustering",
          "type": "array",
          "items": {
            "$ref": "#/definitions/ConvertibleToString"
          }
        },
        "days": {
          "type": "number",
          "description": "BQ: Number of days before this table is set as expired and deleted. Never by default."
        },
        "requirePartitionFilter": {
          "type": "boolean",
          "description": "BQ: Should be require a partition filter on every request ? No by default."
        },
        "materializedView": {
          "$ref": "#/definitions/Materialization",
          "description": "Should we materialize as a table or as a view when saving the results ? TABLE by default."
        },
        "enableRefresh": {
          "type": "boolean",
          "description": "BQ: Enable automatic refresh of materialized view ? false by default."
        },
        "refreshIntervalMs": {
          "type": "number",
          "description": "BQ: Refresh interval in milliseconds. Default to BigQuery default value"
        },
        "id": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "ES: Attribute to use as id of the document. Generated by Elasticsearch if not specified."
        },
        "format": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "FS: File format"
        },
        "extension": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "FS: File extension"
        },
        "sharding": {
          "description": "columns to use for sharding. table will be named table_{sharding(0)}_{sharding(1)}",
          "type": "array",
          "items": {
            "$ref": "#/definitions/ConvertibleToString"
          }
        },
        "partition": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "FS or BQ: List of partition attributes"
        },
        "coalesce": {
          "type": "boolean",
          "description": "When outputting files, should we coalesce it to a single file. Useful when CSV is the output format."
        },
        "path": {
          "type": "string",
          "description": "Optional path attribute if you want to save the file outside of the default location (datasets folder)"
        },
        "options": {
          "$ref": "#/definitions/MapString",
          "description": "Additional Spark writer options (e.g., compression, partitionOverwriteMode)"
        }
      }
    },
    "OutputRefV1": {
      "description": "Output for ref object",
      "type": "object",
      "properties": {
        "database": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Target database name for the resolved reference"
        },
        "domain": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Target domain/dataset name for the resolved reference"
        },
        "table": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Target table name for the resolved reference"
        }
      },
      "required": [
        "table",
        "domain",
        "database"
      ]
    },
    "InputRefV1": {
      "description": "Input for ref object",
      "type": "object",
      "properties": {
        "database": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Database pattern to match, none if any database"
        },
        "domain": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Domain pattern to match, none if any domain match"
        },
        "table": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Table pattern to match"
        }
      },
      "required": [
        "table"
      ]
    },
    "ConnectionV1": {
      "description": "Connection properties to a datawarehouse.",
      "type": "object",
      "properties": {
        "type": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Connection type: jdbc, bigquery, snowflake, redshift, databricks, duckdb, or any Spark-supported format"
        },
        "sparkFormat": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Spark data source format to use (e.g., 'jdbc', 'bigquery', 'parquet'). Required when using Spark engine"
        },
        "loader": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Loader we should use with this connection. Superseded by the loader defined in the YAML table metadata"
        },
        "quote": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Character to use when quoting column and table names"
        },
        "separator": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Catalog/schema separator character used in fully qualified table names. Default is '.'"
        },
        "options": {
          "$ref": "#/definitions/MapString",
          "description": "Connection options"
        }
      },
      "required": [
        "type"
      ]
    },
    "KafkaTopicConfigV1": {
      "properties": {
        "topicName": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Kafka topic name to consume from or produce to"
        },
        "maxRead": {
          "type": "integer",
          "description": "Maximum number of records to read from the topic in a single batch. Default is unlimited"
        },
        "fields": {
          "type": "array",
          "items": {
            "$ref": "#/definitions/ConvertibleToString"
          },
          "description": "List of fields to extract from Kafka messages"
        },
        "partitions": {
          "type": "integer",
          "description": "Number of partitions for the Kafka topic when creating it"
        },
        "replicationFactor": {
          "type": "integer",
          "description": "Replication factor for the Kafka topic when creating it"
        },
        "createOptions": {
          "$ref": "#/definitions/MapString",
          "description": "Additional options passed when creating the Kafka topic"
        },
        "accessOptions": {
          "$ref": "#/definitions/MapString",
          "description": "Kafka consumer/producer configuration options (e.g., security settings, serializers)"
        },
        "headers": {
          "type": "object",
          "additionalProperties": {
            "$ref": "#/definitions/MapString"
          },
          "description": "HTTP headers to include when accessing Kafka via HTTP proxy"
        }
      }
    },
    "JdbcEngineV1": {
      "type": "object",
      "description": "Jdbc engine",
      "properties": {
        "tables": {
          "$ref": "#/definitions/MapTableDdlV1",
          "description": "List of all SQL create statements used to create audit tables for this JDBC engine.\nTables are created only if the execution of the pingSQL statement fails"
        },
        "quote": {
          "type": "string",
          "description": "How to quote identifiers"
        },
        "viewPrefix": {
          "type": "string",
          "description": "When creating views, how they should be prefixed. Some databases like redshift require view name to be prefixed by the character '#'.\nThis is not required for other databases like snowflake or bigquery.\nDefault is empty string"
        },
        "preActions": {
          "type": "string",
          "description": "SQL statements to execute immediately after the database connection is opened (e.g., SET commands)"
        },
        "partitionBy": {
          "type": "string",
          "description": "keyword used to partition the table. Default is PARTITION BY"
        },
        "clusterBy": {
          "type": "string",
          "description": "keyword used to cluster the table. Default is CLUSTER BY"
        },
        "strategyBuilder": {
          "type": "string",
          "description": "Override the default strategy builder used to write data. A strategy is a folder located under metadata/templates/write-strategies/[strategyBuilder]"
        },
        "columnRemarks": {
          "type": "string",
          "description": "How to get column remarks"
        },
        "tableRemarks": {
          "type": "string",
          "description": "How to get table remarks"
        }
      },
      "required": [
        "tables",
        "quote",
        "strategyBuilder"
      ]
    },
    "Materialization": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Table types supported by the Sink option",
      "oneOf": [
        {
          "const": "TABLE",
          "description": "SQL Table"
        },
        {
          "const": "VIEW",
          "description": "SQL View"
        },
        {
          "const": "MATERIALIZED_VIEW",
          "description": "SQL Materialized View"
        },
        {
          "const": "HYBRID",
          "description": "Snowflake OLTP tables"
        }
      ]
    },
    "MapTableDdlV1": {
      "type": "object",
      "description": "Map of table ddl",
      "additionalProperties": {
        "$ref": "#/definitions/TableDdlV1"
      }
    },
    "TableDdlV1": {
      "description": "DDL used to create a table",
      "type": "object",
      "properties": {
        "createSql": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "SQL CREATE DDL statement"
        },
        "pingSql": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "How to test if the table exist.\nUse the following statement by default: 'select count(*) from tableName where 1=0'"
        },
        "selectSql": {
          "$ref": "#/definitions/ConvertibleToString",
          "description": "Override the default select defined by Starlake"
        }
      },
      "required": [
        "createSql"
      ]
    }
  },
  "properties": {
    "env": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Default environment to use. May be also set using the SL_ENV environment variable"
    },
    "datasets": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "When using filesystem storage, default path to store the datasets"
    },
    "incoming": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Incoming folder to use during autoload"
    },
    "dags": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "DAG generation config folder. metadata/dags by default"
    },
    "types": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "types config folder. metadata/types by default"
    },
    "macros": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Macros config folder. metadata/macros by default"
    },
    "tests": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Path to tests folder. Default is ${metadata}/tests"
    },
    "prunePartitionOnMerge": {
      "type": "boolean",
      "description": "Pre-compute incoming partitions to prune partitions on merge statement"
    },
    "writeStrategies": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Location where are located user defined write strategies; Default is ${metadata}/write-strategies"
    },
    "loadStrategies": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Location where are located user defined load strategies; Default is ${metadata}/load-strategies"
    },
    "metadata": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "default metadata folder name. May be also set using the SL_METADATA environment variable"
    },
    "metrics": {
      "$ref": "#/definitions/MetricsV1"
    },
    "validateOnLoad": {
      "type": "boolean",
      "description": "Validate the YAML file when loading it. If set to true fails on any error"
    },
    "rejectWithValue": {
      "type": "boolean",
      "description": "Add value along with the rejection error. Not enabled by default for security reason. Default: false"
    },
    "audit": {
      "$ref": "#/definitions/AuditV1"
    },
    "archive": {
      "type": "boolean",
      "description": "Should ingested files be archived after ingestion ?"
    },
    "sinkReplayToFile": {
      "type": "boolean",
      "description": "Should invalid records be stored in a replay file ?"
    },
    "lock": {
      "$ref": "#/definitions/LockV1"
    },
    "defaultWriteFormat": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Default write format in Spark. parquet is the default"
    },
    "defaultRejectedWriteFormat": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Default write format in Spark for rejected records. parquet is the default"
    },
    "defaultAuditWriteFormat": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Default write format in Spark for audit records. parquet is the default"
    },
    "csvOutput": {
      "type": "boolean",
      "description": "output files in CSV format ? Default is false"
    },
    "csvOutputExt": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "CSV file extension when csvOutput is true. Default is .csv"
    },
    "privacyOnly": {
      "type": "boolean",
      "description": "Only generate privacy tasks. Reserved for internal use"
    },
    "emptyIsNull": {
      "type": "boolean",
      "description": "Should empty strings be considered as null values ?"
    },
    "loader": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Default loader to use when none is specified in the schema. Valid values are 'spark' or 'native'. Default is 'spark'"
    },
    "rowValidatorClass": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Custom row validator class for advanced validation logic. Must implement RowValidator interface"
    },
    "loadStrategyClass": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "In what order should the files for a same table be loaded ? By time (default) or by or name ?\n",
      "anyOf": [
        {
          "const": "ai.starlake.job.load.IngestionNameStrategy",
          "description": "Order pending files by name"
        },
        {
          "const": "ai.starlake.job.load.IngestionTimeStrategy",
          "title": "Order pending files by creation date time"
        }
      ]
    },
    "grouped": {
      "type": "boolean",
      "description": "Should we load of the files to be stored in the same table in a single task or one by one ?"
    },
    "groupedMax": {
      "type": "integer",
      "description": "Maximum number of files to be stored in the same table in a single task"
    },
    "scd2StartTimestamp": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Column name to use for SCD2 start timestamp.@"
    },
    "scd2EndTimestamp": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Column name to use for SCD2 end timestamp."
    },
    "area": {
      "$ref": "#/definitions/AreaV1",
      "description": "stage, ingesting ... areas configuration"
    },
    "hadoop": {
      "$ref": "#/definitions/MapString",
      "description": "Hadoop configuration if applicable"
    },
    "connections": {
      "$ref": "#/definitions/MapConnectionV1",
      "description": "Connections configurations"
    },
    "jdbcEngines": {
      "$ref": "#/definitions/MapJdbcEngineV1",
      "description": "JDBC engine configurations"
    },
    "privacy": {
      "$ref": "#/definitions/PrivacyV1",
      "description": "Privacy algorithms"
    },
    "root": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Root folder for the application. May be also set using the SL_ROOT environment variable"
    },
    "internal": {
      "$ref": "#/definitions/InternalV1",
      "description": "Internal configuration"
    },
    "accessPolicies": {
      "$ref": "#/definitions/AccessPoliciesV1",
      "description": "Access policies configuration"
    },
    "sparkScheduling": {
      "$ref": "#/definitions/SparkSchedulingV1",
      "description": "Spark Job scheduling configuration"
    },
    "udfs": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Coma separated list of UDF to register in Spark jobs. May be also set using the SL_UDFS environment variable"
    },
    "expectations": {
      "$ref": "#/definitions/ExpectationsConfigV1",
      "description": "Expectations configuration"
    },
    "sqlParameterPattern": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Pattern to use to replace parameters in SQL queries in addition to the jinja syntax {{param}}. Default is ${param}"
    },
    "rejectAllOnError": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Should we reject all records when an error occurs ? Default is false"
    },
    "rejectMaxRecords": {
      "type": "integer",
      "description": "Maximum number of records to reject when an error occurs. Default is 100"
    },
    "maxParCopy": {
      "type": "integer",
      "description": "Maximum number of parallel file copy operations during import. Default is 1"
    },
    "kafka": {
      "$ref": "#/definitions/KafkaConfigV1",
      "description": "Kafka configuration for streaming ingestion and message processing"
    },
    "dsvOptions": {
      "$ref": "#/definitions/MapString",
      "description": "DSV ingestion extra options"
    },
    "forceViewPattern": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "reserved"
    },
    "forceDomainPattern": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "reserved"
    },
    "forceTablePattern": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "reserved"
    },
    "forceJobPattern": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "reserved"
    },
    "forceTaskPattern": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "reserved"
    },
    "useLocalFileSystem": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "reserved"
    },
    "sessionDurationServe": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "reserved"
    },
    "database": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Default target database (projectId in GCP). May be also set using the SL_DATABASE environment variable"
    },
    "tenant": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "reserved"
    },
    "connectionRef": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Default connection to use when loading / transforming data"
    },
    "loadConnectionRef": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Default connection to use when loading / transforming data"
    },
    "transformConnectionRef": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Default connection to use when loading / transforming data"
    },
    "schedulePresets": {
      "$ref": "#/definitions/MapString",
      "description": "Map of schedule preset names to cron expressions for reusable scheduling patterns"
    },
    "maxParTask": {
      "type": "integer",
      "description": "How many job to run simultaneously in dev mode (experimental)"
    },
    "refs": {
      "type": "array",
      "description": "Reference mappings for resolving table references in SQL queries across different environments",
      "items": {
        "$ref": "#/definitions/RefV1"
      }
    },
    "dagRef": {
      "$ref": "#/definitions/DagRefV1",
      "description": "Default DAG configuration references for load and transform tasks"
    },
    "forceHalt": {
      "type": "boolean",
      "description": "Force application to stop even when there is some pending thread."
    },
    "jobIdEnvName": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Environment variable name containing the job ID for tracking purposes"
    },
    "archiveTablePattern": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Pattern for naming archive tables. Use {table} as placeholder for the original table name"
    },
    "archiveTable": {
      "type": "boolean",
      "description": "Enable table archiving before overwrite operations. Default is false"
    },
    "version": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Application configuration version for compatibility tracking"
    },
    "autoExportSchema": {
      "type": "boolean",
      "description": "Automatically export table schemas after load/transform operations. Default is false"
    },
    "longJobTimeoutMs": {
      "type": "integer",
      "description": "Timeout in milliseconds for long-running jobs. Default is 3600000 (1 hour)"
    },
    "shortJobTimeoutMs": {
      "type": "integer",
      "description": "Timeout in milliseconds for short-running jobs. Default is 300000 (5 minutes)"
    },
    "createSchemaIfNotExists": {
      "type": "boolean",
      "description": "Automatically create database schema/dataset if it does not exist. Default is true"
    },
    "http": {
      "$ref": "#/definitions/HttpV1",
      "description": "HTTP server configuration for the Starlake REST API"
    },
    "timezone": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Default timezone for date/time operations. Default is UTC"
    },
    "maxInteractiveRecords": {
      "type": "integer",
      "description": "Maximum number of records to return in interactive query mode. Default is 1000"
    },
    "duckdbMode": {
      "type": "boolean",
      "description": "is duckdb mode active"
    },
    "duckdbExtensions": {
      "type": "string",
      "description": "Comma separated list of duckdb extensions to load. Default is spatial, json, httpfs"
    },
    "duckdbPath": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Where to store duckdb files if not using default"
    },
    "testCsvNullString": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "null string value in tests"
    },
    "hiveInTest": {
      "$ref": "#/definitions/ConvertibleToString",
      "description": "Internal use only"
    },
    "spark": {
      "type": "object",
      "description": "Map of string",
      "additionalProperties": true
    },
    "extra": {
      "type": "object",
      "description": "Map of string",
      "additionalProperties": true
    },
    "duckDbEnableExternalAccess": {
      "description": "Allow DuckDB to load / Save data from / to external sources. Default to true",
      "type": "boolean"
    },
    "syncSqlWithYaml": {
      "description": "Update attributes in YAMl file when SQL is updated. Default to true",
      "type": "boolean"
    },
    "syncYamlWithDb": {
      "description": "Update database with YAML transform is run. Default to true",
      "type": "boolean"
    },
    "onExceptionRetries": {
      "description": "Number of retries on transient exceptions",
      "type": "integer"
    }
  },
  "required": []
}