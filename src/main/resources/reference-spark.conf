spark {
  #  sql.hive.convertMetastoreParquet = false
  #extraListeners = com.hortonworks.spark.atlas.SparkAtlasEventTracker
  #sql.queryExecutionListeners = com.hortonworks.spark.atlas.SparkAtlasEventTracker
  #sql.streaming.streamingQueryListeners=com.hortonworks.spark.atlas.SparkAtlasStreamingQueryEventTracker
  #  yarn.principal = "invalid"
  #  yarn.keytab = "invalid"
  #  yarn.principal = ${?SPARK_YARN_PRINCIPAL}
  #  yarn.keytab = ${?SPARK_YARN_KEYTAB}
  debug.maxToStringFields = 100
  master = "local[*]"
  #  sql.catalogImplementation="hive"
  #  sql.catalogImplementation="in-memory"
  sql.legacy.parquet.datetimeRebaseModeInWrite = "CORRECTED"
  sql.streaming.checkpointLocation = "/tmp"
  sql.sources.partitionOverwriteMode = "static"
  sql.sources.partitionOverwriteMode = ${?COMET_SPARK_SQL_SOURCES_PARTITION_OVERWRITE_MODE}
  datasource.bigquery {
    viewsEnabled = "true"
    materializationProject = ${?COMET_SPARK_BIGQUERY_MATERIALIZATION_PROJECT}
    materializationDataset = "materialization"
    materializationDataset = ${?COMET_SPARK_BIGQUERY_MATERIALIZATION_DATASET}
    # materializationExpirationTimeInMinutes (DEFAULT_MATERIALIZATION_EXPRIRATION_TIME_IN_MINUTES = 24 * 60)
    readDataFormat = "AVRO"
    readDataFormat = ${?COMET_SPARK_BIGQUERY_READ_DATA_FORMAT}
  }
}

