# This template executes individual bash jobs and requires the following dag generation options set:
#
# - sl_env_var: starlake variables specified as a map in json format - at least the root project path SL_ROOT should be specified [OPTIONAL]
# - SL_STARLAKE_PATH(starlake): the path to the starlake executable [OPTIONAL]
# - run_dependencies_first(False): whereas the dependencies should be added and executed before each transformation that has to be performed within the dag (if not set, the dependencies will not be added) [OPTIONAL]
# - tags: a list of tags to be applied to the dag [OPTIONAL]
# - catchup(False): whether to catch up the missed runs or not [OPTIONAL]
# - start_date: the start date of the dag (eg. 2022-01-01) [OPTIONAL]
# - end_date: the end date of the dag (eg. 2024-12-31) [OPTIONAL]
# - retries(1): the number of retries to attempt before failing the task [OPTIONAL]
# - retry_delay(300): the delay between retries in seconds [OPTIONAL]
# - default_dag_args: the default dag arguments specified as a map in json format [OPTIONAL]
# - data_cycle_enabled(False): whether the data cycle feature is enabled or not [OPTIONAL]
# - data_cycle: the data cycle to be used for the dag [OPTIONAL]
# - beyond_data_cycle_enabled(True): whether the beyond data cycle feature is enabled or not [OPTIONAL]
# - optional_dataset_enabled(False): whether a dataset can be treated as optional [OPTIONAL]
# - dataset_triggering_strategy(any): the dataset triggering strategy to be used for the non scheduled dag, one of any or all (if not set the default 'any' strategy will be used) [OPTIONAL]
# - min_timedelta_between_runs(900): the minimum timedelta between runs in seconds (900 by default) to avoid multiple dag executions when the strategy for dataset triggering is 'any' [OPTIONAL]
#
{% include 'templates/dags/__starlake_airflow_orchestrator.py' %}
{% include 'templates/dags/__starlake_shell_execution.py' %}
{% include 'templates/dags/transform/__scheduled_task_tpl.py.j2' %}
