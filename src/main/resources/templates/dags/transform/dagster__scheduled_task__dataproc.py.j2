# This template executes individual cloud run jobs and requires the following dag generation options set:
# - dataproc_name: the name of the dataproc cluster [OPTIONAL]
# - dataproc_project_id: the project id of the dataproc cluster (if not set, the project id of the composer environment will be used) [OPTIONAL]
# - dataproc_region(europe-west1): the region of the dataproc cluster (if not set, europe-west1 will be used) [OPTIONAL]
# - dataproc_subnet(default): the subnetwork of the dataproc cluster (if not set, the default subnetwork will be used) [OPTIONAL]
# - dataproc_service_account: the service account of the dataproc cluster (if not set, the default service account will be used) [OPTIONAL]
# - dataproc_image_version(2.2-debian12): the image version of the dataproc cluster (if not set, 2.2-debian12 will be used) [OPTIONAL]
# - dataproc_master_machine_type(n1-standard-4): the master machine type of the dataproc cluster (if not set, n1-standard-4 will be used) [OPTIONAL]
# - dataproc_master_disk_size(1024): the master disk size of the dataproc cluster (if not set, 1024 will be used) [OPTIONAL]
# - dataproc_master_disk_type(pd-standard): the master disk type of the dataproc cluster (if not set, pd-standard will be used) [OPTIONAL]
# - dataproc_worker_machine_type(n1-standard-4): the worker machine type of the dataproc cluster (if not set, n1-standard-4 will be used) [OPTIONAL]
# - dataproc_worker_disk_size(1024): the worker disk size of the dataproc cluster (if not set, 1024 will be used) [OPTIONAL]
# - dataproc_worker_disk_type(pd-standard): the worker disk type of the dataproc cluster (if not set, pd-standard will be used) [OPTIONAL]
# - dataproc_num_workers(4): the number of workers of the dataproc cluster (if not set, 4 will be used) [OPTIONAL]
# - dataproc_cluster_metadata: the metadata to add to the dataproc cluster specified as a map in json format [OPTIONAL]
# - spark_jar_list: the list of spark jars to be used [REQUIRED]
# - spark_bucket: the bucket to use for spark and biqquery temporary storage [REQUIRED]
# - spark_job_main_class(ai.starlake.job.Main): the main class of the spark job (if not set, the main class ai.starlake.job.Main will be used) [OPTIONAL]
# - spark_executor_memory(11g): the amount of memory to use per executor process (if not set, 11g will be used) [OPTIONAL]
# - spark_executor_cores(4): the number of cores to use on each executor (if not set, 4 will be used) [OPTIONAL]
# - spark_executor_instances(1): the number of executor instances (if not set, 1 will be used) [OPTIONAL]
# - sl_env_var: starlake variables specified as a map in json format - at least the root project path SL_ROOT should be specified [OPTIONAL]
# - load_dependencies(False): whereas the dependencies should be added for each transformation that has to be performed within the dag (if not set, the dependencies are not added) [OPTIONAL]
# - tags: a list of tags to be applied to the dag [OPTIONAL]
# - retries(1): the number of retries to attempt before failing the task [OPTIONAL]
# - retry_delay(300): the delay between retries in seconds [OPTIONAL]
# - data_cycle_enabled(False): whether the data cycle feature is enabled or not [OPTIONAL]
# - data_cycle: the data cycle to be used for the dag [OPTIONAL]
# - beyond_data_cycle_enabled(True): whether the beyond data cycle feature is enabled or not [OPTIONAL]
# - optional_dataset_enabled(False): whether a dataset can be treated as optional [OPTIONAL]
# - dataset_triggering_strategy(any): the dataset triggering strategy to be used for the non scheduled dag, one of any or all (if not set the default 'any' strategy will be used) [OPTIONAL]
# Naming rule: scheduled or sensor, global or domain or table, cloudrun or bash or dataproc or serverless with free-text
{% include 'templates/dags/__starlake_dagster_orchestrator.py' %}
{% include 'templates/dags/__starlake_dataproc_execution.py' %}
{% include 'templates/dags/transform/__scheduled_task_tpl.py.j2' %}
