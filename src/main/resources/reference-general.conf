# root / SL_ROOT: Root path for all starlake relative paths
root = "/tmp"
root = ${?SL_ROOT}


# version / SL_APPLICATION_VERSION: Version of the application
version = "0.0.1"
version = ${?SL_APPLICATION_VERSION}

# database / SL_DATABASE: Database name
database = ""
database = ${?SL_DATABASE}


# connectionRef / SL_CONNECTION_REF: Default connection name
connectionRef = ""
connectionRef = ${?SL_CONNECTION_REF}

loadConnectionRef = ""
loadConnectionRef = ${?SL_LOAD_CONNECTION_REF}
loadConnectionRef = ${?SL_CONNECTION_REF}

transformConnectionRef = ""
transformConnectionRef = ${?SL_TRANSFORM_CONNECTION_REF}
transformConnectionRef = ${?SL_CONNECTION_REF}


# tenant / SL_TENANT: Tenant name
tenant="" # tenant name (optional)
tenant=${?SL_TENANT}

hiveInTest = false
hiveInTest = ${?SL_HIVE_IN_TEST}

# validateOnLoad / SL_VALIDATE_ON_LOAD: Validate all sl.yml files on load and stop at least one is invalid
validateOnLoad = false
validateOnLoad = ${?SL_VALIDATE_ON_LOAD}

# rejectWithValue / SL_REJECT_WITH_VALUE: Add value along with the rejection error. Not enabled by default for security reason. Default: false
rejectWithValue = false
rejectWithValue = ${?SL_REJECT_WITH_VALUE}

# autoExportSchema / SL_AUTO_EXPORT_SCHEMA: Export schema to metadata folder when a table is created through a transform. Useful for the IDE autocomplete feature
autoExportSchema = false
autoExportSchema = ${?SL_AUTO_EXPORT_SCHEMA}


#longJobTimeoutMs / SL_LONG_JOB_TIMEOUT_MS: Timeout in milliseconds for long jobs (ingestion, transform, load)
longJobTimeoutMs = 1800000 # 30 minutes
longJobTimeoutMs = ${?SL_LONG_JOB_TIMEOUT_MS}

#shortJobTimeoutMs / SL_SHORT_JOB_TIMEOUT_MS: Timeout in milliseconds for short jobs (apply RLS, check table exists, validation, audit, expectations)
shortJobTimeoutMs = 180000 # 3 minutes
shortJobTimeoutMs = ${?SL_SHORT_JOB_TIMEOUT_MS}

# sessionDurationServe / SL_SESSION_DURATION_SERVE: Session duration in minutes for starlae server
sessionDurationServe = 10 # in minutes
sessionDurationServe = ${?SL_SESSION_DURATION_SERVE}

# datasets / SL_DATASETS: Datasets path, may be relative or absolute
datasets = ${root}"/datasets"
datasets = ${?SL_DATASETS}

# metadata / SL_METADATA: Metadata path, may be relative or absolute
metadata = ${root}"/metadata"
metadata = ${?SL_METADATA}

# dags / SL_DAGS: Dags path, may be relative or absolute
dags = ${metadata}"/dags"
dags = ${?SL_DAGS}

# tests / SL_TESTS: Tests path, may be relative or absolute
tests = "tests"
tests = ${?SL_TESTS}

types = ${metadata}"/types"
types = ${?SL_TYPES}

macros = ${metadata}"/macros"
macros = ${?SL_MACROS}

testCsvNullString = "null"
testCsvNullString = ${?SL_TEST_CSV_NULL_STRING}

# prunePartitionOnMerge / SL_PRUNE_PARTITION_ON_MERGE: pre-compute incoming partitions to prune partitions on merge statement
prunePartitionOnMerge = false
prunePartitionOnMerge = ${?SL_PRUNE_PARTITION_ON_MERGE}

# write strategies path, may be relative or absolute
writeStrategies = ${metadata}"/write-strategies"
writeStrategies = ${?SL_WRITE_STRATEGIES}

loadStrategies = ${metadata}"/load-strategies"
loadStrategies = ${?SL_LOAD_STRATEGIES}

# useLocalFile-System / SL_USE_LOCAL_FILE_SYSTEM: Do not use Hadoop HDFS path abstraction, use java file API  instead
useLocalFileSystem = false
useLocalFileSystem = ${?SL_USE_LOCAL_FILE_SYSTEM}

dsvOptions {
  # any option listed here https://spark.apache.org/docs/latest/sql-data-sources-csv.html
}


createSchemaIfNotExists = true
createSchemaIfNotExists = ${?SL_CREATE_SCHEMA_IF_NOT_EXISTS}

duckDbEnableExternalAccess = true
duckDbEnableExternalAccess = ${?SL_DUCKDB_ENABLE_EXTERNAL_ACCESS}

duckdbMode = false
duckdbMode = ${?SL_DUCKDB_MODE}
duckdbMode = ${?SL_DUAL_MODE}

duckdbPath = ${?SL_DUCKDB_PATH}


duckdbExtensions = "json,httpfs,spatial"
duckdbExtensions = ${?SL_DUCKDB_EXTENSIONS}

area {
  incoming = "incoming"
  incoming = ${?SL_AREA_INCOMING}
  incoming = ${?SL_INCOMING}
  stage = "stage"
  stage = ${?SL_AREA_STAGE}
  unresolved = "unresolved"
  unresolved = ${?SL_AREA_UNRESOLVED}
  archive = "archive"
  archive = ${?SL_AREA_ARCHIVE}
  ingesting = "ingesting"
  ingesting = ${?SL_AREA_INGESTING}
  replay = "replay"
  replay = ${?SL_AREA_REPLAY}
  hiveDatabase = "${domain}_${area}"
  hiveDatabase = ${?SL_AREA_HIVE_DATABASE}
}

# archive / SL_ARCHIVE: Archive file after load
archive = true
archive = ${?SL_ARCHIVE}

# archiveTablePattern / SL_ARCHIVE_TABLE_PATTERN: Pattern for archive table name
archiveTablePattern = "{{domain}}_archive.{{table}}_archive"

# archiveTable / SL_ARCHIVE_TABLE: Archive table name after load
archiveTable = false
archiveTable = ${?SL_ARCHIVE_TABLE}

# defaultWriteFormat / SL_DEFAULT_WRITE_FORMAT: Default format for write
defaultWriteFormat = parquet
defaultWriteFormat = ${?SL_DEFAULT_WRITE_FORMAT}


# defaultRejectedWriteFormat / SL_DEFAULT_REJECTED_WRITE_FORMAT: Default format for rejected write
defaultRejectedWriteFormat = parquet
defaultRejectedWriteFormat = ${defaultWriteFormat}
defaultRejectedWriteFormat = ${?SL_DEFAULT_REJECTED_WRITE_FORMAT}

# defaultAuditWriteFormat / SL_DEFAULT_AUDIT_WRITE_FORMAT: Default format for audit write
defaultAuditWriteFormat = parquet
defaultAuditWriteFormat = ${defaultWriteFormat}
defaultAuditWriteFormat = ${?SL_DEFAULT_AUDIT_WRITE_FORMAT}



# rejectAllOnError / SL_REJECT_ALL_ON_ERROR: Reject all records on error ?
rejectAllOnError = false
rejectAllOnError = ${?SL_REJECT_ALL_ON_ERROR}

# rejectMaxRecords / SL_REJECT_MAX_RECORDS: Max records to reject
rejectMaxRecords = 2147483647
rejectMaxRecords = ${?SL_REJECT_MAX_RECORDS}

# onExceptionRetries / SL_ON_EXCEPTION_RETRIES: Max attempts of a retryable task on eligible exceptions
onExceptionRetries = 3
onExceptionRetries = ${?SL_ON_EXCEPTION_RETRIES}

lock {
  path = ${datasets}"/locks"
  path = ${?SL_LOCK_PATH}

  timeout = -1
  timeout = ${?SL_LOCK_TIMEOUT}
}

# grouped / SL_GROUPED: Grouped load
grouped = false
grouped = ${?SL_GROUPED}

# groupedMax / SL_GROUPED_MAX: Max files per grouped load
groupedMax = 1000000
groupedMax = ${?SL_GROUPED_MAX}

# loadStrategyClass / SL_LOAD_STRATEGY: Load strategy class
loadStrategyClass = "ai.starlake.job.load.IngestionTimeStrategy"
loadStrategyClass = ${?SL_LOAD_STRATEGY}

# sinkReplayToFile / SL_SINK_REPLAY_TO_FILE: Sink replay to file
sinkReplayToFile = false
sinkReplayToFile = ${?SL_SINK_REPLAY_TO_FILE}

# csvOutput / SL_CSV_OUTPUT: Save Format in CSV with coalesce(1)
csvOutput = false
csvOutput = ${?SL_CSV_OUTPUT}

# csvOutputExt / SL_CSV_OUTPUT_EXT: Save file extension when saving in CSV with coalesce(1)
csvOutputExt = ""
csvOutputExt = ${?SL_CSV_OUTPUT_EXT}

#maxParCopy / SL_MAX_PAR_COPY: Max parallelism for a copy job. 1 means no parallelism. Not suported in local/dev mode
maxParCopy = 1
maxParCopy = ${?SL_MAX_PAR_COPY}

maxParTask = 1
maxParTask = ${?SL_MAX_PAR_TASK} # max parallelism for a dag of task. 1 means no parallelism. Not suported in local/dev mode

forceHalt = false
forceHalt = ${?SL_FORCE_HALT} # force jvm halt at the end of the process. Workaround if starlake hangs on.

jobIdEnvName = ${?SL_JOB_ID_ENV_NAME} # name of environment variable to be used to retrieve job id from audit logs. Has less precedence than SL_JOB_ID.

privacy {
  options {
    "none": "ai.starlake.utils.No",
    "hide": "ai.starlake.utils.Hide",
    "hide10X": "ai.starlake.utils.Hide(\"X\",10)",
    "approxLong20": "ai.starlake.utils.ApproxLong(20)",
    "md5": "ai.starlake.utils.Md5",
    "sha1": "ai.starlake.utils.Sha1",
    "sha256": "ai.starlake.utils.Sha256",
    "sha512": "ai.starlake.utils.Sha512",
    "initials": "ai.starlake.utils.Initials"
  }
}

privacyOnly = false
privacyOnly = ${?SL_PRIVACY_ONLY}

scd2StartTimestamp = "sl_start_ts"
scd2StartTimestamp = ${?SL_SCD2_START_TIMESTAMP}
scd2EndTimestamp = "sl_end_ts"
scd2EndTimestamp = ${?SL_SCD2_END_TIMESTAMP}

udfs = ${?SL_UDFS}

emptyIsNull = true
emptyIsNull = ${?SL_EMPTY_IS_NULL}

loader = "spark" # "spark" or "native"
loader = ${?SL_LOADER}

rowValidatorClass = "ai.starlake.job.validator.FlatRowValidator"

env = ""
env = ${?SL_ENV}


sqlParameterPattern = "\\$\\{\\s*%s\\s*\\}"

hadoop {
}

accessPolicies {
  apply = true
  apply = ${?SL_ACCESS_POLICIES_APPLY}
  location = "invalid_location" // europe-west1 or eu or us or ...
  location = ${?SL_ACCESS_POLICIES_LOCATION}
  database = "invalid_project"
  database = ${?SL_DATABASE}
  database = ${?GCLOUD_PROJECT}
  database = ${?SL_ACCESS_POLICIES_PROJECT_ID}

  taxonomy = "invalid_taxonomy"
  taxonomy = ${?SL_ACCESS_POLICIES_TAXONOMY}
}


forceViewPattern = "[a-zA-Z][a-zA-Z0-9_]{1,256}"
forceViewPattern = ${?SL_FORCE_VIEW_PATTERN}

forceDomainPattern = "[_a-zA-Z][$£a-zA-Z0-9_]{1,100}"
forceDomainPattern = ${?SL_FORCE_DOMAIN_PATTERN}

forceTablePattern = "[_a-zA-Z][$£a-zA-Z0-9_]{1,256}"
forceTablePattern = ${?SL_FORCE_TABLE_PATTERN}

forceJobPattern = "[a-zA-Z][a-zA-Z0-9_]{1,100}"
forceJobPattern = ${?SL_FORCE_JOB_PATTERN}

forceTaskPattern = "[a-zA-Z][a-zA-Z0-9_]{1,100}"
forceTaskPattern = ${?SL_FORCE_TASK_PATTERN}

timezone = "UTC"
timezone = ${?SL_TIMEZONE}

maxInteractiveRecords = 1000
maxInteractiveRecords = ${?SL_MAX_INTERACTIVE_RECORDS}


syncSqlWithYaml = false
#syncSqlWithYaml = ${?SL_SYNC_SQL_WITH_YAML}

syncYamlWithDb = true
syncYamlWithDb = ${?SL_SYNC_YAML_WITH_DB}