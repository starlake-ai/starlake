# root / SL_ROOT: Root path for all starlake relatibve paths
root = "/tmp"
root = ${?SL_ROOT}

version = "0.0.1"
version = ${?SL_APPLICATION_VERSION}

database = ""
database = ${?SL_DATABASE}

connectionRef = ""
connectionRef = ${?SL_CONNECTION_REF}

tenant="" # tenant name (optional)
tenant=${?SL_TENANT}

# validate-on-load / SL_VALIDATE_ON_LOAD: Validate all sl.yml files on load and stop at least one is invalid
validateOnLoad = false
validateOnLoad = ${?SL_VALIDATE_ON_LOAD}



rootServe = ${?SL_ROOT_SERVE}

# session-duration-serve / SL_SESSION_DURATION_SERVE: Session duration in minutes for starlae server
sessionDurationServe = 10 # in minutes
sessionDurationServe = ${?SL_SESSION_DURATION_SERVE}

# datasets / SL_DATASETS: Datasets path, may be relative or absolute
datasets = ${root}"/datasets"
datasets = ${?SL_DATASETS}

# metadata / SL_METADATA: Metadata path, may be relative or absolute
metadata = ${root}"/metadata"
metadata = ${?SL_METADATA}

# dags / SL_DAGS: Dags path, may be relative or absolute
dags = ${metadata}"/dags"
dags = ${?SL_DAGS}

# use-local-file-system / SL_USE_LOCAL_FILE_SYSTEM: Do not use Hadoop HDFS path abstraction, use java file API  instead
useLocalFileSystem = false
useLocalFileSystem = ${?SL_USE_LOCAL_FILE_SYSTEM}


dsvOptions {
  # any option listed here https://spark.apache.org/docs/latest/sql-data-sources-csv.html
}



area {
  pending = "pending"
  pending = ${?SL_AREA_PENDING}
  unresolved = "unresolved"
  unresolved = ${?SL_AREA_UNRESOLVED}
  archive = "archive"
  archive = ${?SL_AREA_ARCHIVE}
  ingesting = "ingesting"
  ingesting = ${?SL_AREA_INGESTING}
  accepted = "accepted"
  accepted = ${?SL_AREA_ACCEPTED}
  rejected = "rejected"
  rejected = ${?SL_AREA_REJECTED}
  business = "business"
  business = ${?SL_AREA_BUSINESS}
  replay = "replay"
  replay = ${?SL_AREA_REPLAY}
  hiveDatabase = "${domain}_${area}"
  hiveDatabase = ${?SL_AREA_HIVE_DATABASE}
}

dagRef {
  load = ${?SL_DAG_REF_LOAD}
  transform = ${?SL_DAG_REF_TRANSFORM}
}

archive = true
archive = ${?SL_ARCHIVE}

archiveTablePattern = "{{domain}}_archive.{{table}}_archive"
archiveTable = true
archiveTable = ${?SL_ARCHIVE_TABLE}

defaultFormat = parquet
defaultFormat = ${?SL_DEFAULT_WRITE_FORMAT}

defaultRejectedWriteFormat = parquet
defaultRejectedWriteFormat = ${?SL_DEFAULT_WRITE_FORMAT}
defaultRejectedWriteFormat = ${?SL_DEFAULT_REJECTED_WRITE_FORMAT}

defaultAuditWriteFormat = parquet
defaultAuditWriteFormat = ${?SL_DEFAULT_WRITE_FORMAT}
defaultAuditWriteFormat = ${?SL_DEFAULT_AUDIT_WRITE_FORMAT}

rejectAllOnError = false
rejectAllOnError = ${?SL_REJECT_ALL_ON_ERROR}

rejectMaxRecords = 2147483647
rejectMaxRecords = ${?SL_REJECT_MAX_RECORDS}

hive = false
hive = ${?SL_HIVE}


analyze = true
analyze = ${?SL_ANALYZE}

lock {
  path = ${root}"/locks"
  path = ${?SL_LOCK_PATH}

  timeout = -1
  timeout = ${?SL_LOCK_TIMEOUT}
}

grouped = false
grouped = ${?SL_GROUPED}

groupedMax = 1000000
groupedMax = ${?SL_GROUPED_MAX}

loadStrategyClass = "ai.starlake.job.load.IngestionTimeStrategy"
loadStrategyClass = ${?SL_LOAD_STRATEGY}


sinkReplayToFile = false
sinkReplayToFile = ${?SL_SINK_REPLAY_TO_FILE}

# Save Format in CSV with coalesce(1)
csvOutput = false
csvOutput = ${?SL_CSV_OUTPUT}

csvOutputExt = ""
csvOutputExt = ${?SL_CSV_OUTPUT_EXT}

maxParCopy = 1
maxParCopy = ${?SL_MAX_PAR_COPY}

maxParTask = 1
maxParTask = ${?SL_MAX_PAR_TASK} # max parallelism for a dag of task. 1 means no parallelism. Not suported in local/dev mode

forceHalt = false
forceHalt = ${?SL_FORCE_HALT} # force jvm halt at the end of the process. Workaround if starlake hangs on.

jobIdEnvName = ${?SL_JOB_ID_ENV_NAME} # name of environment variable to be used to retrieve job id from audit logs. Has less precedence than SL_JOB_ID.

scheduling {
  maxJobs = 1
  maxJobs = ${?SL_SCHEDULING_MAX_JOBS}
  mode = "FIFO"
  mode = ${?SL_SCHEDULING_MODE}
  file = ""
  file = ${?SL_SCHEDULING_FILE}
  poolName = "default"
  poolName = ${?SL_SCHEDULING_POOL_NAME}
}

privacy {
  options {
    "none": "ai.starlake.privacy.No",
    "hide": "ai.starlake.privacy.Hide",
    "hide10X": "ai.starlake.privacy.Hide(\"X\",10)",
    "approxLong20": "ai.starlake.privacy.ApproxLong(20)",
    "md5": "ai.starlake.privacy.Md5",
    "sha1": "ai.starlake.privacy.Sha1",
    "sha256": "ai.starlake.privacy.Sha256",
    "sha512": "ai.starlake.privacy.Sha512",
    "initials": "ai.starlake.privacy.Initials"
  }
}

privacyOnly = false
privacyOnly = ${?SL_PRIVACY_ONLY}

mergeForceDistinct = false
mergeForceDistinct = ${?SL_MERGE_FORCE_DISTINCT}

mergeOptimizePartitionWrite = false
mergeOptimizePartitionWrite = ${?SL_MERGE_OPTIMIZE_PARTITION_WRITE}

udfs = ${?SL_UDFS}

chewerPrefix = "starlake.chewer"
chewerPrefix = ${?SL_CHEWER_PREFIX}

emptyIsNull = true
emptyIsNull = ${?SL_EMPTY_IS_NULL}

loader = "spark" # "spark" or "native"
loader = ${?SL_LOADER}
rowValidatorClass = "ai.starlake.job.validator.FlatRowValidator"
rowValidatorClass = ${?SL_ROW_VALIDATOR_CLASS}

treeValidatorClass = "ai.starlake.job.validator.TreeRowValidator"
treeValidatorClass = ${?SL_TREE_VALIDATOR_CLASS}

env = ""
env = ${?SL_ENV}
database = ""


sqlParameterPattern = "\\$\\{\\s*%s\\s*\\}"

hadoop {
}

accessPolicies {
  apply = true
  apply = ${?SL_ACCESS_POLICIES_APPLY}
  location = "invalid_location" // eu or us or ...
  location = ${?SL_ACCESS_POLICIES_LOCATION}
  database = "invalid_project"
  database = ${?SL_DATABASE}
  database = ${?GCLOUD_PROJECT}
  database = ${?SL_ACCESS_POLICIES_PROJECT_ID}

  taxonomy = "invalid_taxonomy"
  taxonomy = ${?SL_ACCESS_POLICIES_TAXONOMY}
}


forceViewPattern = "[a-zA-Z][a-zA-Z0-9_]{1,256}"
forceViewPattern = ${?SL_FORCE_VIEW_PATTERN}

forceDomainPattern = "[a-zA-Z][a-zA-Z0-9_]{1,100}"
forceDomainPattern = ${?SL_FORCE_DOMAIN_PATTERN}

forceTablePattern = "[a-zA-Z][a-zA-Z0-9_]{1,256}"
forceTablePattern = ${?SL_FORCE_TABLE_PATTERN}

forceJobPattern = "[a-zA-Z][a-zA-Z0-9_]{1,100}"
forceJobPattern = ${?SL_FORCE_JOB_PATTERN}

forceTaskPattern = "[a-zA-Z][a-zA-Z0-9_]{1,100}"
forceTaskPattern = ${?SL_FORCE_TASK_PATTERN}

include "reference-audit"
include "reference-expectations"
include "reference-spark"
include "reference-airflow"
include "reference-kafka"
include "reference-connections"
include "reference-service"


internal {
  # See https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/storage/StorageLevel.html
  cacheStorageLevel = "MEMORY_AND_DISK"
  cacheStorageLevel = ${?SL_INTERNAL_CACHE_STORAGE_LEVEL}
  # Parquet is the default format : https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet
  # other values include orc & avro. When using avro, spark-avro dependency should be included at runtime.
  # If you want to use orc or avro format, you should be aware of this
  # For orc format (All fields in the detected schema are NULLABLE) : https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-orc
  # For avro format : https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro
  intermediateBigqueryFormat = "parquet"
  intermediateBigqueryFormat = ${?SL_INTERMEDIATE_BQ_FORMAT}
  substituteVars = true
  substituteVars = ${?SL_INTERNAL_SUBSTITUTE_VARS}
  temporaryGcsBucket = ${?TEMPORARY_GCS_BUCKET}
  temporaryGcsBucket = ${?SL_TEMPORARY_GCS_BUCKET}
}

extra {

}




schedulePresets {

}


refs = []
