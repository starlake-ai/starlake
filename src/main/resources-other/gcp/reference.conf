root = "/tmp"
root = ${?SL_ROOT}

datasets = ${root}"/datasets"
datasets = ${?SL_DATASETS}

metadata = ${root}"/metadata"
metadata = ${?SL_METADATA}

tmpdir = ${root}"/SL_tmp"
tmpdir = ${?SL_TMPDIR}

archive = true
archive = ${?SL_ARCHIVE}

launcher = airflow
launcher = simple
launcher = ${?SL_LAUNCHER}

expectations {
  active = false
  active = ${?SL_EXPECTATIONS_ACTIVE}
  path = ${root}"/expectations/{{domain}}"
  path = ${?SL_EXPECTATIONS_PATH}
  sink {
    type = "DefaultSink" # can be BigQuerySink or JdbcSink or DefaultSink or EsSink
    type = ${?SL_EXPECTATIONS_SINK_TYPE}
    connection-ref = "expectations" // serves as dataset name for BigQuery or Elasticsearch index name

    ## BigQuery options
    # location = "europe-west1"
    # timestamp = "_PARTITIONTIME"
    # clustering = "???"
    # days = 7
    # require-partition-filter = false

    # Jdbc options
    partitions = 1
    batch-size = 1000
  }
}

hive = false
hive = ${?SL_HIVE}

grouped = false
grouped = ${?SL_GROUPED}

analyze = true
analyze = ${?SL_ANALYZE}

# Save Format in CSV with coalesce(1)
csv-output = false
csv-output = ${?SL_CSV_OUTPUT}

privacy-only = false
privacy-only = ${?SL_PRIVACY_ONLY}

default-write-format = parquet
default-write-format = ${?SL_DEFAULT_WRITE_FORMAT}

default-rejected-write-format = parquet
default-rejected-write-format = ${?SL_DEFAULT_REJECTED_WRITE_FORMAT}


merge-force-distinct = false
merge-force-distinct = ${?SL_MERGE_FORCE_DISTINCT}

file-system = "gs://nps-datastore"
#file-system = "hdfs://localhost:9000"
#file-system = "abfs://starlakefs@starlakeacc.dfs.core.windows.net"
file-system = ${?SL_FS}

udfs = ${?SL_UDFS}

metadata-file-system = ${file-system}
metadata-file-system = ${?SL_METADATA_FS}

chewer-prefix = "starlake.chewer"
chewer-prefix = ${?SL_CHEWER_PREFIX}

row-validator-class = "ai.starlake.job.validator.FlatRowValidator"
row-validator-class = ${?SL_ROW_VALIDATOR_CLASS}

lock {
  path = ${root}"/locks"
  path = ${?SL_LOCK_PATH}


  timeout = -1
  timeout = ${?SL_LOCK_TIMEOUT}
}

hadoop {
}

audit {
  path = ${root}"/audit"
  path = ${?SL_AUDIT_PATH}

  audit-timeout = -1
  audit-timeout = ${?SL_LOCK_AUDIT_TIMEOUT}

  max-errors = 100
  sink {
    type = "DefaultSink" # can be BigQuerySink or JdbcSink or DefaultSink or EsSink
    type = ${?SL_AUDIT_SINK_TYPE}
    connection-ref = "audit" // serves as dataset name for BigQuery or Elasticsearch index name
    connection-ref = ${?SL_AUDIT_SINK_NAME}

    ## BigQuery options
    # location = "europe-west1"
    # timestamp = "_PARTITIONTIME"
    # clustering = "???"
    # days = 7
    # require-partition-filter = false


    # Jdbc options
    partitions = 1
    batchSize = 1000
  }

}

metrics {
  active = false
  active = ${?SL_METRICS_ACTIVE}

  #  path = ${root}"/metrics/{domain}/{schema}"
  path = ${root}"/metrics/{{domain}}"
  path = ${?SL_METRICS_PATH}

  discrete-max-cardinality = 10
  discrete-max-cardinality = ${?SL_METRICS_DISCRETE_MAX_CARDINALITY}
}

connections = {
  "audit": {
    "format" = "jdbc"
    #    "mode" = "Append"
    options = {
      "url": "jdbc:postgresql://127.0.0.1:5403/starlake?user=postgres&password=ficpug-Podbid-7fobnu",
      "user": "postgres",
      "password": "ficpug-Podbid-7fobnu",
      "driver": "org.postgresql.Driver"
      # driver = "org.h2.Driver"
    }
  }
}



area {
  pending = "pending"
  pending = ${?SL_AREA_PENDING}
  unresolved = "unresolved"
  unresolved = ${?SL_AREA_UNRESOLVED}
  archive = "archive"
  archive = ${?SL_AREA_ARCHIVE}
  ingesting = "ingesting"
  ingesting = ${?SL_AREA_INGESTING}
  accepted = "accepted"
  accepted = ${?SL_AREA_ACCEPTED}
  rejected = "rejected"
  rejected = ${?SL_AREA_REJECTED}
  business = "business"
  business = ${?SL_AREA_BUSINESS}
}

privacy {
  options = {
    "none": "ai.starlake.utils.No",
    "hide": "ai.starlake.utils.Hide",
    "hide10X": "ai.starlake.utils.Hide(\"X\",10)",
    "approxLong20": "ai.starlake.utils.ApproxLong(20)",
    "md5": "ai.starlake.utils.Md5",
    "sha1": "ai.starlake.utils.Sha1",
    "sha256": "ai.starlake.utils.Sha256",
    "sha512": "ai.starlake.utils.Sha512",
    "initials": "ai.starlake.utils.Initials"
  }
}

spark {
  #  sql.hive.convertMetastoreParquet = false
  #  yarn.principal = "invalid"
  #  yarn.keytab = "invalid"
  #  yarn.principal = ${?SPARK_YARN_PRINCIPAL}
  #  yarn.keytab = ${?SPARK_YARN_KEYTAB}
  debug.maxToStringFields = 100
  #master = "local[*]"
  #  sql.catalogImplementation="hive"
  #  sql.catalogImplementation="in-memory"
  sql.legacy.parquet.datetimeRebaseModeInWrite = "CORRECTED"
  viewsEnabled = "true"
}


# curl -v -H 'Cache-Control: no-cache'  -H 'Content-Type: application/json'  -XPOST localhost:8080/api/experimental/dags/SL_validator/dag_runs -d '{"conf":"{\"key\":\"value\"}"}'

airflow {
  ingest = "SL_ingest"
  ingest = ${?AIRFLOW_INGEST}
  endpoint = "http://127.0.0.1:8080/api/experimental"
  endpoint = ${?AIRFLOW_ENDPOINT}
}


internal {
  # See https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/storage/StorageLevel.html
  cache-storage-level = "MEMORY_AND_DISK"
  cache-storage-level = ${?SL_INTERNAL_CACHE_STORAGE_LEVEL}
}

}
