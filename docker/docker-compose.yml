name: starlake
version: '3.8'

services:
  starlake-db:
    image: ghcr.io/hydradatabase/hydra:latest
    restart: on-failure
    container_name: starlake-db
    ports:
      - ${SL_DB_PORT:-5432}:5432
    environment:
      POSTGRES_USER: ${SL_POSTGRES_USER:-dbuser}
      POSTGRES_PASSWORD: ${SL_POSTGRES_PASSWORD:-dbuser123}
      POSTGRES_DB: ${SL_POSTGRES_DB:-starlake}
      AIRFLOW_DB: ${AIRFLOW_DB:-airflow}
      DAGSTER_DB: ${DAGSTER_DB:-dagster}
    command: postgres -c 'config_file=/etc/postgresql/postgresql.conf'
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./conf/postgresql.conf:/etc/postgresql/postgresql.conf
      - ./scripts/init-airflow-database.sh:/docker-entrypoint-initdb.d/init-airflow-database.sh
      - ./scripts/init-dagster-database.sh:/docker-entrypoint-initdb.d/init-dagster-database.sh

  starlake-nas:
    image: starlakeai/starlake-nas:latest
    build:
      context: .  # Assuming Dockerfile_nas is in the current directory
      dockerfile: Dockerfile_nas
    container_name: starlake-nas
    restart: on-failure
    privileged: true  # Required to access /proc/fs/nfsd
    ports:
      - ${SL_NFS_PORT:-2049}:2049  # NFS default port
    volumes:
      - projects_data:/projects

  starlake-init-airflow-db:
    image: starlakeai/starlake-airflow:latest
    restart: on-failure
    build:
      context: .  # Assuming Dockerfile_airflow is in the current directory
      dockerfile: Dockerfile_airflow
    container_name: starlake-init-airflow-db
    depends_on:
      - starlake-db
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${SL_POSTGRES_USER:-dbuser}:${SL_POSTGRES_PASSWORD:-dbuser123}@starlake-db:5432/${AIRFLOW_DB:-airflow}
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      INSTALL_MYSQL_CLIENT: 'false'
      INSTALL_MSSQL_CLIENT: 'false'
    entrypoint: >
      /bin/bash -c "
      sleep 10 &&
      airflow db upgrade &&
      airflow users create --username ${AIRFLOW_USERNAME:-admin} --firstname ${AIRFLOW_FIRSTNAME:-Admin} --lastname ${AIRFLOW_LASTNAME:-User} --role Admin --email ${AIRFLOW_EMAIL:-admin@example.com} --password ${AIRFLOW_PASSWORD:-admin}"

  starlake-airflow:
    image: starlakeai/starlake-airflow:latest
    build:
      context: .  # Assuming Dockerfile_airflow is in the current directory
      dockerfile: Dockerfile_airflow
    container_name: starlake-airflow
    restart: on-failure
    depends_on:
      - starlake-db
      - starlake-nas
      - starlake-init-airflow-db
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${SL_POSTGRES_USER:-dbuser}:${SL_POSTGRES_PASSWORD:-dbuser123}@starlake-db:5432/${AIRFLOW_DB:-airflow}
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      INSTALL_MYSQL_CLIENT: 'false'
      INSTALL_MSSQL_CLIENT: 'false'
      SL_HOME: /app/starlake
      AIRFLOW__WEBSERVER__BASE_URL: http://starlake-airflow:8080/airflow
    entrypoint: >
      /bin/bash -c "
      sleep 10 &&
      pip install --no-cache-dir starlake-orchestration[airflow] --upgrade  --force-reinstall &&
      sudo mkdir -p /mnt/filestore/projects &&
      sudo mount -v -o nolock starlake-nas:/projects /mnt/filestore/projects &&
      sudo mount -v -o nolock starlake-nas:/projects/dags /opt/airflow/dags &&
      airflow scheduler &
      exec airflow webserver"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock  # Mount Docker socket to run Docker commands from the container
    ports:
      - ${SL_AIRFLOW_PORT:-8080}:8080  # Airflow Webserver port
    privileged: true  # Required for mounting NFS

  starlake-dagster:
    image: starlakeai/starlake-dagster:latest
    build:
      context: .  # Assuming Dockerfile_dagster is in the current directory
      dockerfile: Dockerfile_dagster
    container_name: starlake-dagster
    restart: on-failure
    depends_on:
      - starlake-db
      - starlake-nas
    environment:
      DAGSTER_PG_USERNAME: ${SL_POSTGRES_USER:-dbuser}
      DAGSTER_PG_PASSWORD: ${SL_POSTGRES_PASSWORD:-dbuser123}
      DAGSTER_PG_HOST: starlake-db
      DAGSTER_PG_DB: ${DAGSTER_DB:-dagster}
      SL_HOME: /app/starlake
    entrypoint: >
      /bin/bash -c "
      sleep 10 &&
      pip install --no-cache-dir starlake-orchestration[dagster] --upgrade  --force-reinstall &&
      mkdir -p /mnt/filestore/projects &&
      mount -v -o nolock starlake-nas:/projects /mnt/filestore/projects &&
      mount -v -o nolock starlake-nas:/projects/dags /opt/dagster/app/dags &&
      python3 dagster_code_locations.py &&
      service cron --full-restart &
      exec dagster-webserver -h 0.0.0.0 -p 3000 --path-prefix /dagster"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock  # Mount Docker socket to run Docker commands from the container
    ports:
      - ${SL_DAGSTER_PORT:-3000}:3000  # Dagster Webserver port
    privileged: true  # Required for mounting NFS

  starlake-api:
    image: starlakeai/starlake-1.3-api:${SL_API_VERSION:-0.1}
    pull_policy: always
    container_name: starlake-api
    restart: on-failure
    depends_on:
      - starlake-db
      - starlake-nas
      - starlake-airflow
    privileged: true  # Required for mount permissions
    ports:
      - ${SL_API_HTTP_PORT:-9000}:9000  # starlake-api default port
    environment:
      - SL_HOME=/app/starlake
      - SL_FS=file://
      - SL_ENV=
      - SL_ROOT=
      - SL_USE_LOCAL_FILE_SYSTEM=false
      - SL_API_GIT_COMMAND_ROOT=/git
      - SL_API_SECURE=false
      - SL_API_SESSION_AS_HEADER=true
      - SL_API_HTTP_FRONT_URL=http://starlake-ui
      - SL_API_HTTP_INTERFACE=0.0.0.0
      - SL_API_HTTP_PORT=9000
      - SL_LOG_LEVEL=info
      - SL_API_JDBC_DRIVER=org.postgresql.Driver
      - SL_API_JDBC_USER=${SL_POSTGRES_USER:-dbuser}
      - SL_API_JDBC_PASSWORD=${SL_POSTGRES_PASSWORD:-dbuser123}
      - SL_API_JDBC_URL=jdbc:postgresql://starlake-db:5432/${SL_POSTGRES_DB:-starlake}?user=${SL_POSTGRES_USER:-dbuser}&password=${SL_POSTGRES_PASSWORD:-dbuser123} # JDBC URL to connect to the database
      - SL_API_DOMAIN=localhost
      - SL_API_PROJECT_ROOT=/mnt/filestore/projects
      - ENVIRONMENT=local # local environment
      - FILESTORE_SHARE_NAME=projects  # Environment variable to specify the share name of the NAS
      - FILESTORE_IP_ADDRESS=starlake-nas  # Environment variable to specify the IP address of the NAS
      - FILESTORE_MNT_DIR=/mnt/filestore/projects  # Environment variable to specify the mount path inside starlake-api container

  starlake-ui:
    image: starlakeai/starlake-1.3-ui:${SL_UI_VERSION:-0.1}
    pull_policy: always
    container_name: starlake-ui
    restart: on-failure
    depends_on:
      - starlake-api
    privileged: true  # Required for mount permissions
    ports:
      - ${SL_PORT:-80}:80  # starlake-ui default port
    environment:
      - FILESTORE_SHARE_NAME=projects  # Environment variable to specify the share name of the NAS
      - FILESTORE_IP_ADDRESS=starlake-nas  # Environment variable to specify the IP address of the NAS
      - FILESTORE_MNT_DIR=/mnt/filestore/projects  # Environment variable to specify the mount path inside starlake-api container
#    command: >
#      npm run start:prod
    volumes:
      - .env.docker:/app/.env:ro

volumes:
  projects_data:
#    driver: local
  pgdata:
#    driver: local
